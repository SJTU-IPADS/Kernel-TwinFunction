commit ec5bc284ae28293f357f1f65a841bf8f0ce23637
Author: hselasky <hselasky@FreeBSD.org>
Date:   Thu Feb 1 13:01:44 2018 +0000

    MFC r310014-r327788:
    This is an overwrite merge backport of the LinuxKPI from FreeBSD-head.
    Following is a complete list of MFC'ed revisions and also partially
    MFC'ed revisions in the end. The MFC'ed revision are listed in
    incremental order.
    
    Bump the __FreeBSD_version to force recompilation of any external
    kernel modules.
    
    Sponsored by:   Mellanox Technologies
    
    MFC r310014:
    Remove the only user of sysctl_add_oid().
    
    My plan is to change this function's prototype at some point in the
    future to add a new label argument, which can be used to export all of
    sysctl as metrics that can be scraped by Prometheus. Switch over this
    caller to use the macro wrapper counterpart.
    
    MFC r310031:
    linuxkpi: Fix not-found case of linux_pci_find_irq_dev
    
    Linux list_for_each_entry() does not neccessarily end with the iterator
    NULL (it may be an offset from NULL if the list member is not the first
    element of the member struct).
    
    Reported by:    Coverity
    CID:            1366940
    Reviewed by:    hselasky@
    Sponsored by:   Dell EMC Isilon
    Differential Revision:  https://reviews.freebsd.org/D8780
    
    MFC r313806:
    Whitespace fix.
    
    Obtained from:          kmacy @
    Sponsored by:           Mellanox Technologies
    
    MFC r313807:
    Allow passing a constant atomic_t to atomic_read().
    
    Obtained from:          kmacy @
    Sponsored by:           Mellanox Technologies
    
    MFC r313808:
    Implement more LinuxKPI atomic functions and macros.
    
    Obtained from:          kmacy @
    Sponsored by:           Mellanox Technologies
    
    MFC r313810:
    Allow container_of() to be used with constant data pointers.
    
    Obtained from:          kmacy @
    Sponsored by:           Mellanox Technologies
    
    MFC r313872:
    Implement GFP_DMA32 flag in the LinuxKPI.
    Define all FreeBSD native GFP bits as GFP_NATIVE_MASK.
    
    Obtained from:          kmacy @
    Sponsored by:           Mellanox Technologies
    
    MFC r314040:
    Make the LinuxKPI task struct persistent accross system calls.
    
    A set of helper functions have been added to manage the life of the
    LinuxKPI task struct. When an external system call or task is invoked,
    a check is made to create the task struct by demand. A thread
    destructor callback is registered to free the task struct when a
    thread exits to avoid memory leaks.
    
    This change lays the ground for emulating the Linux kernel more
    closely which is a dependency by the code using the LinuxKPI APIs.
    
    Add new dedicated td_lkpi_task field has been added to struct thread
    instead of abusing td_retval[1].
    
    Fix some header file inclusions to make LINT kernel build properly
    after this change.
    
    Bump the __FreeBSD_version to force a rebuild of all kernel modules.
    
    Sponsored by:           Mellanox Technologies
    
    MFC r314043:
    Add support for LinuxKPI tasklets.
    
    Tasklets are implemented using a taskqueue and a small statemachine on
    top. The additional statemachine is required to ensure all LinuxKPI
    tasklets get serialized. FreeBSD taskqueues do not guarantee
    serialisation of its tasks, except when there is only one worker
    thread configured.
    
    Sponsored by:           Mellanox Technologies
    
    MFC r314044:
    Streamline the LinuxKPI spinlock wrappers.
    
    1) Add better spinlock debug names when WITNESS_ALL is defined.
    
    2) Make sure that the calling thread gets bound to the current CPU
    while a spinlock is locked. Some Linux kernel code depends on that the
    CPU ID doesn't change while a spinlock is locked.
    
    3) Add support for using LinuxKPI spinlocks during a panic().
    
    Sponsored by:           Mellanox Technologies
    
    MFC r314050:
    Replace dummy implementation of RCU in the LinuxKPI with one based on
    the in-kernel concurrency kit's ck_epoch API. Factor RCU hlist_xxx()
    functions into own rculist.h header file.
    
    Obtained from:          kmacy @
    Sponsored by:           Mellanox Technologies
    
    MFC r314105:
    Improve LinuxKPI scatter list support.
    
    The i915kms driver in Linux 4.9 reimplement parts of the scatter list
    functions with regards to performance. In other words there is not so
    much room for changing structure layouts and functionality if the
    i915kms should be built AS-IS. This patch aligns the scatter list
    support to what is expected by the i915kms driver. Remove some
    comments not needed while at it.
    
    Obtained from:          kmacy @
    Sponsored by:           Mellanox Technologies
    
    MFC r314106:
    Optimise unmapped LinuxKPI page allocations.
    
    When allocating unmapped pages, take advantage of the direct map on
    AMD64 to get the virtual address corresponding to a page. Else all
    pages allocated must be mapped because sometimes the virtual address
    of a page is requested.
    
    Move all page allocation and deallocation code into an own C-file.
    
    Add support for GFP_DMA32, GFP_KERNEL, GFP_ATOMIC and __GFP_ZERO
    allocation flags.
    
    Make a clear separation between mapped and unmapped allocations.
    
    Obtained from:          kmacy @
    Sponsored by:           Mellanox Technologies
    
    MFC r314109:
    Convert magic values into macros in the LinuxKPI scatterlist
    implementation.
    
    Suggested by:           cem @
    Sponsored by:           Mellanox Technologies
    
    MFC r314136:
    Implement __test_and_clear_bit() and __test_and_set_bit() in the LinuxKPI.
    
    The clang compiler will optimise these functions down to three AMD64
    instructions if the bit argument is a constant during compilation.
    
    Sponsored by:           Mellanox Technologies
    
    MFC r314205:
    Implement BIT_ULL() macro in the LinuxKPI.
    
    Sponsored by:           Mellanox Technologies
    
    MFC r314207:
    Implement srcu_dereference() macro in the LinuxKPI.
    
    Sponsored by:           Mellanox Technologies
    
    MFC r314214:
    Prototype device structure to ensure LinuxKPI header file can be
    included standalone.
    
    Sponsored by:           Mellanox Technologies
    
    MFC r314215:
    Implement more string functions in the LinuxKPI.
    
    Sponsored by:           Mellanox Technologies
    
    MFC r314336:
    Define __sum16 type in the LinuxKPI.
    
    Sponsored by:           Mellanox Technologies
    
    MFC r314337:
    Implement more bit operation functions in the LinuxKPI.
    Some minor whitespace nits while at it.
    
    Obtained from:          kmacy @
    Sponsored by:           Mellanox Technologies
    
    MFC r314604:
    Update the LinuxKPI RCU and SRCU wrappers for the concurrency kit, CK.
    
    - Optimise the RCU implementation to not allocate and free
    ck_epoch_records during runtime. Instead allocate two sets of
    ck_epoch_records per CPU for general purpose use. The first set is
    only used for reader locks and the second set is only used for
    synchronization and barriers and is protected with a regular mutex to
    prevent simultaneous issues.
    
    - Move the task structure away from the rcu_head structure and into
    the per-CPU structures. This allows the size of the rcu_head structure
    to be reduced down to the size of two pointers.
    
    - Fix a bug where the linux_rcu_barrier() function only waited for one
    per-CPU epoch record to be completed instead of all.
    
    - Use a critical section or a mutex to protect ck_epoch_begin() and
    ck_epoch_end() depending on RCU or SRCU type. All the ck_epoch_xxx()
    functions, except ck_epoch_register(), ck_epoch_unregister() and
    ck_epoch_recycle() are not re-entrant and needs a critical section or
    a mutex to operate in the LinuxKPI, after inspecting the CK
    implementation of the above mentioned functions. The simultaneous
    issues arise from per-CPU epoch records being shared between multiple
    threads depending on the amount of taskswitching and how many threads
    are involved with the RCU and SRCU operations.
    
    - Properly free all epoch records by using safe list traversal at
    LinuxKPI module unload. It turns out the ck_epoch_recycle() always
    have the records on an internal list and use a flag in the epoch
    record to track allocated and free entries. This would lead to use
    after free during module unload.
    
    - Remove redundant synchronize_rcu() call from the
    linux_compat_uninit() function. Let the linux_rcu_runtime_uninit()
    function do the final rcu_barrier() instead.
    
    Sponsored by:           Mellanox Technologies
    
    MFC r314675:
    Remove duplicate prototype in the LinuxKPI to fix compilation warning.
    
    Reported by:            emaste @
    Sponsored by:           Mellanox Technologies
    
    MFC r314771:
    Give LinuxKPI Read-Write semaphores better debug names when
    WITNESS_ALL is defined. The lock name is based on the filename and
    line number where the initialisation happens.
    
    Sponsored by:           Mellanox Technologies
    
    MFC r314772:
    Implement DECLARE_RWSEM() macro in the LinuxKPI to initialize a
    Read-Write semaphore during module init time.
    
    Sponsored by:           Mellanox Technologies
    
    MFC r314774:
    Implement add_timer_on() function in the LinuxKPI.
    
    Obtained from:          kmacy @
    Sponsored by:           Mellanox Technologies
    
    MFC r314843:
    LinuxKPI workqueue cleanup.
    
    This change makes the workqueue implementation behave more like in
    Linux, both functionality wise and structure wise.
    
    All workqueue code has been moved to linux_work.c
    
    Add an atomic based statemachine to the work_struct to ensure proper
    operation. Prior to this change struct_work was directly mapped to a
    FreeBSD task. When a taskqueue has multiple threads the same task may
    end up being executed on more than one worker thread simultaneously.
    This might cause problems with code coming from Linux, which expects
    serial behaviour, similar to Linux tasklets.
    
    Move all global workqueue function names into the linux_xxx domain to
    avoid symbol name clashes in the future.
    
    Implement a few more workqueue related functions and macros.
    
    Create two multithreaded taskqueues for the LinuxKPI during module
    load, one for time-consuming callbacks and one for non-time consuming
    callbacks.
    
    Sponsored by:           Mellanox Technologies
    
    MFC r314853:
    Use grouptaskqueue for tasklets in the LinuxKPI.
    
    This avoids creating own per-CPU threads and also ensures the tasklet
    execution happens on the same CPU core invoking the tasklet.
    
    Sponsored by:           Mellanox Technologies
    
    MFC r314859:
    Make sure jiffies value is cast to an integer in the LinuxKPI before
    doing millisecond conversion. Under FreeBSD jiffies are 32-bit.
    
    Sponsored by:           Mellanox Technologies
    
    MFC r314861:
    Implement time_is_after_eq_jiffies() function in the LinuxKPI.
    
    Sponsored by:           Mellanox Technologies
    
    MFC r314904:
    Implement eth_zero_addr() in the LinuxKPI.
    
    Sponsored by:           Mellanox Technologies
    
    MFC r314905:
    Cleanup the LinuxKPI slab implementation.
    
    Put large functions into linux_slab.c instead of declaring them static
    inline.
    
    Add support for more memory allocation wrappers like kmalloc_array()
    and __vmalloc().
    
    Make sure either the M_WAITOK or the M_NOWAIT flag is set and mask
    away unused memory allocation flags before calling FreeBSD's malloc()
    routine.
    
    Move kmalloc_node() definition to slab.h where it belongs.
    
    Implement support for the SLAB_DESTROY_BY_RCU feature when creating a
    kmem_cache which basically means kmem_cache memory is freed using
    call_rcu().
    
    Sponsored by:           Mellanox Technologies
    
    MFC r314920:
    Fix compilation warning for powerpc64 by not using const keyword in
    return types:
    
    Type qualifiers ignored on function return type [-Wreturn-type]
    
    Reported by:            andreast @
    Sponsored by:           Mellanox Technologies
    
    MFC r314953:
    Don't create any threads before SI_SUB_INIT_IF in the LinuxKPI. Else
    kthread_add() will assert it is called too soon. This fixes a startup
    issue when COMPAT_LINUXKPI is in enabled the kernel configuration
    file.
    
    Reported by:            Michael Butler <imb@protected-networks.net>
    Sponsored by:           Mellanox Technologies
    
    MFC r314965:
    Cleanup the LinuxKPI mutex wrappers.
    
    Add support for using mutexes during KDB and shutdown. This is also
    required for doing mode-switching during panic for drm-next.
    
    Add new mutex functions mutex_init_witness() and mutex_destroy()
    allowing LinuxKPI mutexes to be tracked by witness.
    
    Declare mutex_is_locked() and mutex_is_owned() like inline functions
    to get cleaner warnings. These functions are used inside WARN_ON()
    statements which might look a bit odd if these functions get fully
    expanded.
    
    Give mutexes better debug names through the mutex_name() macro when
    WITNESS_ALL is defined. The mutex_name() macro can prefix parts of the
    filename and line number before the mutex name.
    
    Sponsored by:           Mellanox Technologies
    
    MFC r314970:
    Implement support for mutexes with deadlock avoidance in the LinuxKPI.
    
    When locking a mutex and deadlock is detected the first mutex lock
    call that sees the deadlock will return -EDEADLK .
    
    Sponsored by:           Mellanox Technologies
    
    MFC r314971:
    Fix implementation of the DECLARE_WORK() macro in the LinuxKPI to fully
    initialize the declared work structure and not only the function callback
    pointer.
    
    Sponsored by:           Mellanox Technologies
    
    MFC r315244:
    Set "current" pointer for LinuxKPI interrupts and timer callbacks.
    
    Sponsored by:           Mellanox Technologies
    
    MFC r315410:
    Define some more LinuxKPI task related macros.
    
    Obtained from:          kmacy @
    Sponsored by:           Mellanox Technologies
    
    MFC r315419:
    Implement more userspace memory access functions in the LinuxKPI.
    
    Obtained from:          kmacy @
    Sponsored by:           Mellanox Technologies
    
    MFC r315420:
    The LinuxKPI pagefault disable and enable functions can only be used
    pairwise to support the FreeBSD way of pushing and popping the page
    fault flags. Ensure this by requiring every occurrence of pagefault
    disable function call to have a corresponding pagefault enable call.
    
    Obtained from:          kmacy @
    Sponsored by:           Mellanox Technologies
    
    MFC r315422:
    Use __LP64__ to detect presence of suword64() to fix linking and
    loading of the LinuxKPI on 32-bit platforms.
    
    Reported by:            lwhsu @
    Sponsored by:           Mellanox Technologies
    
    MFC r315442:
    Add comment describing the use of pagefault_disable() and
    pagefault_enable() in the LinuxKPI.
    
    Suggested by:           rpokala@
    Sponsored by:           Mellanox Technologies
    
    MFC r315443:
    Implement minimalistic memory mapping structure, struct mm_struct, and
    some associated helper functions in the LinuxKPI. Let the existing
    linux_alloc_current() function allocate and initialize the new
    structure and let linux_free_current() drop the refcount on the memory
    mapping structure. When the mm_struct's refcount reaches zero, the
    structure is freed.
    
    Obtained from:          kmacy @
    Sponsored by:           Mellanox Technologies
    
    MFC r315457:
    Implement get_pid_task(), pid_task() and some other PID helper
    functions in the LinuxKPI. Add a usage atomic to the task_struct
    structure to facilitate refcounting the task structure when returned
    from get_pid_task(). The get_task_struct() and put_task_struct()
    function is used to manage atomic refcounting. After this change the
    task_struct should only be freed through put_task_struct().
    
    Obtained from:          kmacy @
    Sponsored by:           Mellanox Technologies
    
    MFC r315713:
    Add support for more IPv4 and IPv6 related macros in the LinuxKPI.
    
    Sponsored by:           Mellanox Technologies
    
    MFC r315714:
    Add full VNET support to the inet_get_local_port_range() function in
    the LinuxKPI.
    
    Sponsored by:           Mellanox Technologies
    
    MFC r315719:
    Extend cmpxchg() to support 8- and 16-bit values, and add xchg().
    
    These are needed to support updated revisions of the DRM code.
    
    Reviewed by:    hselasky (previous version)
    
    MFC r315856:
    Add support for ratelimited printouts in the LinuxKPI.
    
    Sponsored by:           Mellanox Technologies
    
    MFC r315859:
    Function macros are preferred in the LinuxKPI.
    
    Sponsored by:           Mellanox Technologies
    
    MFC r315863:
    Add proper error checking for the string to number conversion
    functions in the LinuxKPI.
    
    Sponsored by:           Mellanox Technologies
    
    MFC r315864:
    Use ppsratecheck() for ratelimiting in the LinuxKPI.
    
    Suggested by:           cem @
    Sponsored by:           Mellanox Technologies
    
    MFC r316033:
    Implement a series of physical page management related functions in
    the LinuxKPI for accessing user-space memory in the kernel.
    
    Add functions to hold and wire physical page(s) based on a given range
    of user-space virtual addresses.
    
    Add functions to get and put a reference on, wire, hold, mark
    accessed, copy and dirty a physical page.
    
    Add new VM related structures and defines as a preparation step for
    advancing the memory map capabilities of the LinuxKPI.
    
    Add function to figure out if a virtual address was allocated using
    malloc().
    
    Add function to convert a virtual kernel address into its physical
    page pointer.
    
    Obtained from:          kmacy @
    Sponsored by:           Mellanox Technologies
    
    MFC r316034:
    Add more platforms supporting the direct map feature in the LinuxKPI.
    
    Sponsored by:           Mellanox Technologies
    
    MFC r316035:
    Implement vmalloc_32() in the LinuxKPI.
    
    Obtained from:          kmacy @
    Sponsored by:           Mellanox Technologies
    
    MFC r316521:
    Implement down_write_killable() in the LinuxKPI.
    
    Sponsored by:           Mellanox Technologies
    
    MFC r316522:
    Unify error handling when si_drv1 is NULL in the LinuxKPI.
    
    Make sure the character device poll callback function does not return
    an error code, but a POLLXXX value, in case of failure.
    
    Sponsored by:           Mellanox Technologies
    
    MFC r316561:
    Before registering a new mm_struct in the LinuxKPI check if other
    tasks in the belonging procedure already have a valid mm_struct and
    reference that instead.
    
    The mm_struct in the LinuxKPI should be shared among all tasks
    belonging to the same procedure. This has to do with with the mmap_sem
    semaphore which should serialize all VM operations inside a given
    procedure. Linux based drivers depend on this behaviour.
    
    Sponsored by:           Mellanox Technologies
    
    MFC r316562:
    Implement proper support for memory map operations in the LinuxKPI,
    like open, close and fault using the character device pager.
    
    Some notes about the implementation:
    
    1) Linux drivers set the vm_ops and vm_private_data fields during a
    mmap() call to indicate that the driver wants to use the LinuxKPI VM
    operations. Else these operations are not used.
    
    2) The vm_private_data pointer is associated with a VM area structure
    and inserted into an internal LinuxKPI list. If the vm_private_data
    pointer already exists, the existing VM area structure is used instead
    of the allocated one which gets freed.
    
    3) The LinuxKPI's vm_private_data pointer is used as the callback
    handle for the FreeBSD VM object. The VM subsystem in FreeBSD has a
    similar list to identify equal handles and will only call the
    character device pager's close function once.
    
    4) All LinuxKPI VM operations are serialized through the mmap_sem
    sempaphore, which is per procedure, which prevents simultaneous access
    to the shared VM area structure when receiving page faults.
    
    Obtained from:          kmacy @
    Sponsored by:           Mellanox Technologies
    
    MFC r316563:
    Fix implementation of task_pid_group_leader() in the LinuxKPI.
    
    In FreeBSD thread IDs and procedure IDs have distinct number
    spaces. When asking for the group leader task ID in the LinuxKPI,
    return the procedure ID and let this resolve to the first task in the
    procedure having a valid LinuxKPI task structure pointer.
    
    Sponsored by:           Mellanox Technologies
    
    MFC r316564:
    Implement need_resched() in the LinuxKPI.
    
    Obtained from:          kmacy @
    Sponsored by:           Mellanox Technologies
    
    MFC r316565:
    Define VM_READ, VM_WRITE and VM_EXEC in the LinuxKPI.
    
    Sponsored by:           Mellanox Technologies
    
    MFC r316568:
    Cleanup the bitmap_xxx() functions in the LinuxKPI:
    
    - Move all bitmap related functions from bitops.h to bitmap.h, similar
      to what Linux does.
    
    - Apply some minor code cleanup and simplifications to optimize the
      generated code when using static inline functions.
    
    - Implement the following list of bitmap functions which are needed by
      drm-next and ibcore:
      - bitmap_find_next_zero_area_off()
      - bitmap_find_next_zero_area()
      - bitmap_or()
      - bitmap_and()
      - bitmap_xor()
    
    - Add missing include directives to the qlnxe driver
      (davidcs@ has been notified)
    
    Sponsored by:           Mellanox Technologies
    
    MFC r316606:
    The __stringify() macro in the LinuxKPI should expand any macros
    before stringifying.
    
    Sponsored by:           Mellanox Technologies
    
    MFC r316609:
    Create the LinuxKPI current task structure on the fly if it doesn't
    exist when the current macro is used.
    
    Sponsored by:           Mellanox Technologies
    
    MFC r316656:
    Fix compilation of LinuxKPI for PowerPC.
    
    Found by:               emaste @
    Sponsored by:           Mellanox Technologies
    
    MFC r317135:
    Zero number of CPUs should be translated into the default number of
    CPUs when allocating a LinuxKPI workqueue. This also ensures that the
    created taskqueue always have a non-zero number of worker threads.
    
    Sponsored by:           Mellanox Technologies
    
    MFC r317137:
    Fix problem regarding priority inversion when using the concurrency
    kit, CK, in the LinuxKPI.
    
    When threads are pinned to a CPU core or when there is only one CPU,
    it can happen that a higher priority thread can call the CK
    synchronize function while a lower priority thread holds the read
    lock. Because the CK's synchronize is a simple wait loop this can lead
    to a deadlock situation. To solve this problem use the recently
    introduced CK's wait callback function.
    
    When detecting a CK blocking condition figure out the lowest priority
    among the blockers and update the calling thread's priority and
    yield. If another CPU core is holding the read lock, pin the thread to
    the blocked CPU core and update the priority. The calling threads
    priority and CPU bindings are restored before return.
    
    If a thread holding a CK read lock is detected to be sleeping, pause()
    will be used instead of yield().
    
    Sponsored by:           Mellanox Technologies
    
    MFC r317138:
    Use __typeof() instead of typeof() in some RCU related macros in the LinuxKPI.
    
    Sponsored by:           Mellanox Technologies
    
    MFC r317504:
    Prefer to use real virtual address over direct map address in the
    linux_page_address() function in the LinuxKPI. This solves an issue
    where the return value from linux_page_address() is passed to
    kmem_free().
    
    Sponsored by:           Mellanox Technologies
    
    MFC r317651:
    Add on_each_cpu() and wbinvd_on_all_cpus().
    
    Reviewed by:    hselasky
    Differential Revision:  https://reviews.freebsd.org/D10550
    
    MFC r317828:
    Fix for use after free in the LinuxKPI.
    
    Background:
    The same VM object might be shared by multiple processes and the
    mm_struct is usually freed when a process exits.
    
    Grab a reference on the mm_struct while the vmap is in the
    linux_vma_head list in case the first process which inserted a VM
    object has exited.
    
    Tested by:              kwm @
    Sponsored by:           Mellanox Technologies
    
    MFC r317839:
    Use pmap_invalidate_cache() to implement wbinvd_on_all_cpus().
    
    Suggested by:   jhb
    X-MFC with:     r317651
    
    MFC r318026:
    Fix init order in the LinuxKPI for RCU support.
    
    CPU_FOREACH() is not available until SI_SUB_CPU at SI_ORDER_ANY
    when the LinuxKPI is loaded as part of the kernel.
    
    Sponsored by:           Mellanox Technologies
    
    MFC r318590:
    Add get_cpu() and put_cpu().
    
    MFC r319229:
    Add some miscellaneous definitions to support DRM drivers.
    
    Reviewed by:    hselasky
    Differential Revision:  https://reviews.freebsd.org/D10985
    
    MFC r319312:
    Make sure the thread's priority is restored for all three cases inside
    linux_synchronize_rcu_cb() in the LinuxKPI.
    
    Sponsored by:           Mellanox Technologies
    
    MFC r319316:
    Fixes for refcounting "struct linux_file" in the LinuxKPI.
    
    - Allow "struct linux_file" to be refcounted when its "_file" member
      is NULL by using its "f_count" field. The reference counts are
      transferred to the file structure when the file descriptor is
      installed.
    
    - Add missing vdrop() calls for error cases during open().
    
    - Set the "_file" member of "struct linux_file" during open. This
    allows use of refcounting through get_file() and fput() with LinuxKPI
    character devices.
    
    Sponsored by:           Mellanox Technologies
    
    MFC r319317:
    Fix a reference count leak in the LinuxKPI due to calling VM open when
    it shouldn't be called.
    
    Background:
    The Linux VM open operation is called when a new VMA is
    created on top of the current VMA. This is done through either mremap
    flow or split_vma, usually due to mlock, madvise, munmap and so
    on. This is currently not supported by the LinuxKPI.
    
    Sponsored by:           Mellanox Technologies
    
    MFC r319318:
    Don't acquire a reference on the VM-space when allocating the LinuxKPI
    task structure to avoid deadlock when tearing down the VM object
    during a process exit.
    
    Found by:               markj @
    Sponsored by:           Mellanox Technologies
    
    MFC r319319:
    Remove the VMA handle from its list before calling the LinuxKPI VMA
    close operation to prevent other threads from reusing the VM object
    handle pointer.
    
    Sponsored by:           Mellanox Technologies
    
    MFC r319320:
    Make sure the VMAP's "vm_file" field is referenced in a Linux
    compatible way by the linux_dev_mmap_single() function in the
    LinuxKPI.
    
    Sponsored by:           Mellanox Technologies
    
    MFC r319321:
    Properly set the .d_name field in the cdevsw structure for the
    LinuxKPI.
    
    Obtained from:          kmacy @
    Sponsored by:           Mellanox Technologies
    
    MFC r319338:
    Implement in_atomic() function in the LinuxKPI.
    
    Obtained from:          kmacy @
    Sponsored by:           Mellanox Technologies
    
    MFC r319340:
    Properly implement idr_preload() and idr_preload_end() in the
    LinuxKPI.
    
    Sponsored by:           Mellanox Technologies
    
    MFC r319341:
    Implement print_hex_dump(), print_hex_dump_bytes() and
    printk_ratelimited() in the LinuxKPI.
    
    While at it fix the inclusion guard of printk.h to be similar to the
    rest of the LinuxKPI header files.
    
    Sponsored by:           Mellanox Technologies
    
    MFC r319409:
    Add generic kqueue() and kevent() support to the LinuxKPI character
    devices. The implementation allows read and write filters to be
    created and piggybacks on the poll() file operation to determine when
    a filter should trigger. The piggyback mechanism is simply to check
    for the EWOULDBLOCK or EAGAIN return code from read(), write() or
    ioctl() system calls and then update the kqueue() polling state bits.
    The implementation is similar to the one found in the cuse(3) module.
    Refer to sys/fs/cuse/*.[ch] for more details.
    
    Sponsored by:           Mellanox Technologies
    
    MFC r319410:
    Translate the ERESTARTSYS error code into ERESTART in the LinuxKPI
    ioctl(), read() and write() system call handlers. This error code is
    internal to the kernel and should not be seen by user-space programs
    according to Linux.
    
    Submitted by:           Yanko Yankulov <yanko.yankulov@gmail.com>
    Sponsored by:           Mellanox Technologies
    
    MFC r319444:
    Make sure the selrecord() function is only called from within system
    polling contexts in the LinuxKPI.
    
    After the kqueue() support was added to the LinuxKPI in r319409 the
    Linux poll file operation will be used outside the system file polling
    callback function, which can cause a NULL-pointer panic inside
    selrecord() because curthread->td_sel is set to NULL. This patch moves
    the selrecord() call away from poll_wait() and to the system file poll
    callback function in the LinuxKPI, which essentially wraps the Linux
    one. This is similar to what the cuse(3) module is currently doing.
    Refer to sys/fs/cuse/*.[ch] for more details.
    
    Sponsored by:           Mellanox Technologies
    
    MFC r319500:
    Add support for setting the non-blocking I/O flag for LinuxKPI
    character devices. In Linux the FIONBIO IOCTL is handled by the kernel
    and not the drivers. Also need return success for the FIOASYNC ioctl
    due to existing logic in kern_fcntl() even though it is not supported
    currently.
    
    Sponsored by:           Mellanox Technologies
    
    MFC r319501:
    Improve kqueue() support in the LinuxKPI. Some applications using the
    kqueue() does not set non-blocking I/O mode for event driven read of
    file descriptors. This means the LinuxKPI internal kqueue read and
    write event flags must be updated before the next read and/or write
    system call. Else the read and/or write system call may block. This
    can happen when there is no more data to read following a previous
    read event. Then the application also gets blocked from processing
    other events. This situation can also be solved by the applications
    setting and using non-blocking I/O mode.
    
    Sponsored by:           Mellanox Technologies
    
    MFC r319620:
    Fix init order in the LinuxKPI for IDR support after recent changes.
    
    CPU_FOREACH() is not available until SI_SUB_CPU at SI_ORDER_ANY
    when the LinuxKPI is loaded as part of the kernel.
    
    Sponsored by:   Mellanox Technologies
    
    MFC r319656:
    Add more #ifdef arch checks to the linuxkpi
    
    arm, mips, and powerpc all implement pmap_mapdev_attr() and pmap_unmapdev(),
    so add those archs to the checks.  powerpc also includes the atomic_swap_*()
    functions, so add that to the supported list as well.  Not tested except by
    compiling powerpc.
    
    Reviewed by:    markj
    
    MFC r319675:
    Remove ARM and MIPS from linuxkpi ioremap_attr definition
    
    ARM and MIPS fail universe builds.
    
    ARM and MIPS are missing the following:
    * VM_MEMATTR_WRITE_THROUGH
    * VM_MEMATTR_WRITE_COMBINING
    
    Pointy-hat to:  jhibbits
    
    MFC r319757:
    Augment wait queue support in the LinuxKPI.
    
    In particular:
    - Don't evaluate event conditions with a sleepqueue lock held, since such
      code may attempt to acquire arbitrary locks.
    - Fix the return value for wait_event_interruptible() in the case that the
      wait is interrupted by a signal.
    - Implement wait_on_bit_timeout() and wait_on_atomic_t().
    - Implement some functions used to test for pending signals.
    - Implement a number of wait_event_*() variants and unify the existing
      implementations.
    - Unify the mechanism used by wait_event_*() and schedule() to put the
      calling thread to sleep.
    
    This is required to support updated DRM drivers. Thanks to hselasky for
    finding and fixing a number of bugs in the original revision.
    
    Reviewed by:    hselasky
    Differential Revision:  https://reviews.freebsd.org/D10986
    
    MFC r319758:
    Implement pci_disable_device() in the LinuxKPI.
    
    Submitted by:   kmacy
    
    MFC r320063:
    Remove prototypes for unimplemented LinuxKPI functions.
    
    MFC r320072:
    Avoid including list.h in LinuxKPI headers.
    
    list.h includes a number of FreeBSD headers as a workaround for the
    LIST_HEAD name collision. To reduce pollution, avoid including list.h
    in commonly used headers when it is not explicitly needed.
    
    Reviewed by:    hselasky
    Differential Revision:  https://reviews.freebsd.org/D11249
    
    MFC r320078:
    Add kthread parking support to the LinuxKPI.
    
    Submitted by:   kmacy (original version)
    Reviewed by:    hselasky
    Differential Revision:  https://reviews.freebsd.org/D11264
    
    MFC r320189:
    Allow the VM fault handler to be NULL in the LinuxKPI when handling a
    memory map request. When the VM fault handler is NULL a return code of
    VM_PAGER_BAD is returned from the character device's pager populate
    handler. This fixes compatibility with Linux.
    
    Sponsored by:   Mellanox Technologies
    
    MFC r320192:
    Add a lockdep macro to the LinuxKPI.
    
    Also fix some nearby style issues.
    
    MFC r320193:
    Include kmod.h from the LinuxKPI's module.h.
    
    MFC r320194:
    Add missing lock destructor invocations to the LinuxKPI unload handler.
    
    MFC r320196:
    Update io-mapping.h in the LinuxKPI.
    
    Add io_mapping_init_wc() and add a third (unused) parameter to
    io_mapping_map_wc().
    
    Reviewed by:    hselasky
    Differential Revision:  https://reviews.freebsd.org/D11286
    
    MFC r320333:
    Add noop_lseek() to the LinuxKPI.
    
    MFC r320334:
    Add the thaw_early method to struct dev_pm_ops in the LinuxKPI.
    
    MFC r320335:
    Add a couple of macros to lockdep.h in the LinuxKPI.
    
    MFC r320336:
    Add ns_to_ktime() to the LinuxKPI.
    
    MFC r320337:
    Add u64_to_user_ptr() to the LinuxKPI.
    
    MFC r320364:
    Implement parts of the hrtimer API in the LinuxKPI.
    
    Reviewed by:    hselasky
    Differential Revision:  https://reviews.freebsd.org/D11359
    
    MFC r320580:
    Let io_mapping_init_wc() fall back to an uncacheable mapping.
    
    This allows usage of the function on architectures that don't support
    write-combining.
    
    Reported by:    bz, emaste
    X-MFC With:     r320196
    
    MFC r320627:
    Hold the PCI device list lock when removing an element.
    
    MFC r320633:
    Rename the "driver" field to "bsddriver" to avoid a name collision.
    
    MFC r320634:
    Add some PCI class definitions.
    
    MFC r320635:
    Add a field for the class code to struct pci_driver.
    
    Fill out some previously uninitialized fields as well.
    
    MFC r320636:
    Add some auxiliary types for device driver support.
    
    MFC r320656:
    Invoke suspend/resume methods from the driver pmops if available.
    
    Obtained from:  kmacy (original version)
    
    MFC r320774:
    Fix a bug in synchronize RCU when the calling thread is bound to a CPU.
    
    Set "td_pinned" to zero after "sched_unbind()" to prevent "td_pinned"
    from temporarily becoming negative during "sched_bind()". This can
    happen if "sched_bind()" uses "sched_pin()" and "sched_unpin()".
    
    Sponsored by:   Mellanox Technologies
    
    MFC r320775:
    Complete r320189 which allows a NULL VM fault handler in the LinuxKPI.
    Instead of mapping a dummy page upon a page fault, map the page
    pointed to by the physical address given by IDX_TO_OFF(vmap->vm_pfn).
    To simplify the implementation use OBJT_DEVICE to implement our own
    linux_cdev_pager_fault() instead of using the existing
    linux_cdev_pager_populate().
    
    Some minor code factoring while at it.
    
    Reviewed by:    markj @
    Sponsored by:   Mellanox Technologies
    
    MFC r320810:
    Add TASK_COMM_LEN to the LinuxKPI.
    
    MFC r320811:
    Add device_is_registered() to the LinuxKPI.
    
    MFC r320812:
    Fix the definitions of pgprot_{noncached,writecombine} after r316562.
    
    MFC r320813:
    Add some helper definitions to fs.h in the LinuxKPI.
    
    Add a field to struct linux_file to allow the creation of anonymous
    shmem objects.
    
    MFC r320852:
    Free existing per-thread task structs when unloading linuxkpi.ko.
    
    They are otherwise leaked.
    
    Reported and tested by: ae
    
    MFC r320853:
    Add a few functions to ktime.h in the LinuxKPI, and fix nearby style.
    
    Reviewed by:    hselasky
    Differential Revision:  https://reviews.freebsd.org/D11534
    
    MFC r320854:
    Add some functions to math64.h in the LinuxKPI, and fix nearby style.
    
    Reviewed by:    hselasky
    Differential Revision:  https://reviews.freebsd.org/D11535
    
    MFC r320956:
    Add some functions to jiffies.h.
    
    Also add some checks for overflow to existing functions.
    
    Reviewed by:    hselasky
    Differential Revision:  https://reviews.freebsd.org/D11533
    
    MFC r321773:
    Remove cycle_t type from the LinuxKPI similar to Linux upstream.
    
    Sponsored by:   Mellanox Technologies
    
    MFC r321926:
    Fix LinuxKPI regression after r321920. The mda_unit and si_drv0 fields are not
    wide enough to hold the full 64-bit dev_t. Instead use the "dev" field in
    the "linux_cdev" structure to store and lookup this value.
    
    While at it remove superfluous use of parenthesis inside the
    MAJOR(), MINOR() and MKDEV() macros in the LinuxKPI.
    
    Sponsored by:   Mellanox Technologies
    
    MFC r322028:
    Add subsystem vendor and device ID fields to struct pci_dev.
    
    MFC r322169:
    Fix hrtimer_active() in case of cancellation.
    
    While there, switch to FreeBSD internal callout active status.
    
    Reviewed by:    markj, hselasky
    Sponsored by:   iXsystems, Inc.
    Differential Revision:  https://reviews.freebsd.org/D11900
    
    MFC r322212:
    Add macros for defining attribute groups and for WO and RW attributes.
    
    Reviewed by:    hselasky
    Differential Revision:  https://reviews.freebsd.org/D11872
    
    MFC r322213:
    Add round_jiffies_up(), local_clock() and __setup_timer() to the LinuxKPI.
    
    Reviewed by:    hselasky
    Differential Revision:  https://reviews.freebsd.org/D11871
    
    MFC r322272:
    Fix few issues of LinuxKPI workqueue.
    
    LinuxKPI workqueue wrappers reported "successful" cancellation for works
    already completed in normal way.  This change brings reported status and
    real cancellation fact into sync.  This required for drm-next operation.
    
    Reviewed by:    hselasky (earlier version)
    Sponsored by:   iXsystems, Inc.
    Differential Revision:  https://reviews.freebsd.org/D11904
    
    MFC r322354:
    Make sure the linux_wait_event_common() function in the LinuxKPI properly
    handles a timeout value of MAX_SCHEDULE_TIMEOUT which basically means there
    is no timeout. This is a regression issue after r319757.
    
    While at it change the type of returned variable from "long" to "int" to
    match the actual return type.
    
    Sponsored by:   Mellanox Technologies
    
    MFC r322355:
    Fixes for wait event in the LinuxKPI. These are regression issues
    after r319757.
    
    1) Correct the return value from __wait_event_common() from 1 to 0 in
    case the timeout is specified as MAX_SCHEDULE_TIMEOUT. In the other
    case __ret is zero and will be substituted in the last part of the
    macro with the appropriate value before return.
    
    2) Make sure the "timeout" argument is casted to "int" before
    evaluating negativity. Else the signedness of a "long" might be
    checked instead of the signedness of an integer.
    
    3) The wait_event() function should not have a return value.
    
    Found by:       KrishnamRaju ErapaRaju <Krishna2@chelsio.com>
    Sponsored by:   Mellanox Technologies
    
    MFC r322357:
    Use integer type to pass around jiffies and/or ticks values in the
    LinuxKPI because in FreeBSD ticks are 32-bit.
    
    Sponsored by:   Mellanox Technologies
    
    MFC r322392:
    Add a specialized function for DRM drivers to register themselves.
    
    Such drivers attach to a vgapci bus rather than directly to a pci bus. For
    the rest of the LinuxKPI to work correctly in this case, we override the
    vgapci bus' ivars with those of the grandparent.
    
    Reviewed by:    hselasky
    Differential Revision:  https://reviews.freebsd.org/D11932
    
    MFC r322397:
    Make sure the "vm_flags" and "vm_page_prot" fields get set correctly
    in the VM area structure in the LinuxKPI when doing mmap() and that
    unsupported bits are masked away.
    
    While at it fix some redundant use of parenthesing inside some related
    macros.
    
    Found by:       KrishnamRaju ErapaRaju <Krishna2@chelsio.com>
    Sponsored by:   Mellanox Technologies
    
    MFC r322567:
    Add device resource management fields to struct device.
    
    MFC r322713:
    Add a couple of trivial headers to the LinuxKPI.
    
    MFC r322714:
    Define prefetch() only if it hasn't already been defined.
    
    MFC r322746:
    Fix for deadlock situation in the LinuxKPI's RCU synchronize API.
    
    Deadlock condition:
    The return value of TDQ_LOCKPTR(td) is the same for two threads.
    
    1) The first thread signals a wakeup while keeping the rcu_read_lock().
    This invokes sched_add() which in turn will try to lock TDQ_LOCK().
    
    2) The second thread is calling synchronize_rcu() calling mi_switch() over
    and over again trying to yield(). This prevents the first thread from running
    and releasing the RCU reader lock.
    
    Solution:
    Release the thread lock while yielding to allow other threads to acquire the
    lock pointed to by TDQ_LOCKPTR(td).
    
    Found by:       KrishnamRaju ErapaRaju <Krishna2@chelsio.com>
    Sponsored by:   Mellanox Technologies
    
    MFC r322795:
    Add some miscellaneous definitions to support the DRM drivers.
    
    MFC r322816:
    Set the bus number field when attaching a PCI device.
    
    MFC r323347:
    Add more sanity checks to linux_fget() in the LinuxKPI. This prevents
    returning pointers to file descriptors which were not created by the
    LinuxKPI.
    
    Sponsored by:           Mellanox Technologies
    
    MFC r323349:
    Properly implement poll_wait() in the LinuxKPI. This prevents direct
    use of the linux_poll_wakeup() function from unsafe contexts, which
    can lead to use-after-free issues.
    
    Instead of calling linux_poll_wakeup() directly use the wake_up()
    family of functions in the LinuxKPI to do this.
    
    Bump the FreeBSD version to force recompilation of external kernel modules.
    
    Sponsored by:           Mellanox Technologies
    
    MFC r323703:
    Add support for shared memory functions to the LinuxKPI.
    
    Obtained from:          kmacy @
    Sponsored by:           Mellanox Technologies
    
    MFC r323704:
    Only wire pages in the LinuxKPI instead of holding and wiring them.
    This prevents the page daemon from regularly scanning the held pages.
    
    Suggested by:           kib @
    Sponsored by:           Mellanox Technologies
    
    MFC r323705:
    The LinuxKPI atomics do not have acquire nor release semantics unless
    specified. Fix code to use READ_ONCE() and WRITE_ONCE() where appropriate.
    
    Suggested by:           kib @
    Sponsored by:           Mellanox Technologies
    
    MFC r323910:
    Add support for 32-bit compatibility IOCTLs in the LinuxKPI.
    
    Bump the FreeBSD version to force recompilation of external
    kernel modules due to structure change.
    
    PR:             222504
    Submitted by:   Greg V <greg@unrelenting.technology>
    Sponsored by:   Mellanox Technologies
    
    MFC r324278:
    Make sure the timer belonging to the delayed work in the LinuxKPI
    gets drained before invoking the work function. Else the timer
    mutex may still be in use which can lead to use-after-free situations,
    because the work function might free the work structure before returning.
    
    Sponsored by:   Mellanox Technologies
    
    MFC r324285:
    Add get_random_{int,long} to the LinuxKPI.
    
    Fix some whitespace bugs while here.
    
    Reviewed by:    hselasky
    Differential Revision:  https://reviews.freebsd.org/D12588
    
    MFC r324597:
    Don't call selrecord() outside the select system call in the LinuxKPI, because
    then td->td_sel is NULL and this will result in a segfault inside selrecord().
    This happens when only using kqueue() to poll for read and write events.
    If select() and kqueue() is mixed there won't be a segfault.
    
    Reported by:    Johannes Lundberg
    Sponsored by:   Mellanox Technologies
    
    MFC r324606:
    Make the PHOLD in linux_wait_event_common() unconditional.
    
    After some in-progress work is committed, this would otherwise be the only
    instance of #if(n)def NO_SWAPPING in the tree. Moreover, the requisite
    opt_vm.h include was missing, so the PHOLD/PRELE calls were always being
    compiled in anyway.
    
    MFC r325279:
    Implement ioread16be() in the LinuxKPI.
    
    Sponsored by:   Mellanox Technologies
    
    MFC r325360:
    Remove redundant dev->si_drv1 NULL checks in the LinuxKPI.
    This pointer is checked during the linux_dev_open() callback and does
    not need to be NULL checked again. It should always be set for
    character devices belonging to the "linuxcdevsw" and technically
    there is no need to NULL check this pointer at all.
    
    Suggested by:   kib @
    Sponsored by:   Mellanox Technologies
    
    MFC r325635:
    Remove some not needed comments in the LinuxKPI. Use the Linux source tree
    to lookup documentation for the functions implemented in the LinuxKPI
    instead.
    
    Sponsored by:   Mellanox Technologies
    
    MFC r325707:
    Mask away return codes from del_timer() and del_timer_sync() because
    they are not the same like in Linux.
    
    Sponsored by:   Mellanox Technologies
    
    MFC r325708:
    Remove release and acquire semantics when accessing the "state" field of the
    LinuxKPI task struct. Change type of "state" variable from "int" to
    "atomic_t" to simplify code and avoid unneccessary casting.
    
    Sponsored by:   Mellanox Technologies
    
    MFC r325767:
    Properly handle the case where the linux_cdev_handle_insert() function
    in the LinuxKPI returns NULL. This happens when the VM area's private
    data handle already exists and could cause a so-called NULL pointer
    dereferencing issue prior to this fix.
    
    Found by:       greg@unrelenting.technology
    Sponsored by:   Mellanox Technologies
    
    MFC r327676:
    linuxkpi: Implement kcalloc() based on mallocarray()
    
    This means we now get integer overflow protection, which Linux code
    might expect as it is also provided by kcalloc() in Linux.
    
    MFC r327788:
    linuxkpi: Simplify kmalloc_array.
    
    kmalloc_array seems what we call mallocarray(9).
    
    MFC r312926: (partial, no mergeinfo)
    Revert r312923 a better approach will be taken later
    
    MFC r312927: (partial, no mergeinfo)
    Revert crap accidentally committed
    
    MFC r316665: (partial, no mergeinfo)
    Import CK as of commit 6b141c0bdd21ce8b3e14147af8f87f22b20ecf32
    This brings us changes we needed in ck_epoch.
    
    MFC r317053: (partial, no mergeinfo)
    Remove unneeded include of vm_phys.h.
    
    MFC r317055: (partial, no mergeinfo)
    All these files need sys/vmmeter.h, but now they got it implicitly
    included via sys/pcpu.h.
    
    MFC r322168: (partial, no mergeinfo)
    o Replace __riscv__ with __riscv
    o Replace __riscv64 with (__riscv && __riscv_xlen == 64)
    
    This is required to support new GCC 7.1 compiler.
    This is compatible with current GCC 6.1 compiler.
    
    RISC-V is extensible ISA and the idea here is to have built-in define
    per each extension, so together with __riscv we will have some subset
    of these as well (depending on -march string passed to compiler):
    
    __riscv_compressed
    __riscv_atomic
    __riscv_mul
    __riscv_div
    __riscv_muldiv
    __riscv_fdiv
    __riscv_fsqrt
    __riscv_float_abi_soft
    __riscv_float_abi_single
    __riscv_float_abi_double
    __riscv_cmodel_medlow
    __riscv_cmodel_medany
    __riscv_cmodel_pic
    __riscv_xlen
    
    Reviewed by:    ngie
    Sponsored by:   DARPA, AFRL
    Differential Revision:  https://reviews.freebsd.org/D11901
    
    MFC r322672: (partial, no mergeinfo)
    Move some other SI_SUB_INIT_IF initializations to SI_SUB_TASKQ
    
    Drop the EARLY_AP_STARTUP gtaskqueue code, as gtaskqueues are now
    initialized before APs are started.
    
    Reviewed by:    hselasky@, jhb@
    Sponsored by:   Dell EMC Isilon
    Differential Revision:  https://reviews.freebsd.org/D12054
    
    MFC r326984: (partial, no mergeinfo)
    Update Matthew Macy contact info
    
    Email address has changed, uses consistent name (Matthew, not Matt)
    
    Reported by:    Matthew Macy <mmacy@mattmacy.io>
    Differential Revision:  https://reviews.freebsd.org/D13537

diff --git a/sys/compat/linuxkpi/common/include/asm/atomic-long.h b/sys/compat/linuxkpi/common/include/asm/atomic-long.h
index b7ccba5fcba..b0763f456b9 100644
--- a/sys/compat/linuxkpi/common/include/asm/atomic-long.h
+++ b/sys/compat/linuxkpi/common/include/asm/atomic-long.h
@@ -2,7 +2,7 @@
  * Copyright (c) 2010 Isilon Systems, Inc.
  * Copyright (c) 2010 iX Systems, Inc.
  * Copyright (c) 2010 Panasas, Inc.
- * Copyright (c) 2013, 2014 Mellanox Technologies, Ltd.
+ * Copyright (c) 2013-2017 Mellanox Technologies, Ltd.
  * All rights reserved.
  *
  * Redistribution and use in source and binary forms, with or without
@@ -31,7 +31,7 @@
 #ifndef	_ATOMIC_LONG_H_
 #define	_ATOMIC_LONG_H_
 
-#include <sys/cdefs.h>
+#include <linux/compiler.h>
 #include <sys/types.h>
 #include <machine/atomic.h>
 
@@ -54,13 +54,13 @@ atomic_long_add_return(long i, atomic_long_t *v)
 static inline void
 atomic_long_set(atomic_long_t *v, long i)
 {
-	atomic_store_rel_long(&v->counter, i);
+	WRITE_ONCE(v->counter, i);
 }
 
 static inline long
 atomic_long_read(atomic_long_t *v)
 {
-	return atomic_load_acq_long(&v->counter);
+	return READ_ONCE(v->counter);
 }
 
 static inline long
@@ -75,6 +75,12 @@ atomic_long_dec(atomic_long_t *v)
 	return atomic_fetchadd_long(&v->counter, -1) - 1;
 }
 
+static inline long
+atomic_long_xchg(atomic_long_t *v, long val)
+{
+	return atomic_swap_long(&v->counter, val);
+}
+
 static inline int
 atomic_long_add_unless(atomic_long_t *v, long a, long u)
 {
diff --git a/sys/compat/linuxkpi/common/include/asm/atomic.h b/sys/compat/linuxkpi/common/include/asm/atomic.h
index 4912eefd03b..7f25319fce4 100644
--- a/sys/compat/linuxkpi/common/include/asm/atomic.h
+++ b/sys/compat/linuxkpi/common/include/asm/atomic.h
@@ -2,7 +2,7 @@
  * Copyright (c) 2010 Isilon Systems, Inc.
  * Copyright (c) 2010 iX Systems, Inc.
  * Copyright (c) 2010 Panasas, Inc.
- * Copyright (c) 2013-2016 Mellanox Technologies, Ltd.
+ * Copyright (c) 2013-2017 Mellanox Technologies, Ltd.
  * All rights reserved.
  *
  * Redistribution and use in source and binary forms, with or without
@@ -28,10 +28,11 @@
  *
  * $FreeBSD$
  */
-#ifndef	_ASM_ATOMIC_H_
+
+#ifndef _ASM_ATOMIC_H_
 #define	_ASM_ATOMIC_H_
 
-#include <sys/cdefs.h>
+#include <linux/compiler.h>
 #include <sys/types.h>
 #include <machine/atomic.h>
 
@@ -71,6 +72,12 @@ atomic_sub_return(int i, atomic_t *v)
 static inline void
 atomic_set(atomic_t *v, int i)
 {
+	WRITE_ONCE(v->counter, i);
+}
+
+static inline void
+atomic_set_release(atomic_t *v, int i)
+{
 	atomic_store_rel_int(&v->counter, i);
 }
 
@@ -81,9 +88,9 @@ atomic_set_mask(unsigned int mask, atomic_t *v)
 }
 
 static inline int
-atomic_read(atomic_t *v)
+atomic_read(const atomic_t *v)
 {
-	return atomic_load_acq_int(&v->counter);
+	return READ_ONCE(v->counter);
 }
 
 static inline int
@@ -123,12 +130,13 @@ static inline int
 atomic_xchg(atomic_t *v, int i)
 {
 #if defined(__i386__) || defined(__amd64__) || \
-    defined(__arm__) || defined(__aarch64__)
+    defined(__arm__) || defined(__aarch64__) || \
+    defined(__powerpc__)
 	return (atomic_swap_int(&v->counter, i));
 #else
 	int ret;
 	for (;;) {
-		ret = atomic_load_acq_int(&v->counter);
+		ret = READ_ONCE(v->counter);
 		if (atomic_cmpset_int(&v->counter, ret, i))
 			break;
 	}
@@ -144,7 +152,7 @@ atomic_cmpxchg(atomic_t *v, int old, int new)
 	for (;;) {
 		if (atomic_cmpset_int(&v->counter, old, new))
 			break;
-		ret = atomic_load_acq_int(&v->counter);
+		ret = READ_ONCE(v->counter);
 		if (ret != old)
 			break;
 	}
@@ -152,31 +160,47 @@ atomic_cmpxchg(atomic_t *v, int old, int new)
 }
 
 #define	cmpxchg(ptr, old, new) ({				\
-	__typeof(*(ptr)) __ret = (old);				\
-	CTASSERT(sizeof(__ret) == 4 || sizeof(__ret) == 8);	\
-	for (;;) {						\
-		if (sizeof(__ret) == 4) {			\
-			if (atomic_cmpset_int((volatile int *)	\
-			    (ptr), (old), (new)))		\
-				break;				\
-			__ret = atomic_load_acq_int(		\
-			    (volatile int *)(ptr));		\
-			if (__ret != (old))			\
-				break;				\
-		} else {					\
-			if (atomic_cmpset_64(			\
-			    (volatile int64_t *)(ptr),		\
-			    (old), (new)))			\
-				break;				\
-			__ret = atomic_load_acq_64(		\
-			    (volatile int64_t *)(ptr));		\
-			if (__ret != (old))			\
-				break;				\
-		}						\
+	__typeof(*(ptr)) __ret;					\
+								\
+	CTASSERT(sizeof(__ret) == 1 || sizeof(__ret) == 2 ||	\
+	    sizeof(__ret) == 4 || sizeof(__ret) == 8);		\
+								\
+	__ret = (old);						\
+	switch (sizeof(__ret)) {				\
+	case 1:							\
+		while (!atomic_fcmpset_8((volatile int8_t *)(ptr), \
+		    (int8_t *)&__ret, (new)) && __ret == (old))	\
+			;					\
+		break;						\
+	case 2:							\
+		while (!atomic_fcmpset_16((volatile int16_t *)(ptr), \
+		    (int16_t *)&__ret, (new)) && __ret == (old)) \
+			;					\
+		break;						\
+	case 4:							\
+		while (!atomic_fcmpset_32((volatile int32_t *)(ptr), \
+		    (int32_t *)&__ret, (new)) && __ret == (old)) \
+			;					\
+		break;						\
+	case 8:							\
+		while (!atomic_fcmpset_64((volatile int64_t *)(ptr), \
+		    (int64_t *)&__ret, (new)) && __ret == (old)) \
+			;					\
+		break;						\
 	}							\
 	__ret;							\
 })
 
+#define	cmpxchg_relaxed(...)	cmpxchg(__VA_ARGS__)
+
+#define	xchg(ptr, v) ({						\
+	__typeof(*(ptr)) __ret;					\
+								\
+	__ret = *(ptr);						\
+	*(ptr) = v;						\
+	__ret;							\
+})
+
 #define	LINUX_ATOMIC_OP(op, c_op)				\
 static inline void atomic_##op(int i, atomic_t *v)		\
 {								\
@@ -187,8 +211,26 @@ static inline void atomic_##op(int i, atomic_t *v)		\
 		c = old;					\
 }
 
+#define	LINUX_ATOMIC_FETCH_OP(op, c_op)				\
+static inline int atomic_fetch_##op(int i, atomic_t *v)		\
+{								\
+	int c, old;						\
+								\
+	c = v->counter;						\
+	while ((old = atomic_cmpxchg(v, c, c c_op i)) != c)	\
+		c = old;					\
+								\
+	return (c);						\
+}
+
 LINUX_ATOMIC_OP(or, |)
 LINUX_ATOMIC_OP(and, &)
+LINUX_ATOMIC_OP(andnot, &~)
 LINUX_ATOMIC_OP(xor, ^)
 
+LINUX_ATOMIC_FETCH_OP(or, |)
+LINUX_ATOMIC_FETCH_OP(and, &)
+LINUX_ATOMIC_FETCH_OP(andnot, &~)
+LINUX_ATOMIC_FETCH_OP(xor, ^)
+
 #endif					/* _ASM_ATOMIC_H_ */
diff --git a/sys/compat/linuxkpi/common/include/asm/atomic64.h b/sys/compat/linuxkpi/common/include/asm/atomic64.h
index f8dd2bbf583..5cf6bcfb074 100644
--- a/sys/compat/linuxkpi/common/include/asm/atomic64.h
+++ b/sys/compat/linuxkpi/common/include/asm/atomic64.h
@@ -1,5 +1,5 @@
 /*-
- * Copyright (c) 2016 Mellanox Technologies, Ltd.
+ * Copyright (c) 2016-2017 Mellanox Technologies, Ltd.
  * All rights reserved.
  *
  * Redistribution and use in source and binary forms, with or without
@@ -28,7 +28,7 @@
 #ifndef	_ASM_ATOMIC64_H_
 #define	_ASM_ATOMIC64_H_
 
-#include <sys/cdefs.h>
+#include <linux/compiler.h>
 #include <sys/types.h>
 #include <machine/atomic.h>
 
@@ -36,6 +36,8 @@ typedef struct {
 	volatile int64_t counter;
 } atomic64_t;
 
+#define	ATOMIC64_INIT(x)	{ .counter = (x) }
+
 /*------------------------------------------------------------------------*
  *	64-bit atomic operations
  *------------------------------------------------------------------------*/
@@ -72,7 +74,7 @@ atomic64_set(atomic64_t *v, int64_t i)
 static inline int64_t
 atomic64_read(atomic64_t *v)
 {
-	return atomic_load_acq_64(&v->counter);
+	return READ_ONCE(v->counter);
 }
 
 static inline int64_t
@@ -106,12 +108,13 @@ static inline int64_t
 atomic64_xchg(atomic64_t *v, int64_t i)
 {
 #if defined(__i386__) || defined(__amd64__) || \
-    defined(__arm__) || defined(__aarch64__)
+    defined(__arm__) || defined(__aarch64__) || \
+    defined(__powerpc64__)
 	return (atomic_swap_64(&v->counter, i));
 #else
 	int64_t ret;
 	for (;;) {
-		ret = atomic_load_acq_64(&v->counter);
+		ret = READ_ONCE(v->counter);
 		if (atomic_cmpset_64(&v->counter, ret, i))
 			break;
 	}
@@ -127,7 +130,7 @@ atomic64_cmpxchg(atomic64_t *v, int64_t old, int64_t new)
 	for (;;) {
 		if (atomic_cmpset_64(&v->counter, old, new))
 			break;
-		ret = atomic_load_acq_64(&v->counter);
+		ret = READ_ONCE(v->counter);
 		if (ret != old)
 			break;
 	}
diff --git a/sys/compat/linuxkpi/common/include/asm/msr.h b/sys/compat/linuxkpi/common/include/asm/msr.h
new file mode 100644
index 00000000000..34519df4381
--- /dev/null
+++ b/sys/compat/linuxkpi/common/include/asm/msr.h
@@ -0,0 +1,36 @@
+/*-
+ * Copyright (c) 2017 Mark Johnston <markj@FreeBSD.org>
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in
+ *    the documentation and/or other materials provided with the
+ *    distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR AND CONTRIBUTORS ``AS IS'' AND
+ * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED.  IN NO EVENT SHALL THE AUTHOR OR CONTRIBUTORS BE LIABLE
+ * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+ * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
+ * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
+ * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+ * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
+ * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
+ * SUCH DAMAGE.
+ *
+ * $FreeBSD$
+ */
+
+#ifndef _ASM_MSR_H_
+#define	_ASM_MSR_H_
+
+#include <machine/cpufunc.h>
+
+#define	rdmsrl(msr, val)	((val) = rdmsr(msr))
+
+#endif /* _ASM_MSR_H_ */
diff --git a/sys/compat/linuxkpi/common/include/asm/pgtable.h b/sys/compat/linuxkpi/common/include/asm/pgtable.h
index c54eb04c720..1df10306036 100644
--- a/sys/compat/linuxkpi/common/include/asm/pgtable.h
+++ b/sys/compat/linuxkpi/common/include/asm/pgtable.h
@@ -33,4 +33,11 @@
 
 #include <linux/page.h>
 
+typedef unsigned long	pteval_t;
+typedef unsigned long	pmdval_t;
+typedef unsigned long	pudval_t;
+typedef unsigned long	pgdval_t;
+typedef unsigned long	pgprotval_t;
+typedef struct page *pgtable_t;
+
 #endif	/* _ASM_PGTABLE_H_ */
diff --git a/sys/compat/linuxkpi/common/include/asm/smp.h b/sys/compat/linuxkpi/common/include/asm/smp.h
new file mode 100644
index 00000000000..3a20d1ff4bf
--- /dev/null
+++ b/sys/compat/linuxkpi/common/include/asm/smp.h
@@ -0,0 +1,48 @@
+/*-
+ * Copyright (c) 2017 Mark Johnston <markj@FreeBSD.org>
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in
+ *    the documentation and/or other materials provided with the
+ *    distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR AND CONTRIBUTORS ``AS IS'' AND
+ * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED.  IN NO EVENT SHALL THE AUTHOR OR CONTRIBUTORS BE LIABLE
+ * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+ * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
+ * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
+ * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+ * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
+ * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
+ * SUCH DAMAGE.
+ *
+ * $FreeBSD$
+ */
+
+#ifndef _ASM_SMP_H_
+#define	_ASM_SMP_H_
+
+#if defined(__i386__) || defined(__amd64__)
+
+#define	wbinvd_on_all_cpus()	linux_wbinvd_on_all_cpus()
+
+int	linux_wbinvd_on_all_cpus(void);
+
+#endif
+
+#define	get_cpu() ({			\
+	sched_pin();			\
+	PCPU_GET(cpuid);		\
+})
+
+#define	put_cpu()			\
+	sched_unpin()
+
+#endif /* _ASM_SMP_H_ */
diff --git a/sys/compat/linuxkpi/common/include/linux/atomic.h b/sys/compat/linuxkpi/common/include/linux/atomic.h
new file mode 100644
index 00000000000..736dea8179c
--- /dev/null
+++ b/sys/compat/linuxkpi/common/include/linux/atomic.h
@@ -0,0 +1,35 @@
+/*-
+ * Copyright (c) 2017 Mark Johnston <markj@FreeBSD.org>
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in
+ *    the documentation and/or other materials provided with the
+ *    distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR AND CONTRIBUTORS ``AS IS'' AND
+ * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED.  IN NO EVENT SHALL THE AUTHOR OR CONTRIBUTORS BE LIABLE
+ * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+ * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
+ * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
+ * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+ * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
+ * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
+ * SUCH DAMAGE.
+ *
+ * $FreeBSD$
+ */
+
+#ifndef _LINUX_ATOMIC_H_
+#define	_LINUX_ATOMIC_H_
+
+#include <asm/atomic.h>
+#include <asm/atomic64.h>
+
+#endif /* _LINUX_ATOMIC_H_ */
diff --git a/sys/compat/linuxkpi/common/include/linux/bitmap.h b/sys/compat/linuxkpi/common/include/linux/bitmap.h
new file mode 100644
index 00000000000..897682f8f6b
--- /dev/null
+++ b/sys/compat/linuxkpi/common/include/linux/bitmap.h
@@ -0,0 +1,279 @@
+/*
+ * Copyright (c) 2013-2017 Mellanox Technologies, Ltd.
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice unmodified, this list of conditions, and the following
+ *    disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS OR
+ * IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+ * OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.
+ * IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT,
+ * INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT
+ * NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+ * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+ * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF
+ * THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ *
+ * $FreeBSD$
+ */
+
+#ifndef _LINUX_BITMAP_H_
+#define	_LINUX_BITMAP_H_
+
+#include <linux/bitops.h>
+
+static inline void
+bitmap_zero(unsigned long *addr, const unsigned int size)
+{
+	memset(addr, 0, BITS_TO_LONGS(size) * sizeof(long));
+}
+
+static inline void
+bitmap_fill(unsigned long *addr, const unsigned int size)
+{
+	const unsigned int tail = size & (BITS_PER_LONG - 1);
+
+	memset(addr, 0xff, BIT_WORD(size) * sizeof(long));
+
+	if (tail)
+		addr[BIT_WORD(size)] = BITMAP_LAST_WORD_MASK(tail);
+}
+
+static inline int
+bitmap_full(unsigned long *addr, const unsigned int size)
+{
+	const unsigned int end = BIT_WORD(size);
+	const unsigned int tail = size & (BITS_PER_LONG - 1);
+	unsigned int i;
+
+	for (i = 0; i != end; i++) {
+		if (addr[i] != ~0UL)
+			return (0);
+	}
+
+	if (tail) {
+		const unsigned long mask = BITMAP_LAST_WORD_MASK(tail);
+
+		if ((addr[end] & mask) != mask)
+			return (0);
+	}
+	return (1);
+}
+
+static inline int
+bitmap_empty(unsigned long *addr, const unsigned int size)
+{
+	const unsigned int end = BIT_WORD(size);
+	const unsigned int tail = size & (BITS_PER_LONG - 1);
+	unsigned int i;
+
+	for (i = 0; i != end; i++) {
+		if (addr[i] != 0)
+			return (0);
+	}
+
+	if (tail) {
+		const unsigned long mask = BITMAP_LAST_WORD_MASK(tail);
+
+		if ((addr[end] & mask) != 0)
+			return (0);
+	}
+	return (1);
+}
+
+static inline void
+bitmap_set(unsigned long *map, unsigned int start, int nr)
+{
+	const unsigned int size = start + nr;
+	int bits_to_set = BITS_PER_LONG - (start % BITS_PER_LONG);
+	unsigned long mask_to_set = BITMAP_FIRST_WORD_MASK(start);
+
+	map += BIT_WORD(start);
+
+	while (nr - bits_to_set >= 0) {
+		*map |= mask_to_set;
+		nr -= bits_to_set;
+		bits_to_set = BITS_PER_LONG;
+		mask_to_set = ~0UL;
+		map++;
+	}
+
+	if (nr) {
+		mask_to_set &= BITMAP_LAST_WORD_MASK(size);
+		*map |= mask_to_set;
+	}
+}
+
+static inline void
+bitmap_clear(unsigned long *map, unsigned int start, int nr)
+{
+	const unsigned int size = start + nr;
+	int bits_to_clear = BITS_PER_LONG - (start % BITS_PER_LONG);
+	unsigned long mask_to_clear = BITMAP_FIRST_WORD_MASK(start);
+
+	map += BIT_WORD(start);
+
+	while (nr - bits_to_clear >= 0) {
+		*map &= ~mask_to_clear;
+		nr -= bits_to_clear;
+		bits_to_clear = BITS_PER_LONG;
+		mask_to_clear = ~0UL;
+		map++;
+	}
+
+	if (nr) {
+		mask_to_clear &= BITMAP_LAST_WORD_MASK(size);
+		*map &= ~mask_to_clear;
+	}
+}
+
+static inline unsigned int
+bitmap_find_next_zero_area_off(const unsigned long *map,
+    const unsigned int size, unsigned int start,
+    unsigned int nr, unsigned int align_mask,
+    unsigned int align_offset)
+{
+	unsigned int index;
+	unsigned int end;
+	unsigned int i;
+
+retry:
+	index = find_next_zero_bit(map, size, start);
+
+	index = (((index + align_offset) + align_mask) & ~align_mask) - align_offset;
+
+	end = index + nr;
+	if (end > size)
+		return (end);
+
+	i = find_next_bit(map, end, index);
+	if (i < end) {
+		start = i + 1;
+		goto retry;
+	}
+	return (index);
+}
+
+static inline unsigned int
+bitmap_find_next_zero_area(const unsigned long *map,
+    const unsigned int size, unsigned int start,
+    unsigned int nr, unsigned int align_mask)
+{
+	return (bitmap_find_next_zero_area_off(map, size,
+	    start, nr, align_mask, 0));
+}
+
+static inline int
+bitmap_find_free_region(unsigned long *bitmap, int bits, int order)
+{
+	int pos;
+	int end;
+
+	for (pos = 0; (end = pos + (1 << order)) <= bits; pos = end) {
+		if (!linux_reg_op(bitmap, pos, order, REG_OP_ISFREE))
+			continue;
+		linux_reg_op(bitmap, pos, order, REG_OP_ALLOC);
+		return (pos);
+	}
+	return (-ENOMEM);
+}
+
+static inline int
+bitmap_allocate_region(unsigned long *bitmap, int pos, int order)
+{
+	if (!linux_reg_op(bitmap, pos, order, REG_OP_ISFREE))
+		return (-EBUSY);
+	linux_reg_op(bitmap, pos, order, REG_OP_ALLOC);
+	return (0);
+}
+
+static inline void
+bitmap_release_region(unsigned long *bitmap, int pos, int order)
+{
+	linux_reg_op(bitmap, pos, order, REG_OP_RELEASE);
+}
+
+static inline unsigned int
+bitmap_weight(unsigned long *addr, const unsigned int size)
+{
+	const unsigned int end = BIT_WORD(size);
+	const unsigned int tail = size & (BITS_PER_LONG - 1);
+	unsigned int retval = 0;
+	unsigned int i;
+
+	for (i = 0; i != end; i++)
+		retval += hweight_long(addr[i]);
+
+	if (tail) {
+		const unsigned long mask = BITMAP_LAST_WORD_MASK(tail);
+
+		retval += hweight_long(addr[end] & mask);
+	}
+	return (retval);
+}
+
+static inline int
+bitmap_equal(const unsigned long *pa,
+    const unsigned long *pb, unsigned size)
+{
+	const unsigned int end = BIT_WORD(size);
+	const unsigned int tail = size & (BITS_PER_LONG - 1);
+	unsigned int i;
+
+	for (i = 0; i != end; i++) {
+		if (pa[i] != pb[i])
+			return (0);
+	}
+
+	if (tail) {
+		const unsigned long mask = BITMAP_LAST_WORD_MASK(tail);
+
+		if ((pa[end] ^ pb[end]) & mask)
+			return (0);
+	}
+	return (1);
+}
+
+static inline void
+bitmap_or(unsigned long *dst, const unsigned long *src1,
+    const unsigned long *src2, const unsigned int size)
+{
+	const unsigned int end = BITS_TO_LONGS(size);
+	unsigned int i;
+
+	for (i = 0; i != end; i++)
+		dst[i] = src1[i] | src2[i];
+}
+
+static inline void
+bitmap_and(unsigned long *dst, const unsigned long *src1,
+    const unsigned long *src2, const unsigned int size)
+{
+	const unsigned int end = BITS_TO_LONGS(size);
+	unsigned int i;
+
+	for (i = 0; i != end; i++)
+		dst[i] = src1[i] & src2[i];
+}
+
+static inline void
+bitmap_xor(unsigned long *dst, const unsigned long *src1,
+    const unsigned long *src2, const unsigned int size)
+{
+	const unsigned int end = BITS_TO_LONGS(size);
+	unsigned int i;
+
+	for (i = 0; i != end; i++)
+		dst[i] = src1[i] ^ src2[i];
+}
+
+#endif					/* _LINUX_BITMAP_H_ */
diff --git a/sys/compat/linuxkpi/common/include/linux/bitops.h b/sys/compat/linuxkpi/common/include/linux/bitops.h
index 9e1fa2bc456..e3af17da006 100644
--- a/sys/compat/linuxkpi/common/include/linux/bitops.h
+++ b/sys/compat/linuxkpi/common/include/linux/bitops.h
@@ -2,7 +2,7 @@
  * Copyright (c) 2010 Isilon Systems, Inc.
  * Copyright (c) 2010 iX Systems, Inc.
  * Copyright (c) 2010 Panasas, Inc.
- * Copyright (c) 2013-2015 Mellanox Technologies, Ltd.
+ * Copyright (c) 2013-2017 Mellanox Technologies, Ltd.
  * All rights reserved.
  *
  * Redistribution and use in source and binary forms, with or without
@@ -31,16 +31,20 @@
 #ifndef	_LINUX_BITOPS_H_
 #define	_LINUX_BITOPS_H_
 
+#include <sys/param.h>
 #include <sys/types.h>
 #include <sys/systm.h>
 #include <sys/errno.h>
+#include <sys/libkern.h>
 
 #define	BIT(nr)			(1UL << (nr))
+#define	BIT_ULL(nr)		(1ULL << (nr))
 #ifdef __LP64__
 #define	BITS_PER_LONG		64
 #else
 #define	BITS_PER_LONG		32
 #endif
+
 #define	BITMAP_FIRST_WORD_MASK(start)	(~0UL << ((start) % BITS_PER_LONG))
 #define	BITMAP_LAST_WORD_MASK(n)	(~0UL >> (BITS_PER_LONG - (n)))
 #define	BITS_TO_LONGS(n)	howmany((n), BITS_PER_LONG)
@@ -49,6 +53,12 @@
 #define	GENMASK(h, l)		(((~0UL) >> (BITS_PER_LONG - (h) - 1)) & ((~0UL) << (l)))
 #define BITS_PER_BYTE           8
 
+#define	hweight8(x)	bitcount((uint8_t)(x))
+#define	hweight16(x)	bitcount16(x)
+#define	hweight32(x)	bitcount32(x)
+#define	hweight64(x)	bitcount64(x)
+#define	hweight_long(x)	bitcountl(x)
+
 static inline int
 __ffs(int mask)
 {
@@ -73,10 +83,15 @@ __flsl(long mask)
 	return (flsl(mask) - 1);
 }
 
+static inline int
+fls64(uint64_t mask)
+{
+	return (flsll(mask));
+}
+
 static inline uint32_t
 ror32(uint32_t word, unsigned int shift)
 {
-
 	return ((word >> shift) | (word << (32 - shift)));
 }
 
@@ -241,70 +256,6 @@ find_next_zero_bit(const unsigned long *addr, unsigned long size,
 	return (bit);
 }
 
-static inline void
-bitmap_zero(unsigned long *addr, int size)
-{
-	int len;
-
-	len = BITS_TO_LONGS(size) * sizeof(long);
-	memset(addr, 0, len);
-}
-
-static inline void
-bitmap_fill(unsigned long *addr, int size)
-{
-	int tail;
-	int len;
-
-	len = (size / BITS_PER_LONG) * sizeof(long);
-	memset(addr, 0xff, len);
-	tail = size & (BITS_PER_LONG - 1);
-	if (tail) 
-		addr[size / BITS_PER_LONG] = BITMAP_LAST_WORD_MASK(tail);
-}
-
-static inline int
-bitmap_full(unsigned long *addr, int size)
-{
-	unsigned long mask;
-	int tail;
-	int len;
-	int i;
-
-	len = size / BITS_PER_LONG;
-	for (i = 0; i < len; i++)
-		if (addr[i] != ~0UL)
-			return (0);
-	tail = size & (BITS_PER_LONG - 1);
-	if (tail) {
-		mask = BITMAP_LAST_WORD_MASK(tail);
-		if ((addr[i] & mask) != mask)
-			return (0);
-	}
-	return (1);
-}
-
-static inline int
-bitmap_empty(unsigned long *addr, int size)
-{
-	unsigned long mask;
-	int tail;
-	int len;
-	int i;
-
-	len = size / BITS_PER_LONG;
-	for (i = 0; i < len; i++)
-		if (addr[i] != 0)
-			return (0);
-	tail = size & (BITS_PER_LONG - 1);
-	if (tail) {
-		mask = BITMAP_LAST_WORD_MASK(tail);
-		if ((addr[i] & mask) != 0)
-			return (0);
-	}
-	return (1);
-}
-
 #define	__set_bit(i, a)							\
     atomic_set_long(&((volatile unsigned long *)(a))[BIT_WORD(i)], BIT_MASK(i))
 
@@ -318,8 +269,7 @@ bitmap_empty(unsigned long *addr, int size)
     atomic_clear_long(&((volatile unsigned long *)(a))[BIT_WORD(i)], BIT_MASK(i))
 
 #define	test_bit(i, a)							\
-    !!(atomic_load_acq_long(&((volatile unsigned long *)(a))[BIT_WORD(i)]) &	\
-    BIT_MASK(i))
+    !!(READ_ONCE(((volatile unsigned long *)(a))[BIT_WORD(i)]) & BIT_MASK(i))
 
 static inline int
 test_and_clear_bit(long bit, volatile unsigned long *var)
@@ -337,6 +287,21 @@ test_and_clear_bit(long bit, volatile unsigned long *var)
 }
 
 static inline int
+__test_and_clear_bit(long bit, volatile unsigned long *var)
+{
+	long val;
+
+	var += BIT_WORD(bit);
+	bit %= BITS_PER_LONG;
+	bit = (1UL << bit);
+
+	val = *var;
+	*var &= ~bit;
+
+	return !!(val & bit);
+}
+
+static inline int
 test_and_set_bit(long bit, volatile unsigned long *var)
 {
 	long val;
@@ -351,46 +316,19 @@ test_and_set_bit(long bit, volatile unsigned long *var)
 	return !!(val & bit);
 }
 
-static inline void
-bitmap_set(unsigned long *map, int start, int nr)
+static inline int
+__test_and_set_bit(long bit, volatile unsigned long *var)
 {
-	unsigned long *p = map + BIT_WORD(start);
-	const int size = start + nr;
-	int bits_to_set = BITS_PER_LONG - (start % BITS_PER_LONG);
-	unsigned long mask_to_set = BITMAP_FIRST_WORD_MASK(start);
-
-	while (nr - bits_to_set >= 0) {
-		*p |= mask_to_set;
-		nr -= bits_to_set;
-		bits_to_set = BITS_PER_LONG;
-		mask_to_set = ~0UL;
-		p++;
-	}
-	if (nr) {
-		mask_to_set &= BITMAP_LAST_WORD_MASK(size);
-		*p |= mask_to_set;
-	}
-}
+	long val;
 
-static inline void
-bitmap_clear(unsigned long *map, int start, int nr)
-{
-	unsigned long *p = map + BIT_WORD(start);
-	const int size = start + nr;
-	int bits_to_clear = BITS_PER_LONG - (start % BITS_PER_LONG);
-	unsigned long mask_to_clear = BITMAP_FIRST_WORD_MASK(start);
-
-	while (nr - bits_to_clear >= 0) {
-		*p &= ~mask_to_clear;
-		nr -= bits_to_clear;
-		bits_to_clear = BITS_PER_LONG;
-		mask_to_clear = ~0UL;
-		p++;
-	}
-	if (nr) {
-		mask_to_clear &= BITMAP_LAST_WORD_MASK(size);
-		*p &= ~mask_to_clear;
-	}
+	var += BIT_WORD(bit);
+	bit %= BITS_PER_LONG;
+	bit = (1UL << bit);
+
+	val = *var;
+	*var |= bit;
+
+	return !!(val & bit);
 }
 
 enum {
@@ -400,7 +338,7 @@ enum {
 };
 
 static inline int
-__reg_op(unsigned long *bitmap, int pos, int order, int reg_op)
+linux_reg_op(unsigned long *bitmap, int pos, int order, int reg_op)
 {
         int nbits_reg;
         int index;
@@ -444,70 +382,18 @@ __reg_op(unsigned long *bitmap, int pos, int order, int reg_op)
         return ret;
 }
 
-static inline int 
-bitmap_find_free_region(unsigned long *bitmap, int bits, int order)
-{
-        int pos;
-        int end;
-
-        for (pos = 0 ; (end = pos + (1 << order)) <= bits; pos = end) {
-                if (!__reg_op(bitmap, pos, order, REG_OP_ISFREE))
-                        continue;
-                __reg_op(bitmap, pos, order, REG_OP_ALLOC);
-                return pos;
-        }
-        return -ENOMEM;
-}
-
-static inline int
-bitmap_allocate_region(unsigned long *bitmap, int pos, int order)
-{
-        if (!__reg_op(bitmap, pos, order, REG_OP_ISFREE))
-                return -EBUSY;
-        __reg_op(bitmap, pos, order, REG_OP_ALLOC);
-        return 0;
-}
-
-static inline void 
-bitmap_release_region(unsigned long *bitmap, int pos, int order)
-{
-        __reg_op(bitmap, pos, order, REG_OP_RELEASE);
-}
-
 #define for_each_set_bit(bit, addr, size) \
 	for ((bit) = find_first_bit((addr), (size));		\
 	     (bit) < (size);					\
 	     (bit) = find_next_bit((addr), (size), (bit) + 1))
 
-static inline unsigned
-bitmap_weight(unsigned long *bitmap, unsigned nbits)
-{
-	unsigned bit;
-	unsigned retval = 0;
-
-	for_each_set_bit(bit, bitmap, nbits)
-		retval++;
-	return (retval);
-}
 
-static inline int
-bitmap_equal(const unsigned long *pa,
-    const unsigned long *pb, unsigned bits)
+static inline uint64_t
+sign_extend64(uint64_t value, int index)
 {
-	unsigned x;
-	unsigned y = bits / BITS_PER_LONG;
- 
-	for (x = 0; x != y; x++) {
-		if (pa[x] != pb[x])
-			return (0);
-	}
+	uint8_t shift = 63 - index;
 
-	y = bits % BITS_PER_LONG;
-	if (y != 0) {
-		if ((pa[x] ^ pb[x]) & BITMAP_LAST_WORD_MASK(y))
-			return (0);
-	}
-	return (1);
+	return ((int64_t)(value << shift) >> shift);
 }
 
 #endif	/* _LINUX_BITOPS_H_ */
diff --git a/sys/compat/linuxkpi/common/include/linux/bottom_half.h b/sys/compat/linuxkpi/common/include/linux/bottom_half.h
new file mode 100644
index 00000000000..9f8dc02f279
--- /dev/null
+++ b/sys/compat/linuxkpi/common/include/linux/bottom_half.h
@@ -0,0 +1,34 @@
+/*-
+ * Copyright (c) 2017 Hans Petter Selasky
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice unmodified, this list of conditions, and the following
+ *    disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS OR
+ * IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+ * OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.
+ * IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT,
+ * INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT
+ * NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+ * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+ * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF
+ * THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ *
+ * $FreeBSD$
+ */
+#ifndef _LINUX_BOTTOM_HALF_H_
+#define	_LINUX_BOTTOM_HALF_H_
+
+extern void local_bh_enable(void);
+extern void local_bh_disable(void);
+
+#endif					/* _LINUX_BOTTOM_HALF_H_ */
diff --git a/sys/compat/linuxkpi/common/include/linux/cdev.h b/sys/compat/linuxkpi/common/include/linux/cdev.h
index 856307c1457..3207d778035 100644
--- a/sys/compat/linuxkpi/common/include/linux/cdev.h
+++ b/sys/compat/linuxkpi/common/include/linux/cdev.h
@@ -95,7 +95,6 @@ cdev_add(struct linux_cdev *cdev, dev_t dev, unsigned count)
 	args.mda_gid = 0;
 	args.mda_mode = 0700;
 	args.mda_si_drv1 = cdev;
-	args.mda_unit = dev;
 
 	error = make_dev_s(&args, &cdev->cdev, "%s",
 	    kobject_name(&cdev->kobj));
@@ -121,7 +120,6 @@ cdev_add_ext(struct linux_cdev *cdev, dev_t dev, uid_t uid, gid_t gid, int mode)
 	args.mda_gid = gid;
 	args.mda_mode = mode;
 	args.mda_si_drv1 = cdev;
-	args.mda_unit = dev;
 
 	error = make_dev_s(&args, &cdev->cdev, "%s/%d",
 	    kobject_name(&cdev->kobj), MINOR(dev));
diff --git a/sys/compat/linuxkpi/common/include/linux/clocksource.h b/sys/compat/linuxkpi/common/include/linux/clocksource.h
index 5e2cd5ee95e..f775aa6bb32 100644
--- a/sys/compat/linuxkpi/common/include/linux/clocksource.h
+++ b/sys/compat/linuxkpi/common/include/linux/clocksource.h
@@ -33,9 +33,6 @@
 
 #include <asm/types.h>
 
-#define	CLOCKSOURCE_MASK(x) ((cycle_t)(-1ULL >> ((-(x)) & 63)))
-
-/* clocksource cycle base type */
-typedef u64 cycle_t;
+#define	CLOCKSOURCE_MASK(x) ((u64)(-1ULL >> ((-(x)) & 63)))
 
 #endif					/* _LINUX_CLOCKSOURCE_H */
diff --git a/sys/compat/linuxkpi/common/include/linux/compat.h b/sys/compat/linuxkpi/common/include/linux/compat.h
index 01b8a85b976..62ea3363394 100644
--- a/sys/compat/linuxkpi/common/include/linux/compat.h
+++ b/sys/compat/linuxkpi/common/include/linux/compat.h
@@ -2,7 +2,7 @@
  * Copyright (c) 2010 Isilon Systems, Inc.
  * Copyright (c) 2010 iX Systems, Inc.
  * Copyright (c) 2010 Panasas, Inc.
- * Copyright (c) 2013, 2014 Mellanox Technologies, Ltd.
+ * Copyright (c) 2013-2017 Mellanox Technologies, Ltd.
  * All rights reserved.
  *
  * Redistribution and use in source and binary forms, with or without
@@ -31,10 +31,29 @@
 #ifndef	_LINUX_COMPAT_H_
 #define	_LINUX_COMPAT_H_
 
+#include <sys/param.h>
+#include <sys/proc.h>
+#include <sys/malloc.h>
+
 struct thread;
 struct task_struct;
 
-void linux_set_current(struct thread *td, struct task_struct *t);
-void linux_clear_current(struct thread *td);
+extern int linux_alloc_current(struct thread *, int flags);
+extern void linux_free_current(struct task_struct *);
+
+static inline void
+linux_set_current(struct thread *td)
+{
+	if (__predict_false(td->td_lkpi_task == NULL))
+		linux_alloc_current(td, M_WAITOK);
+}
+
+static inline int
+linux_set_current_flags(struct thread *td, int flags)
+{
+	if (__predict_false(td->td_lkpi_task == NULL))
+		return (linux_alloc_current(td, flags));
+	return (0);
+}
 
 #endif	/* _LINUX_COMPAT_H_ */
diff --git a/sys/compat/linuxkpi/common/include/linux/compiler.h b/sys/compat/linuxkpi/common/include/linux/compiler.h
index 1184fe965ef..ae60553de37 100644
--- a/sys/compat/linuxkpi/common/include/linux/compiler.h
+++ b/sys/compat/linuxkpi/common/include/linux/compiler.h
@@ -56,7 +56,9 @@
 #define	__devexit
 #define __exit
 #define	__rcu
-#define	__stringify(x)			#x
+#define	__malloc
+#define	___stringify(...)		#__VA_ARGS__
+#define	__stringify(...)		___stringify(__VA_ARGS__)
 #define	__attribute_const__		__attribute__((__const__))
 #undef __always_inline
 #define	__always_inline			inline
diff --git a/sys/compat/linuxkpi/common/include/linux/completion.h b/sys/compat/linuxkpi/common/include/linux/completion.h
index 73c1a99dfc4..656b54164cb 100644
--- a/sys/compat/linuxkpi/common/include/linux/completion.h
+++ b/sys/compat/linuxkpi/common/include/linux/completion.h
@@ -32,7 +32,6 @@
 #define	_LINUX_COMPLETION_H_
 
 #include <linux/errno.h>
-#include <linux/wait.h>
 
 struct completion {
 	unsigned int done;
@@ -62,8 +61,8 @@ struct completion {
 	linux_completion_done(c)
 
 extern void linux_complete_common(struct completion *, int);
-extern long linux_wait_for_common(struct completion *, int);
-extern long linux_wait_for_timeout_common(struct completion *, long, int);
+extern int linux_wait_for_common(struct completion *, int);
+extern int linux_wait_for_timeout_common(struct completion *, int, int);
 extern int linux_try_wait_for_completion(struct completion *);
 extern int linux_completion_done(struct completion *);
 
diff --git a/sys/compat/linuxkpi/common/include/linux/device.h b/sys/compat/linuxkpi/common/include/linux/device.h
index 19e9c6eddcc..edc6cd8d9e0 100644
--- a/sys/compat/linuxkpi/common/include/linux/device.h
+++ b/sys/compat/linuxkpi/common/include/linux/device.h
@@ -49,16 +49,47 @@
 enum irqreturn	{ IRQ_NONE = 0, IRQ_HANDLED, IRQ_WAKE_THREAD, };
 typedef enum irqreturn	irqreturn_t;
 
+struct device;
+struct fwnode_handle;
+
 struct class {
 	const char	*name;
 	struct module	*owner;
 	struct kobject	kobj;
 	devclass_t	bsdclass;
+	const struct dev_pm_ops *pm;
 	void		(*class_release)(struct class *class);
 	void		(*dev_release)(struct device *dev);
 	char *		(*devnode)(struct device *dev, umode_t *mode);
 };
 
+struct dev_pm_ops {
+	int (*suspend)(struct device *dev);
+	int (*suspend_late)(struct device *dev);
+	int (*resume)(struct device *dev);
+	int (*resume_early)(struct device *dev);
+	int (*freeze)(struct device *dev);
+	int (*freeze_late)(struct device *dev);
+	int (*thaw)(struct device *dev);
+	int (*thaw_early)(struct device *dev);
+	int (*poweroff)(struct device *dev);
+	int (*poweroff_late)(struct device *dev);
+	int (*restore)(struct device *dev);
+	int (*restore_early)(struct device *dev);
+	int (*runtime_suspend)(struct device *dev);
+	int (*runtime_resume)(struct device *dev);
+	int (*runtime_idle)(struct device *dev);
+};
+
+struct device_driver {
+	const char	*name;
+	const struct dev_pm_ops *pm;
+};
+
+struct device_type {
+	const char	*name;
+};
+
 struct device {
 	struct device	*parent;
 	struct list_head irqents;
@@ -71,6 +102,8 @@ struct device {
 	 * done somewhere else.
 	 */
 	bool		bsddev_attached_here;
+	struct device_driver *driver;
+	struct device_type *type;
 	dev_t		devt;
 	struct class	*class;
 	void		(*release)(struct device *dev);
@@ -82,6 +115,10 @@ struct device {
 	unsigned int	msix;
 	unsigned int	msix_max;
 	const struct attribute_group **groups;
+	struct fwnode_handle *fwnode;
+
+	spinlock_t	devres_lock;
+	struct list_head devres_head;
 };
 
 extern struct device linux_root_device;
@@ -111,7 +148,13 @@ struct device_attribute {
 
 #define	DEVICE_ATTR(_name, _mode, _show, _store)			\
 	struct device_attribute dev_attr_##_name =			\
-	    { { #_name, NULL, _mode }, _show, _store }
+	    __ATTR(_name, _mode, _show, _store)
+#define	DEVICE_ATTR_RO(_name)						\
+	struct device_attribute dev_attr_##_name = __ATTR_RO(_name)
+#define	DEVICE_ATTR_WO(_name)						\
+	struct device_attribute dev_attr_##_name = __ATTR_WO(_name)
+#define	DEVICE_ATTR_RW(_name)						\
+	struct device_attribute dev_attr_##_name = __ATTR_RW(_name)
 
 /* Simple class attribute that is just a static string */
 struct class_attribute_string {
@@ -139,9 +182,22 @@ show_class_attr_string(struct class *class,
 #define	dev_warn(dev, fmt, ...)	device_printf((dev)->bsddev, fmt, ##__VA_ARGS__)
 #define	dev_info(dev, fmt, ...)	device_printf((dev)->bsddev, fmt, ##__VA_ARGS__)
 #define	dev_notice(dev, fmt, ...)	device_printf((dev)->bsddev, fmt, ##__VA_ARGS__)
+#define	dev_dbg(dev, fmt, ...)	do { } while (0)
 #define	dev_printk(lvl, dev, fmt, ...)					\
 	    device_printf((dev)->bsddev, fmt, ##__VA_ARGS__)
 
+#define	dev_err_ratelimited(dev, ...) do {	\
+	static linux_ratelimit_t __ratelimited;	\
+	if (linux_ratelimited(&__ratelimited))	\
+		dev_err(dev, __VA_ARGS__);	\
+} while (0)
+
+#define	dev_warn_ratelimited(dev, ...) do {	\
+	static linux_ratelimit_t __ratelimited;	\
+	if (linux_ratelimited(&__ratelimited))	\
+		dev_warn(dev, __VA_ARGS__);	\
+} while (0)
+
 static inline void *
 dev_get_drvdata(const struct device *dev)
 {
@@ -241,6 +297,9 @@ device_initialize(struct device *dev)
 	dev->bsddev = bsddev;
 	MPASS(dev->bsddev != NULL);
 	kobject_init(&dev->kobj, &linux_dev_ktype);
+
+	spin_lock_init(&dev->devres_lock);
+	INIT_LIST_HEAD(&dev->devres_head);
 }
 
 static inline int
@@ -316,13 +375,20 @@ device_create_with_groups(struct class *class,
 	return dev;
 }
 
+static inline bool
+device_is_registered(struct device *dev)
+{
+
+	return (dev->bsddev != NULL);
+}
+
 static inline int
 device_register(struct device *dev)
 {
 	device_t bsddev = NULL;
 	int unit = -1;
 
-	if (dev->bsddev != NULL)
+	if (device_is_registered(dev))
 		goto done;
 
 	if (dev->devt) {
@@ -413,7 +479,7 @@ class_create(struct module *owner, const char *name)
 
 	class = kzalloc(sizeof(*class), M_WAITOK);
 	class->owner = owner;
-	class->name= name;
+	class->name = name;
 	class->class_release = linux_class_kfree;
 	error = class_register(class);
 	if (error) {
diff --git a/sys/compat/linuxkpi/common/include/linux/dma-mapping.h b/sys/compat/linuxkpi/common/include/linux/dma-mapping.h
index 5f7e7129861..24508f68357 100644
--- a/sys/compat/linuxkpi/common/include/linux/dma-mapping.h
+++ b/sys/compat/linuxkpi/common/include/linux/dma-mapping.h
@@ -129,8 +129,10 @@ dma_alloc_coherent(struct device *dev, size_t size, dma_addr_t *dma_handle,
 
 	if (dev != NULL && dev->dma_mask)
 		high = *dev->dma_mask;
-	else
+	else if (flag & GFP_DMA32)
 		high = BUS_SPACE_MAXADDR_32BIT;
+	else
+		high = BUS_SPACE_MAXADDR;
 	align = PAGE_SIZE << get_order(size);
 	mem = (void *)kmem_alloc_contig(kmem_arena, size, flag, 0, high, align,
 	    0, VM_MEMATTR_DEFAULT);
diff --git a/sys/compat/linuxkpi/common/include/linux/etherdevice.h b/sys/compat/linuxkpi/common/include/linux/etherdevice.h
index 2c4a4887b28..2aa4cfa337f 100644
--- a/sys/compat/linuxkpi/common/include/linux/etherdevice.h
+++ b/sys/compat/linuxkpi/common/include/linux/etherdevice.h
@@ -100,6 +100,12 @@ eth_broadcast_addr(u8 *pa)
 }
 
 static inline void
+eth_zero_addr(u8 *pa)
+{
+	memset(pa, 0, 6);
+}
+
+static inline void
 random_ether_addr(u8 * dst)
 {
 	if (read_random(dst, 6) == 0)
diff --git a/sys/compat/linuxkpi/common/include/linux/file.h b/sys/compat/linuxkpi/common/include/linux/file.h
index 559ac043768..41fab51a696 100644
--- a/sys/compat/linuxkpi/common/include/linux/file.h
+++ b/sys/compat/linuxkpi/common/include/linux/file.h
@@ -2,7 +2,7 @@
  * Copyright (c) 2010 Isilon Systems, Inc.
  * Copyright (c) 2010 iX Systems, Inc.
  * Copyright (c) 2010 Panasas, Inc.
- * Copyright (c) 2013-2015 Mellanox Technologies, Ltd.
+ * Copyright (c) 2013-2017 Mellanox Technologies, Ltd.
  * All rights reserved.
  *
  * Redistribution and use in source and binary forms, with or without
@@ -39,6 +39,7 @@
 #include <sys/proc.h>
 
 #include <linux/fs.h>
+#include <linux/slab.h>
 
 struct linux_file;
 
@@ -52,26 +53,38 @@ linux_fget(unsigned int fd)
 	cap_rights_t rights;
 	struct file *file;
 
+	/* lookup file pointer by file descriptor index */
 	if (fget_unlocked(curthread->td_proc->p_fd, fd,
-	    cap_rights_init(&rights), &file, NULL) != 0) {
+	    cap_rights_init(&rights), &file, NULL) != 0)
+		return (NULL);
+
+	/* check if file handle really belongs to us */
+	if (file->f_data == NULL ||
+	    file->f_ops != &linuxfileops) {
+		fdrop(file, curthread);
 		return (NULL);
 	}
-	return (struct linux_file *)file->f_data;
+	return ((struct linux_file *)file->f_data);
 }
 
+extern void linux_file_free(struct linux_file *filp);
+
 static inline void
 fput(struct linux_file *filp)
 {
-	if (filp->_file == NULL) {
-		kfree(filp);
-		return;
-	}
-	if (refcount_release(&filp->_file->f_count)) {
-		_fdrop(filp->_file, curthread);
-		kfree(filp);
+	if (refcount_release(filp->_file == NULL ?
+	    &filp->f_count : &filp->_file->f_count)) {
+		linux_file_free(filp);
 	}
 }
 
+static inline unsigned int
+file_count(struct linux_file *filp)
+{
+	return (filp->_file == NULL ?
+	    filp->f_count : filp->_file->f_count);
+}
+
 static inline void
 put_unused_fd(unsigned int fd)
 {
@@ -105,6 +118,10 @@ fd_install(unsigned int fd, struct linux_file *filp)
 	} else {
 		filp->_file = file;
 		finit(file, filp->f_mode, DTYPE_DEV, filp, &linuxfileops);
+
+		/* transfer reference count from "filp" to "file" */
+		while (refcount_release(&filp->f_count) == 0)
+			refcount_acquire(&file->f_count);
 	}
 
 	/* drop the extra reference */
@@ -141,18 +158,18 @@ get_unused_fd_flags(int flags)
 	return fd;
 }
 
+extern struct linux_file *linux_file_alloc(void);
+
 static inline struct linux_file *
 alloc_file(int mode, const struct file_operations *fops)
 {
 	struct linux_file *filp;
 
-	filp = kzalloc(sizeof(*filp), GFP_KERNEL);
-	if (filp == NULL)
-		return (NULL);
+	filp = linux_file_alloc();
 	filp->f_op = fops;
 	filp->f_mode = mode;
 
-	return filp;
+	return (filp);
 }
 
 struct fd {
@@ -170,7 +187,7 @@ static inline struct fd fdget(unsigned int fd)
 	return (struct fd){f};
 }
 
-#define	file	linux_file
-#define	fget	linux_fget
+#define	file		linux_file
+#define	fget(...)	linux_fget(__VA_ARGS__)
 
 #endif	/* _LINUX_FILE_H_ */
diff --git a/sys/compat/linuxkpi/common/include/linux/fs.h b/sys/compat/linuxkpi/common/include/linux/fs.h
index fd59fcf37f1..04fc78df491 100644
--- a/sys/compat/linuxkpi/common/include/linux/fs.h
+++ b/sys/compat/linuxkpi/common/include/linux/fs.h
@@ -2,7 +2,7 @@
  * Copyright (c) 2010 Isilon Systems, Inc.
  * Copyright (c) 2010 iX Systems, Inc.
  * Copyright (c) 2010 Panasas, Inc.
- * Copyright (c) 2013-2016 Mellanox Technologies, Ltd.
+ * Copyright (c) 2013-2017 Mellanox Technologies, Ltd.
  * All rights reserved.
  *
  * Redistribution and use in source and binary forms, with or without
@@ -41,6 +41,7 @@
 #include <linux/types.h>
 #include <linux/wait.h>
 #include <linux/semaphore.h>
+#include <linux/spinlock.h>
 
 struct module;
 struct kiocb;
@@ -52,9 +53,11 @@ struct pipe_inode_info;
 struct vm_area_struct;
 struct poll_table_struct;
 struct files_struct;
+struct pfs_node;
 
 #define	inode	vnode
 #define	i_cdev	v_rdev
+#define	i_private v_data
 
 #define	S_IRUGO	(S_IRUSR | S_IRGRP | S_IROTH)
 #define	S_IWUGO	(S_IWUSR | S_IWGRP | S_IWOTH)
@@ -64,10 +67,22 @@ typedef struct files_struct *fl_owner_t;
 
 struct dentry {
 	struct inode	*d_inode;
+	struct pfs_node	*d_pfs_node;
 };
 
 struct file_operations;
 
+struct linux_file_wait_queue {
+	struct wait_queue wq;
+	struct wait_queue_head *wqh;
+	atomic_t state;
+#define	LINUX_FWQ_STATE_INIT 0
+#define	LINUX_FWQ_STATE_NOT_READY 1
+#define	LINUX_FWQ_STATE_QUEUED 2
+#define	LINUX_FWQ_STATE_READY 3
+#define	LINUX_FWQ_STATE_MAX 4
+};
+
 struct linux_file {
 	struct file	*_file;
 	const struct file_operations	*f_op;
@@ -79,6 +94,21 @@ struct linux_file {
 	struct selinfo	f_selinfo;
 	struct sigio	*f_sigio;
 	struct vnode	*f_vnode;
+#define	f_inode	f_vnode
+	volatile u_int	f_count;
+
+	/* anonymous shmem object */
+	vm_object_t	f_shmem;
+
+	/* kqfilter support */
+	int		f_kqflags;
+#define	LINUX_KQ_FLAG_HAS_READ (1 << 0)
+#define	LINUX_KQ_FLAG_HAS_WRITE (1 << 1)
+#define	LINUX_KQ_FLAG_NEED_READ (1 << 2)
+#define	LINUX_KQ_FLAG_NEED_WRITE (1 << 3)
+	/* protects f_selinfo.si_note */
+	spinlock_t	f_kqlock;
+	struct linux_file_wait_queue f_wait_queue;
 };
 
 #define	file		linux_file
@@ -107,6 +137,7 @@ struct file_operations {
 	ssize_t (*write)(struct file *, const char __user *, size_t, loff_t *);
 	unsigned int (*poll) (struct file *, struct poll_table_struct *);
 	long (*unlocked_ioctl)(struct file *, unsigned int, unsigned long);
+	long (*compat_ioctl)(struct file *, unsigned int, unsigned long);
 	int (*mmap)(struct file *, struct vm_area_struct *);
 	int (*open)(struct inode *, struct file *);
 	int (*release)(struct inode *, struct file *);
@@ -127,7 +158,6 @@ struct file_operations {
 	int (*readdir)(struct file *, void *, filldir_t);
 	int (*ioctl)(struct inode *, struct file *, unsigned int,
 	    unsigned long);
-	long (*compat_ioctl)(struct file *, unsigned int, unsigned long);
 	int (*flush)(struct file *, fl_owner_t id);
 	int (*fsync)(struct file *, struct dentry *, int datasync);
 	int (*aio_fsync)(struct kiocb *, int datasync);
@@ -145,7 +175,8 @@ struct file_operations {
 	int (*setlease)(struct file *, long, struct file_lock **);
 #endif
 };
-#define	fops_get(fops)	(fops)
+#define	fops_get(fops)		(fops)
+#define	replace_fops(f, fops)	((f)->f_op = (fops))
 
 #define	FMODE_READ	FREAD
 #define	FMODE_WRITE	FWRITE
@@ -213,11 +244,15 @@ nonseekable_open(struct inode *inode, struct file *filp)
 	return 0;
 }
 
-static inline dev_t
-iminor(struct inode *inode)
+extern unsigned int linux_iminor(struct inode *);
+#define	iminor(...) linux_iminor(__VA_ARGS__)
+
+static inline struct linux_file *
+get_file(struct linux_file *f)
 {
 
-	return (minor(dev2unit(inode->v_rdev)));
+	refcount_acquire(f->_file == NULL ? &f->f_count : &f->_file->f_count);
+	return (f);
 }
 
 static inline struct inode *
@@ -242,7 +277,36 @@ iput(struct inode *inode)
 static inline loff_t 
 no_llseek(struct file *file, loff_t offset, int whence)
 {
-        return -ESPIPE;
+
+	return (-ESPIPE);
+}
+
+static inline loff_t
+noop_llseek(struct linux_file *file, loff_t offset, int whence)
+{
+
+	return (file->_file->f_offset);
 }
 
+/* Shared memory support */
+unsigned long linux_invalidate_mapping_pages(vm_object_t, pgoff_t, pgoff_t);
+struct page *linux_shmem_read_mapping_page_gfp(vm_object_t, int, gfp_t);
+struct linux_file *linux_shmem_file_setup(const char *, loff_t, unsigned long);
+void linux_shmem_truncate_range(vm_object_t, loff_t, loff_t);
+
+#define	invalidate_mapping_pages(...) \
+  linux_invalidate_mapping_pages(__VA_ARGS__)
+
+#define	shmem_read_mapping_page(...) \
+  linux_shmem_read_mapping_page_gfp(__VA_ARGS__, 0)
+
+#define	shmem_read_mapping_page_gfp(...) \
+  linux_shmem_read_mapping_page_gfp(__VA_ARGS__)
+
+#define	shmem_file_setup(...) \
+  linux_shmem_file_setup(__VA_ARGS__)
+
+#define	shmem_truncate_range(...) \
+  linux_shmem_truncate_range(__VA_ARGS__)
+
 #endif /* _LINUX_FS_H_ */
diff --git a/sys/compat/linuxkpi/common/include/linux/gfp.h b/sys/compat/linuxkpi/common/include/linux/gfp.h
index 1c73022ff13..3d9ddbaef87 100644
--- a/sys/compat/linuxkpi/common/include/linux/gfp.h
+++ b/sys/compat/linuxkpi/common/include/linux/gfp.h
@@ -2,7 +2,7 @@
  * Copyright (c) 2010 Isilon Systems, Inc.
  * Copyright (c) 2010 iX Systems, Inc.
  * Copyright (c) 2010 Panasas, Inc.
- * Copyright (c) 2013 Mellanox Technologies, Ltd.
+ * Copyright (c) 2013-2017 Mellanox Technologies, Ltd.
  * All rights reserved.
  *
  * Redistribution and use in source and binary forms, with or without
@@ -53,7 +53,7 @@
 #define	__GFP_IO	0
 #define	__GFP_NO_KSWAPD	0
 #define	__GFP_WAIT	M_WAITOK
-#define	__GFP_DMA32     0
+#define	__GFP_DMA32	(1U << 24) /* LinuxKPI only */
 
 #define	GFP_NOWAIT	M_NOWAIT
 #define	GFP_ATOMIC	(M_NOWAIT | M_USE_RESERVE)
@@ -63,99 +63,110 @@
 #define	GFP_HIGHUSER_MOVABLE	M_WAITOK
 #define	GFP_IOFS	M_NOWAIT
 #define	GFP_NOIO	M_NOWAIT
-#define	GFP_DMA32	0
+#define	GFP_DMA32	__GFP_DMA32
 #define	GFP_TEMPORARY	M_NOWAIT
+#define	GFP_NATIVE_MASK	(M_NOWAIT | M_WAITOK | M_USE_RESERVE | M_ZERO)
 
-static inline void *
-page_address(struct page *page)
+/*
+ * Resolve a page into a virtual address:
+ *
+ * NOTE: This function only works for pages allocated by the kernel.
+ */
+extern void *linux_page_address(struct page *);
+
+#define	page_address(page) linux_page_address(page)
+
+/*
+ * Page management for unmapped pages:
+ */
+extern vm_page_t linux_alloc_pages(gfp_t flags, unsigned int order);
+extern void linux_free_pages(vm_page_t page, unsigned int order);
+
+static inline struct page *
+alloc_page(gfp_t flags)
 {
 
-	if (page->object != kmem_object && page->object != kernel_object)
-		return (NULL);
-	return ((void *)(uintptr_t)(VM_MIN_KERNEL_ADDRESS +
-	    IDX_TO_OFF(page->pindex)));
+	return (linux_alloc_pages(flags, 0));
 }
 
-static inline unsigned long
-linux_get_page(gfp_t mask)
+static inline struct page *
+alloc_pages(gfp_t flags, unsigned int order)
 {
 
-	return kmem_malloc(kmem_arena, PAGE_SIZE, mask);
+	return (linux_alloc_pages(flags, order));
 }
 
-#define	get_zeroed_page(mask)	linux_get_page((mask) | M_ZERO)
-#define	alloc_page(mask)	virt_to_page(linux_get_page((mask)))
-#define	__get_free_page(mask)	linux_get_page((mask))
+static inline struct page *
+alloc_pages_node(int node_id, gfp_t flags, unsigned int order)
+{
+
+	return (linux_alloc_pages(flags, order));
+}
 
 static inline void
-free_page(unsigned long page)
+__free_pages(struct page *page, unsigned int order)
 {
 
-	if (page == 0)
-		return;
-	kmem_free(kmem_arena, page, PAGE_SIZE);
+	linux_free_pages(page, order);
 }
 
 static inline void
-__free_page(struct page *m)
+__free_page(struct page *page)
 {
 
-	if (m->object != kmem_object)
-		panic("__free_page:  Freed page %p not allocated via wrappers.",
-		    m);
-	kmem_free(kmem_arena, (vm_offset_t)page_address(m), PAGE_SIZE);
+	linux_free_pages(page, 0);
 }
 
-static inline void
-__free_pages(struct page *m, unsigned int order)
+/*
+ * Page management for mapped pages:
+ */
+extern vm_offset_t linux_alloc_kmem(gfp_t flags, unsigned int order);
+extern void linux_free_kmem(vm_offset_t, unsigned int order);
+
+static inline vm_offset_t
+get_zeroed_page(gfp_t flags)
 {
-	size_t size;
 
-	if (m == NULL)
-		return;
-	size = PAGE_SIZE << order;
-	kmem_free(kmem_arena, (vm_offset_t)page_address(m), size);
+	return (linux_alloc_kmem(flags | __GFP_ZERO, 0));
 }
 
-static inline void free_pages(uintptr_t addr, unsigned int order)
+static inline vm_offset_t
+__get_free_page(gfp_t flags)
 {
-	if (addr == 0)
-		return;
-	__free_pages(virt_to_page((void *)addr), order);
+
+	return (linux_alloc_kmem(flags, 0));
 }
 
-/*
- * Alloc pages allocates directly from the buddy allocator on linux so
- * order specifies a power of two bucket of pages and the results
- * are expected to be aligned on the size as well.
- */
-static inline struct page *
-alloc_pages(gfp_t gfp_mask, unsigned int order)
+static inline vm_offset_t
+__get_free_pages(gfp_t flags, unsigned int order)
 {
-	unsigned long page;
-	size_t size;
-
-	size = PAGE_SIZE << order;
-	page = kmem_alloc_contig(kmem_arena, size, gfp_mask,
-	    0, ~(vm_paddr_t)0, size, 0, VM_MEMATTR_DEFAULT);
-	if (page == 0)
-		return (NULL);
-        return (virt_to_page(page));
+
+	return (linux_alloc_kmem(flags, order));
 }
 
-static inline uintptr_t __get_free_pages(gfp_t gfp_mask, unsigned int order)
+static inline void
+free_pages(uintptr_t addr, unsigned int order)
 {
-	struct page *page;
+	if (addr == 0)
+		return;
 
-	page = alloc_pages(gfp_mask, order);
-	if (page == NULL)
-		return (0);
-	return ((uintptr_t)page_address(page));
+	linux_free_kmem(addr, order);
 }
 
-#define alloc_pages_node(node, mask, order)     alloc_pages(mask, order)
+static inline void
+free_page(uintptr_t addr)
+{
+	if (addr == 0)
+		return;
 
-#define kmalloc_node(chunk, mask, node)         kmalloc(chunk, mask)
+	linux_free_kmem(addr, 0);
+}
+
+static inline bool
+gfpflags_allow_blocking(const gfp_t gfp_flags)
+{
+	return ((gfp_flags & (M_WAITOK | M_NOWAIT)) == M_WAITOK);
+}
 
 #define	SetPageReserved(page)	do { } while (0)	/* NOP */
 #define	ClearPageReserved(page)	do { } while (0)	/* NOP */
diff --git a/sys/compat/linuxkpi/common/include/linux/hrtimer.h b/sys/compat/linuxkpi/common/include/linux/hrtimer.h
new file mode 100644
index 00000000000..651f9ac1d95
--- /dev/null
+++ b/sys/compat/linuxkpi/common/include/linux/hrtimer.h
@@ -0,0 +1,78 @@
+/*-
+ * Copyright (c) 2017 Mark Johnston <markj@FreeBSD.org>
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice unmodified, this list of conditions, and the following
+ *    disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS OR
+ * IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+ * OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.
+ * IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT,
+ * INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT
+ * NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+ * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+ * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF
+ * THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ *
+ * $FreeBSD$
+ */
+
+#ifndef _LINUX_HRTIMER_H_
+#define	_LINUX_HRTIMER_H_
+
+#include <sys/_callout.h>
+#include <sys/_mutex.h>
+
+#include <linux/ktime.h>
+#include <linux/timer.h>
+
+enum hrtimer_mode {
+	HRTIMER_MODE_REL,
+};
+
+enum hrtimer_restart {
+	HRTIMER_RESTART,
+	HRTIMER_NORESTART,
+};
+
+struct hrtimer {
+	enum hrtimer_restart (*function)(struct hrtimer *);
+	struct mtx mtx;
+	struct callout callout;
+};
+
+#define	hrtimer_active(hrtimer)	linux_hrtimer_active(hrtimer)
+#define	hrtimer_cancel(hrtimer)	linux_hrtimer_cancel(hrtimer)
+#define	hrtimer_init(hrtimer, clock, mode) do {			\
+	CTASSERT((clock) == CLOCK_MONOTONIC);			\
+	CTASSERT((mode) == HRTIMER_MODE_REL);			\
+	linux_hrtimer_init(hrtimer);				\
+} while (0)
+#define	hrtimer_set_expires(hrtimer, time)			\
+	linux_hrtimer_set_expires(hrtimer, time)
+#define	hrtimer_start(hrtimer, time, mode) do {			\
+	CTASSERT((mode) == HRTIMER_MODE_REL);			\
+	linux_hrtimer_start(hrtimer, time);			\
+} while (0)
+#define	hrtimer_start_range_ns(hrtimer, time, prec, mode) do {	\
+	CTASSERT((mode) == HRTIMER_MODE_REL);			\
+	linux_hrtimer_start_range_ns(hrtimer, time, prec);	\
+} while (0)
+
+bool	linux_hrtimer_active(struct hrtimer *);
+int	linux_hrtimer_cancel(struct hrtimer *);
+void	linux_hrtimer_init(struct hrtimer *);
+void	linux_hrtimer_set_expires(struct hrtimer *, ktime_t);
+void	linux_hrtimer_start(struct hrtimer *, ktime_t);
+void	linux_hrtimer_start_range_ns(struct hrtimer *, ktime_t, int64_t);
+
+#endif /* _LINUX_HRTIMER_H_ */
diff --git a/sys/compat/linuxkpi/common/include/linux/idr.h b/sys/compat/linuxkpi/common/include/linux/idr.h
index 234fcb627cf..013b47c708d 100644
--- a/sys/compat/linuxkpi/common/include/linux/idr.h
+++ b/sys/compat/linuxkpi/common/include/linux/idr.h
@@ -75,9 +75,8 @@ struct idr {
 	SYSINIT(name##_ida_sysinit, SI_SUB_DRIVERS, SI_ORDER_FIRST,	\
 	    ida_init, &(name))
 
-#define	idr_preload(x) do { } while (0)
-#define	idr_preload_end() do { } while (0)
-
+void	idr_preload(gfp_t gfp_mask);
+void	idr_preload_end(void);
 void	*idr_find(struct idr *idp, int id);
 void	*idr_get_next(struct idr *idp, int *nextid);
 int	idr_pre_get(struct idr *idp, gfp_t gfp_mask);
diff --git a/sys/compat/linuxkpi/common/include/linux/in.h b/sys/compat/linuxkpi/common/include/linux/in.h
index f390c1d6b37..96d66fbae2f 100644
--- a/sys/compat/linuxkpi/common/include/linux/in.h
+++ b/sys/compat/linuxkpi/common/include/linux/in.h
@@ -41,5 +41,7 @@
 
 #define	ipv4_is_zeronet(be)	IN_ZERONET(ntohl(be))
 #define	ipv4_is_loopback(be)	IN_LOOPBACK(ntohl(be))
+#define	ipv4_is_multicast(be)	IN_MULTICAST(ntohl(be))
+#define	ipv4_is_lbcast(be)	((be) == INADDR_BROADCAST)
 
 #endif	/* _LINUX_IN_H_ */
diff --git a/sys/compat/linuxkpi/common/include/linux/interrupt.h b/sys/compat/linuxkpi/common/include/linux/interrupt.h
index ed58518730b..6f7b96dbaa6 100644
--- a/sys/compat/linuxkpi/common/include/linux/interrupt.h
+++ b/sys/compat/linuxkpi/common/include/linux/interrupt.h
@@ -148,4 +148,25 @@ free_irq(unsigned int irq, void *device)
 	kfree(irqe);
 }
 
+/*
+ * LinuxKPI tasklet support
+ */
+typedef void tasklet_func_t(unsigned long);
+
+struct tasklet_struct {
+	TAILQ_ENTRY(tasklet_struct) entry;
+	tasklet_func_t *func;
+	unsigned long data;
+};
+
+#define	DECLARE_TASKLET(name, func, data)	\
+struct tasklet_struct name = { { NULL, NULL }, func, data }
+
+#define	tasklet_hi_schedule(t)	tasklet_schedule(t)
+
+extern void tasklet_schedule(struct tasklet_struct *);
+extern void tasklet_kill(struct tasklet_struct *);
+extern void tasklet_init(struct tasklet_struct *, tasklet_func_t *,
+    unsigned long data);
+
 #endif	/* _LINUX_INTERRUPT_H_ */
diff --git a/sys/compat/linuxkpi/common/include/linux/io-mapping.h b/sys/compat/linuxkpi/common/include/linux/io-mapping.h
index 8650dba65e6..12643be9c6e 100644
--- a/sys/compat/linuxkpi/common/include/linux/io-mapping.h
+++ b/sys/compat/linuxkpi/common/include/linux/io-mapping.h
@@ -28,52 +28,90 @@
  *
  * $FreeBSD$
  */
-#ifndef	_LINUX_IO_MAPPING_H_
+
+#ifndef _LINUX_IO_MAPPING_H_
 #define	_LINUX_IO_MAPPING_H_
 
+#include <sys/types.h>
+#include <machine/vm.h>
+
 #include <linux/types.h>
 #include <linux/io.h>
+#include <linux/slab.h>
 
-struct io_mapping;
+struct io_mapping {
+	unsigned long base;
+	unsigned long size;
+	void *mem;
+	vm_memattr_t attr;
+};
+
+static inline struct io_mapping *
+io_mapping_init_wc(struct io_mapping *mapping, resource_size_t base,
+    unsigned long size)
+{
+
+	mapping->base = base;
+	mapping->size = size;
+#ifdef VM_MEMATTR_WRITE_COMBINING
+	mapping->mem = ioremap_wc(base, size);
+	mapping->attr = VM_MEMATTR_WRITE_COMBINING;
+#else
+	mapping->mem = ioremap_nocache(base, size);
+	mapping->attr = VM_MEMATTR_UNCACHEABLE;
+#endif
+	return (mapping);
+}
 
 static inline struct io_mapping *
 io_mapping_create_wc(resource_size_t base, unsigned long size)
 {
+	struct io_mapping *mapping;
 
-	return ioremap_wc(base, size);
+	mapping = kmalloc(sizeof(*mapping), GFP_KERNEL);
+	if (mapping == NULL)
+		return (NULL);
+	return (io_mapping_init_wc(mapping, base, size));
+}
+
+static inline void
+io_mapping_fini(struct io_mapping *mapping)
+{
+
+	iounmap(mapping->mem);
 }
 
 static inline void
 io_mapping_free(struct io_mapping *mapping)
 {
 
-	iounmap(mapping);
+	io_mapping_fini(mapping->mem);
+	kfree(mapping);
 }
 
 static inline void *
 io_mapping_map_atomic_wc(struct io_mapping *mapping, unsigned long offset)
 {
 
-	return (((char *)mapping) + offset);
+	return ((char *)mapping->mem + offset);
 }
 
 static inline void
 io_mapping_unmap_atomic(void *vaddr)
 {
-
 }
 
 static inline void *
-io_mapping_map_wc(struct io_mapping *mapping, unsigned long offset)
+io_mapping_map_wc(struct io_mapping *mapping, unsigned long offset,
+    unsigned long size)
 {
 
-	return (((char *) mapping) + offset);
+	return ((char *)mapping->mem + offset);
 }
 
 static inline void
 io_mapping_unmap(void *vaddr)
 {
-
 }
 
-#endif	/* _LINUX_IO_MAPPING_H_ */
+#endif /* _LINUX_IO_MAPPING_H_ */
diff --git a/sys/compat/linuxkpi/common/include/linux/io.h b/sys/compat/linuxkpi/common/include/linux/io.h
index 09093467568..1958b7aef5c 100644
--- a/sys/compat/linuxkpi/common/include/linux/io.h
+++ b/sys/compat/linuxkpi/common/include/linux/io.h
@@ -36,6 +36,7 @@
 #include <sys/types.h>
 
 #include <linux/compiler.h>
+#include <linux/types.h>
 
 static inline uint32_t
 __raw_readl(const volatile void *addr)
@@ -108,6 +109,13 @@ ioread16(const volatile void *addr)
 	return *(const volatile uint16_t *)addr;
 }
 
+#undef ioread16be
+static inline uint16_t
+ioread16be(const volatile void *addr)
+{
+	return be16toh(*(const volatile uint16_t *)addr);
+}
+
 #undef ioread32
 static inline uint32_t
 ioread32(const volatile void *addr)
@@ -179,7 +187,7 @@ _outb(u_char data, u_int port)
 }
 #endif
 
-#if defined(__i386__) || defined(__amd64__)
+#if defined(__i386__) || defined(__amd64__) || defined(__powerpc__)
 void *_ioremap_attr(vm_paddr_t phys_addr, unsigned long size, int attr);
 #else
 #define	_ioremap_attr(...) NULL
diff --git a/sys/compat/linuxkpi/common/include/linux/jiffies.h b/sys/compat/linuxkpi/common/include/linux/jiffies.h
index 9a85f616152..78b96ec54fa 100644
--- a/sys/compat/linuxkpi/common/include/linux/jiffies.h
+++ b/sys/compat/linuxkpi/common/include/linux/jiffies.h
@@ -32,7 +32,6 @@
 #define	_LINUX_JIFFIES_H_
 
 #include <linux/types.h>
-#include <linux/kernel.h>
 #include <linux/time.h>
 
 #include <sys/time.h>
@@ -41,7 +40,7 @@
 
 #define jiffies                 ticks
 #define	jiffies_64		ticks
-#define jiffies_to_msecs(x)     (((int64_t)(x)) * 1000 / hz)
+#define	jiffies_to_msecs(x)     (((int64_t)(int)(x)) * 1000 / hz)
 
 #define	MAX_JIFFY_OFFSET	((INT_MAX >> 1) - 1)
 
@@ -51,6 +50,7 @@
 #define	time_before_eq(a, b)	time_after_eq(b, a)
 #define	time_in_range(a,b,c)	\
 	(time_after_eq(a,b) && time_before_eq(a,c))
+#define	time_is_after_eq_jiffies(a) time_after_eq(a, jiffies)
 
 #define	HZ	hz
 
@@ -68,11 +68,14 @@ timespec_to_jiffies(const struct timespec *ts)
 }
 
 static inline int
-msecs_to_jiffies(const u64 msec)
+msecs_to_jiffies(uint64_t msec)
 {
-	u64 result;
+	uint64_t msec_max, result;
 
-	result = howmany(msec * (u64)hz, 1000ULL);
+	msec_max = -1ULL / (uint64_t)hz;
+	if (msec > msec_max)
+		msec = msec_max;
+	result = howmany(msec * (uint64_t)hz, 1000ULL);
 	if (result > MAX_JIFFY_OFFSET)
 		result = MAX_JIFFY_OFFSET;
 
@@ -80,31 +83,65 @@ msecs_to_jiffies(const u64 msec)
 }
 
 static inline int
-usecs_to_jiffies(const u64 u)
+usecs_to_jiffies(uint64_t usec)
 {
-	u64 result;
+	uint64_t usec_max, result;
 
-	result = howmany(u * (u64)hz, 1000000ULL);
+	usec_max = -1ULL / (uint64_t)hz;
+	if (usec > usec_max)
+		usec = usec_max;
+	result = howmany(usec * (uint64_t)hz, 1000000ULL);
 	if (result > MAX_JIFFY_OFFSET)
 		result = MAX_JIFFY_OFFSET;
 
 	return ((int)result);
 }
 
-static inline u64
-nsecs_to_jiffies(const u64 n)
+static inline uint64_t
+nsecs_to_jiffies64(uint64_t nsec)
+{
+	uint64_t nsec_max, result;
+
+	nsec_max = -1ULL / (uint64_t)hz;
+	if (nsec > nsec_max)
+		nsec = nsec_max;
+	result = howmany(nsec * (uint64_t)hz, 1000000000ULL);
+	if (result > MAX_JIFFY_OFFSET)
+		result = MAX_JIFFY_OFFSET;
+
+	return (result);
+}
+
+static inline uint64_t
+nsecs_to_jiffies(uint64_t n)
 {
+
 	return (usecs_to_jiffies(howmany(n, 1000ULL)));
 }
 
-static inline u64
+static inline uint64_t
+jiffies_to_nsecs(int j)
+{
+
+	return ((1000000000ULL / hz) * (uint64_t)(unsigned int)j);
+}
+
+static inline uint64_t
+jiffies_to_usecs(int j)
+{
+
+	return ((1000000ULL / hz) * (uint64_t)(unsigned int)j);
+}
+
+static inline uint64_t
 get_jiffies_64(void)
 {
-	return ((u64)(unsigned)ticks);
+
+	return ((uint64_t)(unsigned int)ticks);
 }
 
 static inline int
-linux_timer_jiffies_until(unsigned long expires)
+linux_timer_jiffies_until(int expires)
 {
 	int delta = expires - jiffies;
 	/* guard against already expired values */
diff --git a/sys/compat/linuxkpi/common/include/linux/kdev_t.h b/sys/compat/linuxkpi/common/include/linux/kdev_t.h
index c0bb97e5ba4..9477ba739e3 100644
--- a/sys/compat/linuxkpi/common/include/linux/kdev_t.h
+++ b/sys/compat/linuxkpi/common/include/linux/kdev_t.h
@@ -31,9 +31,11 @@
 #ifndef	_LINUX_KDEV_T_H_
 #define	_LINUX_KDEV_T_H_
 
-#define MAJOR(dev)      major((dev))
-#define MINOR(dev)      minor((dev))
-#define MKDEV(ma, mi)   makedev((ma), (mi))
+#include <sys/types.h>
+
+#define MAJOR(dev)      major(dev)
+#define MINOR(dev)      minor(dev)
+#define MKDEV(ma, mi)   makedev(ma, mi)
 
 static inline uint16_t
 old_encode_dev(dev_t dev)
diff --git a/sys/compat/linuxkpi/common/include/linux/kernel.h b/sys/compat/linuxkpi/common/include/linux/kernel.h
index b813bd4f95e..c2641320871 100644
--- a/sys/compat/linuxkpi/common/include/linux/kernel.h
+++ b/sys/compat/linuxkpi/common/include/linux/kernel.h
@@ -41,14 +41,14 @@
 #include <sys/smp.h>
 #include <sys/stddef.h>
 #include <sys/syslog.h>
+#include <sys/time.h>
 
 #include <linux/bitops.h>
 #include <linux/compiler.h>
 #include <linux/errno.h>
-#include <linux/kthread.h>
+#include <linux/sched.h>
 #include <linux/types.h>
 #include <linux/jiffies.h>
-#include <linux/wait.h>
 #include <linux/log2.h> 
 #include <asm/byteorder.h>
 
@@ -86,7 +86,9 @@
 #define	S64_C(x) x ## LL
 #define	U64_C(x) x ## ULL
 
-#define	BUILD_BUG_ON(x)		CTASSERT(!(x))
+#define	BUILD_BUG_ON(x)			CTASSERT(!(x))
+#define	BUILD_BUG_ON_MSG(x, msg)	BUILD_BUG_ON(x)
+#define	BUILD_BUG_ON_NOT_POWER_OF_2(x)	BUILD_BUG_ON(!powerof2(x))
 
 #define	BUG()			panic("BUG at %s:%d", __FILE__, __LINE__)
 #define	BUG_ON(cond)		do {				\
@@ -118,6 +120,8 @@
       unlikely(__ret);						\
 })
 
+#define	oops_in_progress	SCHEDULER_STOPPED()
+
 #undef	ALIGN
 #define	ALIGN(x, y)		roundup2((x), (y))
 #undef PTR_ALIGN
@@ -224,6 +228,11 @@ scnprintf(char *buf, size_t size, const char *fmt, ...)
 	log_once(LOG_INFO, pr_fmt(fmt), ##__VA_ARGS__)
 #define pr_cont(fmt, ...) \
 	printk(KERN_CONT fmt, ##__VA_ARGS__)
+#define	pr_warn_ratelimited(...) do {		\
+	static linux_ratelimit_t __ratelimited;	\
+	if (linux_ratelimited(&__ratelimited))	\
+		pr_warning(__VA_ARGS__);	\
+} while (0)
 
 #ifndef WARN
 #define	WARN(condition, ...) ({			\
@@ -245,17 +254,106 @@ scnprintf(char *buf, size_t size, const char *fmt, ...)
 
 #define container_of(ptr, type, member)				\
 ({								\
-	__typeof(((type *)0)->member) *_p = (ptr);		\
-	(type *)((char *)_p - offsetof(type, member));		\
+	const __typeof(((type *)0)->member) *__p = (ptr);	\
+	(type *)((uintptr_t)__p - offsetof(type, member));	\
 })
   
 #define	ARRAY_SIZE(x)	(sizeof(x) / sizeof((x)[0]))
 
-#define	simple_strtoul(...) strtoul(__VA_ARGS__)
-#define	simple_strtol(...) strtol(__VA_ARGS__)
-#define	kstrtol(a,b,c) ({*(c) = strtol(a,0,b); 0;})
-#define	kstrtoint(a,b,c) ({*(c) = strtol(a,0,b); 0;})
-#define	kstrtouint(a,b,c) ({*(c) = strtol(a,0,b); 0;})
+#define	u64_to_user_ptr(val)	((void *)(uintptr_t)(val))
+
+static inline unsigned long long
+simple_strtoull(const char *cp, char **endp, unsigned int base)
+{
+	return (strtouq(cp, endp, base));
+}
+
+static inline long long
+simple_strtoll(const char *cp, char **endp, unsigned int base)
+{
+	return (strtoq(cp, endp, base));
+}
+
+static inline unsigned long
+simple_strtoul(const char *cp, char **endp, unsigned int base)
+{
+	return (strtoul(cp, endp, base));
+}
+
+static inline long
+simple_strtol(const char *cp, char **endp, unsigned int base)
+{
+	return (strtol(cp, endp, base));
+}
+
+static inline int
+kstrtoul(const char *cp, unsigned int base, unsigned long *res)
+{
+	char *end;
+
+	*res = strtoul(cp, &end, base);
+
+	if (*cp == 0 || *end != 0)
+		return (-EINVAL);
+	return (0);
+}
+
+static inline int
+kstrtol(const char *cp, unsigned int base, long *res)
+{
+	char *end;
+
+	*res = strtol(cp, &end, base);
+
+	if (*cp == 0 || *end != 0)
+		return (-EINVAL);
+	return (0);
+}
+
+static inline int
+kstrtoint(const char *cp, unsigned int base, int *res)
+{
+	char *end;
+	long temp;
+
+	*res = temp = strtol(cp, &end, base);
+
+	if (*cp == 0 || *end != 0)
+		return (-EINVAL);
+	if (temp != (int)temp)
+		return (-ERANGE);
+	return (0);
+}
+
+static inline int
+kstrtouint(const char *cp, unsigned int base, unsigned int *res)
+{
+	char *end;
+	unsigned long temp;
+
+	*res = temp = strtoul(cp, &end, base);
+
+	if (*cp == 0 || *end != 0)
+		return (-EINVAL);
+	if (temp != (unsigned int)temp)
+		return (-ERANGE);
+	return (0);
+}
+
+static inline int
+kstrtou32(const char *cp, unsigned int base, u32 *res)
+{
+	char *end;
+	unsigned long temp;
+
+	*res = temp = strtoul(cp, &end, base);
+
+	if (*cp == 0 || *end != 0)
+		return (-EINVAL);
+	if (temp != (u32)temp)
+		return (-ERANGE);
+	return (0);
+}
 
 #define min(x, y)	((x) < (y) ? (x) : (y))
 #define max(x, y)	((x) > (y) ? (x) : (y))
@@ -331,4 +429,15 @@ abs64(int64_t x)
 	return (x < 0 ? -x : x);
 }
 
+typedef struct linux_ratelimit {
+	struct timeval lasttime;
+	int counter;
+} linux_ratelimit_t;
+
+static inline bool
+linux_ratelimited(linux_ratelimit_t *rl)
+{
+	return (ppsratecheck(&rl->lasttime, &rl->counter, 1));
+}
+
 #endif	/* _LINUX_KERNEL_H_ */
diff --git a/sys/compat/linuxkpi/common/include/linux/kobject.h b/sys/compat/linuxkpi/common/include/linux/kobject.h
index a000c4eebc1..f7280ff951b 100644
--- a/sys/compat/linuxkpi/common/include/linux/kobject.h
+++ b/sys/compat/linuxkpi/common/include/linux/kobject.h
@@ -35,6 +35,7 @@
 
 #include <linux/kernel.h>
 #include <linux/kref.h>
+#include <linux/list.h>
 #include <linux/slab.h>
 
 struct kobject;
@@ -134,6 +135,11 @@ kobject_create_and_add(const char *name, struct kobject *parent)
 	return (NULL);
 }
 
+static inline void
+kobject_del(struct kobject *kobj __unused)
+{
+}
+
 static inline char *
 kobject_name(const struct kobject *kobj)
 {
diff --git a/sys/compat/linuxkpi/common/include/linux/kthread.h b/sys/compat/linuxkpi/common/include/linux/kthread.h
index 2e0da123d52..3afd21dc935 100644
--- a/sys/compat/linuxkpi/common/include/linux/kthread.h
+++ b/sys/compat/linuxkpi/common/include/linux/kthread.h
@@ -2,7 +2,7 @@
  * Copyright (c) 2010 Isilon Systems, Inc.
  * Copyright (c) 2010 iX Systems, Inc.
  * Copyright (c) 2010 Panasas, Inc.
- * Copyright (c) 2013-2016 Mellanox Technologies, Ltd.
+ * Copyright (c) 2013-2017 Mellanox Technologies, Ltd.
  * All rights reserved.
  *
  * Redistribution and use in source and binary forms, with or without
@@ -31,74 +31,43 @@
 #ifndef	_LINUX_KTHREAD_H_
 #define	_LINUX_KTHREAD_H_
 
-#include <sys/param.h>
-#include <sys/lock.h>
-#include <sys/mutex.h>
-#include <sys/kernel.h>
-#include <sys/kthread.h>
-#include <sys/sleepqueue.h>
-
-#include <linux/slab.h>
 #include <linux/sched.h>
 
-static inline void
-linux_kthread_fn(void *arg)
-{
-	struct task_struct *task;
-	struct thread *td = curthread;
-
-	task = arg;
-	task_struct_fill(td, task);
-	task_struct_set(td, task);
-	if (task->should_stop == 0)
-		task->task_ret = task->task_fn(task->task_data);
-	PROC_LOCK(td->td_proc);
-	task->should_stop = TASK_STOPPED;
-	wakeup(task);
-	PROC_UNLOCK(td->td_proc);
-	task_struct_set(td, NULL);
-	kthread_exit();
-}
-
-static inline struct task_struct *
-linux_kthread_create(int (*threadfn)(void *data), void *data)
-{
-	struct task_struct *task;
-
-	task = kzalloc(sizeof(*task), GFP_KERNEL);
-	task->task_fn = threadfn;
-	task->task_data = data;
-
-	return (task);
-}
+#include <sys/unistd.h>
+#include <sys/kthread.h>
 
-#define	kthread_run(fn, data, fmt, ...)					\
-({									\
-	struct task_struct *_task;					\
+#define	kthread_run(fn, data, fmt, ...)	({				\
+	struct task_struct *__task;					\
+	struct thread *__td;						\
 									\
-	_task = linux_kthread_create((fn), (data));			\
-	if (kthread_add(linux_kthread_fn, _task, NULL, &_task->task_thread,	\
-	    0, 0, fmt, ## __VA_ARGS__)) {				\
-		kfree(_task);						\
-		_task = NULL;						\
-	}								\
-	_task;								\
+	if (kthread_add(linux_kthread_fn, NULL, NULL, &__td,		\
+	    RFSTOPPED, 0, fmt, ## __VA_ARGS__))				\
+		__task = NULL;						\
+	else								\
+		__task = linux_kthread_setup_and_run(__td, fn, data);	\
+	__task;								\
 })
 
-#define	kthread_should_stop()	current->should_stop
+int linux_kthread_stop(struct task_struct *);
+bool linux_kthread_should_stop_task(struct task_struct *);
+bool linux_kthread_should_stop(void);
+int linux_kthread_park(struct task_struct *);
+void linux_kthread_parkme(void);
+bool linux_kthread_should_park(void);
+void linux_kthread_unpark(struct task_struct *);
+void linux_kthread_fn(void *);
+struct task_struct *linux_kthread_setup_and_run(struct thread *,
+    linux_task_fn_t *, void *arg);
+int linux_in_atomic(void);
 
-static inline int
-kthread_stop(struct task_struct *task)
-{
+#define	kthread_stop(task)		linux_kthread_stop(task)
+#define	kthread_should_stop()		linux_kthread_should_stop()
+#define	kthread_should_stop_task(task)	linux_kthread_should_stop_task(task)
+#define	kthread_park(task)		linux_kthread_park(task)
+#define	kthread_parkme()		linux_kthread_parkme()
+#define	kthread_should_park()		linux_kthread_should_park()
+#define	kthread_unpark(task)		linux_kthread_unpark(task)
 
-	PROC_LOCK(task->task_thread->td_proc);
-	task->should_stop = TASK_SHOULD_STOP;
-	wake_up_process(task);
-	while (task->should_stop != TASK_STOPPED)
-		msleep(task, &task->task_thread->td_proc->p_mtx, PWAIT,
-		    "kstop", hz);
-	PROC_UNLOCK(task->task_thread->td_proc);
-	return task->task_ret;
-}
+#define	in_atomic()			linux_in_atomic()
 
-#endif	/* _LINUX_KTHREAD_H_ */
+#endif /* _LINUX_KTHREAD_H_ */
diff --git a/sys/compat/linuxkpi/common/include/linux/ktime.h b/sys/compat/linuxkpi/common/include/linux/ktime.h
index 7c6c40fe1a8..a1b8af008e6 100644
--- a/sys/compat/linuxkpi/common/include/linux/ktime.h
+++ b/sys/compat/linuxkpi/common/include/linux/ktime.h
@@ -26,8 +26,9 @@
  *
  * $FreeBSD$
  */
+
 #ifndef _LINUX_KTIME_H
-#define _LINUX_KTIME_H
+#define	_LINUX_KTIME_H
 
 #include <linux/types.h>
 #include <linux/time.h>
@@ -51,6 +52,15 @@ ktime_to_ns(ktime_t kt)
 	return kt.tv64;
 }
 
+static inline ktime_t
+ns_to_ktime(uint64_t nsec)
+{
+	ktime_t kt;
+
+	kt.tv64 = nsec;
+	return (kt);
+}
+
 static inline int64_t
 ktime_divns(const ktime_t kt, int64_t div)
 {
@@ -144,7 +154,7 @@ timeval_to_ktime(struct timeval tv)
 #define ktime_to_timeval(kt)		ns_to_timeval((kt).tv64)
 #define ktime_to_ns(kt)			((kt).tv64)
 
-static inline s64
+static inline int64_t
 ktime_get_ns(void)
 {
 	struct timespec ts;
@@ -155,6 +165,8 @@ ktime_get_ns(void)
 	return (ktime_to_ns(kt));
 }
 
+#define	ktime_get_raw_ns()	ktime_get_ns()
+
 static inline ktime_t
 ktime_get(void)
 {
@@ -164,4 +176,22 @@ ktime_get(void)
 	return (timespec_to_ktime(ts));
 }
 
-#endif	/* _LINUX_KTIME_H */
+static inline ktime_t
+ktime_get_boottime(void)
+{
+	struct timespec ts;
+
+	nanouptime(&ts);
+	return (timespec_to_ktime(ts));
+}
+
+static inline ktime_t
+ktime_get_real(void)
+{
+	struct timespec ts;
+
+	nanotime(&ts);
+	return (timespec_to_ktime(ts));
+}
+
+#endif /* _LINUX_KTIME_H */
diff --git a/sys/compat/linuxkpi/common/include/linux/list.h b/sys/compat/linuxkpi/common/include/linux/list.h
index d73cbcb7339..c235c269246 100644
--- a/sys/compat/linuxkpi/common/include/linux/list.h
+++ b/sys/compat/linuxkpi/common/include/linux/list.h
@@ -70,7 +70,9 @@
 #include <vm/vm_object.h>
 #include <vm/pmap.h>
 
+#ifndef prefetch
 #define	prefetch(x)
+#endif
 
 #define LINUX_LIST_HEAD_INIT(name) { &(name), &(name) }
 
@@ -389,10 +391,6 @@ hlist_move_list(struct hlist_head *old, struct hlist_head *new)
 	old->first = NULL;
 }
 
-/**
- * list_is_singular - tests whether a list has just one entry.
- * @head: the list to test.
- */
 static inline int list_is_singular(const struct list_head *head)
 {
 	return !list_empty(head) && (head->next == head->prev);
@@ -410,20 +408,6 @@ static inline void __list_cut_position(struct list_head *list,
 	new_first->prev = head;
 }
 
-/**
- * list_cut_position - cut a list into two
- * @list: a new list to add all removed entries
- * @head: a list with entries
- * @entry: an entry within head, could be the head itself
- *	and if so we won't cut the list
- *
- * This helper moves the initial part of @head, up to and
- * including @entry, from @head to @list. You should
- * pass on @entry an element you know is on @head. @list
- * should be an empty list or a list you do not care about
- * losing its data.
- *
- */
 static inline void list_cut_position(struct list_head *list,
 		struct list_head *head, struct list_head *entry)
 {
@@ -438,11 +422,6 @@ static inline void list_cut_position(struct list_head *list,
 		__list_cut_position(list, head, entry);
 }
 
-/**
- *  list_is_last - tests whether @list is the last entry in list @head
- *   @list: the entry to test
- *    @head: the head of the list
- */
 static inline int list_is_last(const struct list_head *list,
                                 const struct list_head *head)
 {
diff --git a/sys/compat/linuxkpi/common/include/linux/lockdep.h b/sys/compat/linuxkpi/common/include/linux/lockdep.h
index 27386935bbe..4bf902de0f8 100644
--- a/sys/compat/linuxkpi/common/include/linux/lockdep.h
+++ b/sys/compat/linuxkpi/common/include/linux/lockdep.h
@@ -28,14 +28,25 @@
  *
  * $FreeBSD$
  */
-#ifndef	_LINUX_LOCKDEP_H_
+
+#ifndef _LINUX_LOCKDEP_H_
 #define	_LINUX_LOCKDEP_H_
 
 struct lock_class_key {
 };
 
-#define lockdep_set_class(lock, key)
+#define	lockdep_set_class(lock, key)
+
+#define	lockdep_set_class_and_name(lock, key, name)
+
+#define	lockdep_assert_held(m)				\
+	sx_assert(&(m)->sx, SA_XLOCKED)
+
+#define	lockdep_assert_held_once(m)			\
+	sx_assert(&(m)->sx, SA_XLOCKED | SA_NOTRECURSED)
+
+#define	lockdep_is_held(m)	(sx_xholder(&(m)->sx) == curthread)
 
-#define lockdep_set_class_and_name(lock, key, name)
+#define	might_lock(m)	do { } while (0)
 
-#endif  /* _LINUX_LOCKDEP_H_ */
+#endif /* _LINUX_LOCKDEP_H_ */
diff --git a/sys/compat/linuxkpi/common/include/linux/math64.h b/sys/compat/linuxkpi/common/include/linux/math64.h
index 2a488f1216f..9b22416a9e9 100644
--- a/sys/compat/linuxkpi/common/include/linux/math64.h
+++ b/sys/compat/linuxkpi/common/include/linux/math64.h
@@ -26,6 +26,7 @@
  *
  * $FreeBSD$
  */
+
 #ifndef _LINUX_MATH64_H
 #define	_LINUX_MATH64_H
 
@@ -40,16 +41,47 @@
 })
 
 static inline uint64_t
+div64_u64_rem(uint64_t dividend, uint64_t divisor, uint64_t *remainder)
+{
+
+	*remainder = dividend % divisor;
+	return (dividend / divisor);
+}
+
+static inline int64_t
+div64_s64(int64_t dividend, int64_t divisor)
+{
+
+	return (dividend / divisor);
+}
+
+static inline uint64_t
+div64_u64(uint64_t dividend, uint64_t divisor)
+{
+
+	return (dividend / divisor);
+}
+
+static inline uint64_t
 div_u64_rem(uint64_t dividend, uint32_t divisor, uint32_t *remainder)
 {
+
 	*remainder = dividend % divisor;
 	return (dividend / divisor);
 }
 
+static inline int64_t
+div_s64(int64_t dividend, int32_t divisor)
+{
+
+	return (dividend / divisor);
+}
+
 static inline uint64_t
 div_u64(uint64_t dividend, uint32_t divisor)
 {
+
 	return (dividend / divisor);
 }
 
-#endif					/* _LINUX_MATH64_H */
+#endif /* _LINUX_MATH64_H */
diff --git a/sys/compat/linuxkpi/common/include/linux/mm.h b/sys/compat/linuxkpi/common/include/linux/mm.h
index 3835e34153d..a649c8c785f 100644
--- a/sys/compat/linuxkpi/common/include/linux/mm.h
+++ b/sys/compat/linuxkpi/common/include/linux/mm.h
@@ -2,7 +2,7 @@
  * Copyright (c) 2010 Isilon Systems, Inc.
  * Copyright (c) 2010 iX Systems, Inc.
  * Copyright (c) 2010 Panasas, Inc.
- * Copyright (c) 2013-2015 Mellanox Technologies, Ltd.
+ * Copyright (c) 2013-2017 Mellanox Technologies, Ltd.
  * Copyright (c) 2015 François Tigeot
  * Copyright (c) 2015 Matthew Dillon <dillon@backplane.com>
  * All rights reserved.
@@ -36,16 +36,96 @@
 #include <linux/spinlock.h>
 #include <linux/gfp.h>
 #include <linux/kernel.h>
+#include <linux/mm_types.h>
+#include <linux/pfn.h>
+#include <linux/list.h>
+
+#include <asm/pgtable.h>
 
 #define	PAGE_ALIGN(x)	ALIGN(x, PAGE_SIZE)
 
+/*
+ * Make sure our LinuxKPI defined virtual memory flags don't conflict
+ * with the ones defined by FreeBSD:
+ */
+CTASSERT((VM_PROT_ALL & -(1 << 8)) == 0);
+
+#define	VM_READ			VM_PROT_READ
+#define	VM_WRITE		VM_PROT_WRITE
+#define	VM_EXEC			VM_PROT_EXECUTE
+
+#define	VM_PFNINTERNAL		(1 << 8)	/* FreeBSD private flag to vm_insert_pfn() */
+#define	VM_MIXEDMAP		(1 << 9)
+#define	VM_NORESERVE		(1 << 10)
+#define	VM_PFNMAP		(1 << 11)
+#define	VM_IO			(1 << 12)
+#define	VM_MAYWRITE		(1 << 13)
+#define	VM_DONTCOPY		(1 << 14)
+#define	VM_DONTEXPAND		(1 << 15)
+#define	VM_DONTDUMP		(1 << 16)
+
+#define	VMA_MAX_PREFAULT_RECORD	1
+
+#define	FOLL_WRITE		(1 << 0)
+#define	FOLL_FORCE		(1 << 1)
+
+#define	VM_FAULT_OOM		(1 << 0)
+#define	VM_FAULT_SIGBUS		(1 << 1)
+#define	VM_FAULT_MAJOR		(1 << 2)
+#define	VM_FAULT_WRITE		(1 << 3)
+#define	VM_FAULT_HWPOISON	(1 << 4)
+#define	VM_FAULT_HWPOISON_LARGE	(1 << 5)
+#define	VM_FAULT_SIGSEGV	(1 << 6)
+#define	VM_FAULT_NOPAGE		(1 << 7)
+#define	VM_FAULT_LOCKED		(1 << 8)
+#define	VM_FAULT_RETRY		(1 << 9)
+#define	VM_FAULT_FALLBACK	(1 << 10)
+
+#define	FAULT_FLAG_WRITE	(1 << 0)
+#define	FAULT_FLAG_MKWRITE	(1 << 1)
+#define	FAULT_FLAG_ALLOW_RETRY	(1 << 2)
+#define	FAULT_FLAG_RETRY_NOWAIT	(1 << 3)
+#define	FAULT_FLAG_KILLABLE	(1 << 4)
+#define	FAULT_FLAG_TRIED	(1 << 5)
+#define	FAULT_FLAG_USER		(1 << 6)
+#define	FAULT_FLAG_REMOTE	(1 << 7)
+#define	FAULT_FLAG_INSTRUCTION	(1 << 8)
+
+typedef int (*pte_fn_t)(linux_pte_t *, pgtable_t, unsigned long addr, void *data);
+
 struct vm_area_struct {
-	vm_offset_t	vm_start;
-	vm_offset_t	vm_end;
-	vm_offset_t	vm_pgoff;
-	vm_paddr_t	vm_pfn;		/* PFN For mmap. */
-	vm_size_t	vm_len;		/* length for mmap. */
-	vm_memattr_t	vm_page_prot;
+	vm_offset_t vm_start;
+	vm_offset_t vm_end;
+	vm_offset_t vm_pgoff;
+	pgprot_t vm_page_prot;
+	unsigned long vm_flags;
+	struct mm_struct *vm_mm;
+	void   *vm_private_data;
+	const struct vm_operations_struct *vm_ops;
+	struct linux_file *vm_file;
+
+	/* internal operation */
+	vm_paddr_t vm_pfn;		/* PFN for memory map */
+	vm_size_t vm_len;		/* length for memory map */
+	vm_pindex_t vm_pfn_first;
+	int	vm_pfn_count;
+	int    *vm_pfn_pcount;
+	vm_object_t vm_obj;
+	vm_map_t vm_cached_map;
+	TAILQ_ENTRY(vm_area_struct) vm_entry;
+};
+
+struct vm_fault {
+	unsigned int flags;
+	pgoff_t	pgoff;
+	void   *virtual_address;	/* user-space address */
+	struct page *page;
+};
+
+struct vm_operations_struct {
+	void    (*open) (struct vm_area_struct *);
+	void    (*close) (struct vm_area_struct *);
+	int     (*fault) (struct vm_area_struct *, struct vm_fault *);
 };
 
 /*
@@ -69,12 +149,11 @@ get_order(unsigned long size)
 static inline void *
 lowmem_page_address(struct page *page)
 {
-
-	return page_address(page);
+	return (page_address(page));
 }
 
 /*
- * This only works via mmap ops.
+ * This only works via memory map operations.
  */
 static inline int
 io_remap_pfn_range(struct vm_area_struct *vma,
@@ -88,6 +167,27 @@ io_remap_pfn_range(struct vm_area_struct *vma,
 	return (0);
 }
 
+static inline int
+apply_to_page_range(struct mm_struct *mm, unsigned long address,
+    unsigned long size, pte_fn_t fn, void *data)
+{
+	return (-ENOTSUP);
+}
+
+static inline int
+zap_vma_ptes(struct vm_area_struct *vma, unsigned long address,
+    unsigned long size)
+{
+	return (-ENOTSUP);
+}
+
+static inline int
+remap_pfn_range(struct vm_area_struct *vma, unsigned long addr,
+    unsigned long pfn, unsigned long size, pgprot_t prot)
+{
+	return (-ENOTSUP);
+}
+
 static inline unsigned long
 vma_pages(struct vm_area_struct *vma)
 {
@@ -103,9 +203,67 @@ set_page_dirty(struct vm_page *page)
 }
 
 static inline void
+set_page_dirty_lock(struct vm_page *page)
+{
+	vm_page_lock(page);
+	vm_page_dirty(page);
+	vm_page_unlock(page);
+}
+
+static inline void
+mark_page_accessed(struct vm_page *page)
+{
+	vm_page_reference(page);
+}
+
+static inline void
 get_page(struct vm_page *page)
 {
-	vm_page_hold(page);
+	vm_page_lock(page);
+	vm_page_wire(page);
+	vm_page_unlock(page);
 }
 
-#endif	/* _LINUX_MM_H_ */
+extern long
+get_user_pages(unsigned long start, unsigned long nr_pages,
+    int gup_flags, struct page **,
+    struct vm_area_struct **);
+
+extern int
+__get_user_pages_fast(unsigned long start, int nr_pages, int write,
+    struct page **);
+
+extern long
+get_user_pages_remote(struct task_struct *, struct mm_struct *,
+    unsigned long start, unsigned long nr_pages,
+    int gup_flags, struct page **,
+    struct vm_area_struct **);
+
+static inline void
+put_page(struct vm_page *page)
+{
+	vm_page_lock(page);
+	vm_page_unwire(page, PQ_ACTIVE);
+	vm_page_unlock(page);
+}
+
+#define	copy_highpage(to, from) pmap_copy_page(from, to)
+
+static inline pgprot_t
+vm_get_page_prot(unsigned long vm_flags)
+{
+	return (vm_flags & VM_PROT_ALL);
+}
+
+static inline vm_page_t
+vmalloc_to_page(const void *addr)
+{
+	vm_paddr_t paddr;
+
+	paddr = pmap_kextract((vm_offset_t)addr);
+	return (PHYS_TO_VM_PAGE(paddr));
+}
+
+extern int is_vmalloc_addr(const void *addr);
+
+#endif					/* _LINUX_MM_H_ */
diff --git a/sys/compat/linuxkpi/common/include/linux/mm_types.h b/sys/compat/linuxkpi/common/include/linux/mm_types.h
new file mode 100644
index 00000000000..44aad34c9ba
--- /dev/null
+++ b/sys/compat/linuxkpi/common/include/linux/mm_types.h
@@ -0,0 +1,68 @@
+/*-
+ * Copyright (c) 2017 Mellanox Technologies, Ltd.
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice unmodified, this list of conditions, and the following
+ *    disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS OR
+ * IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+ * OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.
+ * IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT,
+ * INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT
+ * NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+ * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+ * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF
+ * THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ *
+ * $FreeBSD$
+ */
+
+#ifndef _LINUX_MM_TYPES_H_
+#define	_LINUX_MM_TYPES_H_
+
+#include <linux/types.h>
+#include <linux/page.h>
+#include <linux/rwsem.h>
+
+#include <asm/atomic.h>
+
+struct vm_area_struct;
+struct task_struct;
+
+struct mm_struct {
+	struct vm_area_struct *mmap;
+	atomic_t mm_count;
+	atomic_t mm_users;
+	size_t pinned_vm;
+	struct rw_semaphore mmap_sem;
+};
+
+extern void linux_mm_dtor(struct mm_struct *mm);
+
+static inline void
+mmdrop(struct mm_struct *mm)
+{
+	if (__predict_false(atomic_dec_and_test(&mm->mm_count)))
+		linux_mm_dtor(mm);
+}
+
+static inline void
+mmput(struct mm_struct *mm)
+{
+	if (__predict_false(atomic_dec_and_test(&mm->mm_users)))
+		mmdrop(mm);
+}
+
+extern struct mm_struct *linux_get_task_mm(struct task_struct *);
+#define	get_task_mm(task) linux_get_task_mm(task)
+
+#endif					/* _LINUX_MM_TYPES_H_ */
diff --git a/sys/compat/linuxkpi/common/include/linux/module.h b/sys/compat/linuxkpi/common/include/linux/module.h
index 7db9f08c84b..d3eecc8e1f2 100644
--- a/sys/compat/linuxkpi/common/include/linux/module.h
+++ b/sys/compat/linuxkpi/common/include/linux/module.h
@@ -37,6 +37,7 @@
 
 #include <linux/list.h>
 #include <linux/compiler.h>
+#include <linux/kmod.h>
 #include <linux/kobject.h>
 #include <linux/sysfs.h>
 #include <linux/moduleparam.h>
@@ -45,6 +46,8 @@
 #define MODULE_AUTHOR(name)
 #define MODULE_DESCRIPTION(name)
 #define MODULE_LICENSE(name)
+#define	MODULE_INFO(tag, info)
+#define	MODULE_FIRMWARE(firmware)
 
 #define	THIS_MODULE	((struct module *)0)
 
@@ -99,4 +102,6 @@ _module_run(void *arg)
 #define	module_put(module)
 #define	try_module_get(module)	1
 
+#define	postcore_initcall(fn)	module_init(fn)
+
 #endif	/* _LINUX_MODULE_H_ */
diff --git a/sys/compat/linuxkpi/common/include/linux/mutex.h b/sys/compat/linuxkpi/common/include/linux/mutex.h
index 68bbfaa6059..36911b1b74c 100644
--- a/sys/compat/linuxkpi/common/include/linux/mutex.h
+++ b/sys/compat/linuxkpi/common/include/linux/mutex.h
@@ -2,7 +2,7 @@
  * Copyright (c) 2010 Isilon Systems, Inc.
  * Copyright (c) 2010 iX Systems, Inc.
  * Copyright (c) 2010 Panasas, Inc.
- * Copyright (c) 2013, 2014 Mellanox Technologies, Ltd.
+ * Copyright (c) 2013-2017 Mellanox Technologies, Ltd.
  * All rights reserved.
  *
  * Redistribution and use in source and binary forms, with or without
@@ -32,6 +32,7 @@
 #define	_LINUX_MUTEX_H_
 
 #include <sys/param.h>
+#include <sys/proc.h>
 #include <sys/lock.h>
 #include <sys/sx.h>
 
@@ -41,24 +42,90 @@ typedef struct mutex {
 	struct sx sx;
 } mutex_t;
 
-#define	mutex_lock(_m)			sx_xlock(&(_m)->sx)
+/*
+ * By defining CONFIG_NO_MUTEX_SKIP LinuxKPI mutexes and asserts will
+ * not be skipped during panic().
+ */
+#ifdef CONFIG_NO_MUTEX_SKIP
+#define	MUTEX_SKIP(void) 0
+#else
+#define	MUTEX_SKIP(void) unlikely(SCHEDULER_STOPPED() || kdb_active)
+#endif
+
+#define	mutex_lock(_m) do {			\
+	if (MUTEX_SKIP())			\
+		break;				\
+	sx_xlock(&(_m)->sx);			\
+} while (0)
+
 #define	mutex_lock_nested(_m, _s)	mutex_lock(_m)
-#define	mutex_lock_interruptible(_m)	({ mutex_lock((_m)); 0; })
-#define	mutex_unlock(_m)		sx_xunlock(&(_m)->sx)
-#define	mutex_trylock(_m)		!!sx_try_xlock(&(_m)->sx)
+#define	mutex_lock_nest_lock(_m, _s)	mutex_lock(_m)
+
+#define	mutex_lock_interruptible(_m) ({		\
+	MUTEX_SKIP() ? 0 :			\
+	(sx_xlock_sig(&(_m)->sx) ? -EINTR : 0);	\
+})
+
+#define	mutex_unlock(_m) do {			\
+	if (MUTEX_SKIP())			\
+		break;				\
+	sx_xunlock(&(_m)->sx);			\
+} while (0)
+
+#define	mutex_trylock(_m) ({			\
+	MUTEX_SKIP() ? 1 :			\
+	!!sx_try_xlock(&(_m)->sx);		\
+})
+
+#define	mutex_init(_m) \
+	linux_mutex_init(_m, mutex_name(#_m), SX_NOWITNESS)
+
+#define	mutex_init_witness(_m) \
+	linux_mutex_init(_m, mutex_name(#_m), SX_DUPOK)
+
+#define	mutex_destroy(_m) \
+	linux_mutex_destroy(_m)
+
+static inline bool
+mutex_is_locked(mutex_t *m)
+{
+	return ((struct thread *)SX_OWNER(m->sx.sx_lock) != NULL);
+}
+
+static inline bool
+mutex_is_owned(mutex_t *m)
+{
+	return (sx_xlocked(&m->sx));
+}
 
-#define DEFINE_MUTEX(lock)						\
+#ifdef WITNESS_ALL
+/* NOTE: the maximum WITNESS name is 64 chars */
+#define	__mutex_name(name, file, line)		\
+	(((const char *){file ":" #line "-" name}) + 	\
+	(sizeof(file) > 16 ? sizeof(file) - 16 : 0))
+#else
+#define	__mutex_name(name, file, line)	name
+#endif
+#define	_mutex_name(...)	__mutex_name(__VA_ARGS__)
+#define	mutex_name(name)	_mutex_name(name, __FILE__, __LINE__)
+
+#define	DEFINE_MUTEX(lock)						\
 	mutex_t lock;							\
-	SX_SYSINIT_FLAGS(lock, &(lock).sx, "lnxmtx", SX_NOWITNESS)
+	SX_SYSINIT_FLAGS(lock, &(lock).sx, mutex_name(#lock), SX_DUPOK)
 
 static inline void
-linux_mutex_init(mutex_t *m)
+linux_mutex_init(mutex_t *m, const char *name, int flags)
 {
-
-	memset(&m->sx, 0, sizeof(m->sx));
-	sx_init_flags(&m->sx, "lnxmtx",  SX_NOWITNESS);
+	memset(m, 0, sizeof(*m));
+	sx_init_flags(&m->sx, name, flags);
 }
 
-#define	mutex_init(m)	linux_mutex_init(m)
+static inline void
+linux_mutex_destroy(mutex_t *m)
+{
+	if (mutex_is_owned(m))
+		mutex_unlock(m);
+	sx_destroy(&m->sx);
+}
 
-#endif	/* _LINUX_MUTEX_H_ */
+#endif					/* _LINUX_MUTEX_H_ */
diff --git a/sys/compat/linuxkpi/common/include/linux/page.h b/sys/compat/linuxkpi/common/include/linux/page.h
index 2ad9eb68753..c2dbab769c2 100644
--- a/sys/compat/linuxkpi/common/include/linux/page.h
+++ b/sys/compat/linuxkpi/common/include/linux/page.h
@@ -34,24 +34,50 @@
 #include <linux/types.h>
 
 #include <sys/param.h>
+#include <sys/vmmeter.h>
 
 #include <machine/atomic.h>
 #include <vm/vm.h>
 #include <vm/vm_page.h>
 #include <vm/pmap.h>
 
+typedef unsigned long linux_pte_t;
+typedef unsigned long linux_pmd_t;
+typedef unsigned long linux_pgd_t;
 typedef unsigned long pgprot_t;
 
 #define page	vm_page
 
-#define	virt_to_page(x)		PHYS_TO_VM_PAGE(vtophys((x)))
-#define	page_to_pfn(pp)		(VM_PAGE_TO_PHYS((pp)) >> PAGE_SHIFT)
+#define	LINUXKPI_PROT_VALID (1 << 3)
+#define	LINUXKPI_CACHE_MODE_SHIFT 4
+
+CTASSERT((VM_PROT_ALL & -LINUXKPI_PROT_VALID) == 0);
+
+static inline pgprot_t
+cachemode2protval(vm_memattr_t attr)
+{
+	return ((attr << LINUXKPI_CACHE_MODE_SHIFT) | LINUXKPI_PROT_VALID);
+}
+
+static inline vm_memattr_t
+pgprot2cachemode(pgprot_t prot)
+{
+	if (prot & LINUXKPI_PROT_VALID)
+		return (prot >> LINUXKPI_CACHE_MODE_SHIFT);
+	else
+		return (VM_MEMATTR_DEFAULT);
+}
+
+#define	virt_to_page(x)		PHYS_TO_VM_PAGE(vtophys(x))
+#define	page_to_pfn(pp)		(VM_PAGE_TO_PHYS(pp) >> PAGE_SHIFT)
 #define	pfn_to_page(pfn)	(PHYS_TO_VM_PAGE((pfn) << PAGE_SHIFT))
-#define	nth_page(page,n)	pfn_to_page(page_to_pfn((page)) + (n))
+#define	nth_page(page,n)	pfn_to_page(page_to_pfn(page) + (n))
 
-#define	clear_page(page)		memset((page), 0, PAGE_SIZE)
-#define	pgprot_noncached(prot)		((pgprot_t)VM_MEMATTR_UNCACHEABLE)
-#define	pgprot_writecombine(prot)	((pgprot_t)VM_MEMATTR_WRITE_COMBINING)
+#define	clear_page(page)		memset(page, 0, PAGE_SIZE)
+#define	pgprot_noncached(prot)		\
+	(((prot) & VM_PROT_ALL) | cachemode2protval(VM_MEMATTR_UNCACHEABLE))
+#define	pgprot_writecombine(prot)	\
+	(((prot) & VM_PROT_ALL) | cachemode2protval(VM_MEMATTR_WRITE_COMBINING))
 
 #undef	PAGE_MASK
 #define	PAGE_MASK	(~(PAGE_SIZE-1))
diff --git a/sys/compat/linuxkpi/common/include/linux/pci.h b/sys/compat/linuxkpi/common/include/linux/pci.h
index 305fa6f312a..70e62b0a7db 100644
--- a/sys/compat/linuxkpi/common/include/linux/pci.h
+++ b/sys/compat/linuxkpi/common/include/linux/pci.h
@@ -56,13 +56,21 @@
 struct pci_device_id {
 	uint32_t	vendor;
 	uint32_t	device;
-        uint32_t	subvendor;
+	uint32_t	subvendor;
 	uint32_t	subdevice;
+	uint32_t	class;
 	uint32_t	class_mask;
 	uintptr_t	driver_data;
 };
 
 #define	MODULE_DEVICE_TABLE(bus, table)
+
+#define	PCI_BASE_CLASS_DISPLAY		0x03
+#define	PCI_CLASS_DISPLAY_VGA		0x0300
+#define	PCI_CLASS_DISPLAY_OTHER		0x0380
+#define	PCI_BASE_CLASS_BRIDGE		0x06
+#define	PCI_CLASS_BRIDGE_ISA		0x0601
+
 #define	PCI_ANY_ID		(-1)
 #define	PCI_VENDOR_ID_APPLE		0x106b
 #define	PCI_VENDOR_ID_ASUSTEK		0x1043
@@ -72,16 +80,20 @@ struct pci_device_id {
 #define	PCI_VENDOR_ID_IBM		0x1014
 #define	PCI_VENDOR_ID_INTEL		0x8086
 #define	PCI_VENDOR_ID_MELLANOX			0x15b3
+#define	PCI_VENDOR_ID_REDHAT_QUMRANET	0x1af4
 #define	PCI_VENDOR_ID_SERVERWORKS	0x1166
 #define	PCI_VENDOR_ID_SONY		0x104d
 #define	PCI_VENDOR_ID_TOPSPIN			0x1867
 #define	PCI_VENDOR_ID_VIA		0x1106
+#define	PCI_SUBVENDOR_ID_REDHAT_QUMRANET	0x1af4
+#define	PCI_DEVICE_ID_ATI_RADEON_QY	0x5159
 #define	PCI_DEVICE_ID_MELLANOX_TAVOR		0x5a44
 #define	PCI_DEVICE_ID_MELLANOX_TAVOR_BRIDGE	0x5a46
 #define	PCI_DEVICE_ID_MELLANOX_ARBEL_COMPAT	0x6278
 #define	PCI_DEVICE_ID_MELLANOX_ARBEL		0x6282
 #define	PCI_DEVICE_ID_MELLANOX_SINAI_OLD	0x5e8c
 #define	PCI_DEVICE_ID_MELLANOX_SINAI		0x6274
+#define	PCI_SUBDEVICE_ID_QEMU		0x1100
 
 #define PCI_DEVFN(slot, func)   ((((slot) & 0x1f) << 3) | ((func) & 0x07))
 #define PCI_SLOT(devfn)         (((devfn) >> 3) & 0x1f)
@@ -171,9 +183,16 @@ struct pci_driver {
 	int  (*suspend) (struct pci_dev *dev, pm_message_t state);	/* Device suspended */
 	int  (*resume) (struct pci_dev *dev);		/* Device woken up */
 	void (*shutdown) (struct pci_dev *dev);		/* Device shutdown */
-	driver_t			driver;
+	driver_t			bsddriver;
 	devclass_t			bsdclass;
-        const struct pci_error_handlers       *err_handler;
+	struct device_driver		driver;
+	const struct pci_error_handlers       *err_handler;
+	bool				isdrm;
+};
+
+struct pci_bus {
+	struct pci_dev	*self;
+	int		number;
 };
 
 extern struct list_head pci_drivers;
@@ -186,12 +205,16 @@ struct pci_dev {
 	struct device		dev;
 	struct list_head	links;
 	struct pci_driver	*pdrv;
+	struct pci_bus		*bus;
 	uint64_t		dma_mask;
 	uint16_t		device;
 	uint16_t		vendor;
+	uint16_t		subsystem_vendor;
+	uint16_t		subsystem_device;
 	unsigned int		irq;
 	unsigned int		devfn;
-	u8			revision;
+	uint32_t		class;
+	uint8_t			revision;
 };
 
 static inline struct resource_list_entry *
@@ -220,18 +243,19 @@ static inline struct device *
 linux_pci_find_irq_dev(unsigned int irq)
 {
 	struct pci_dev *pdev;
+	struct device *found;
 
+	found = NULL;
 	spin_lock(&pci_lock);
 	list_for_each_entry(pdev, &pci_devices, links) {
-		if (irq == pdev->dev.irq)
-			break;
-		if (irq >= pdev->dev.msix && irq < pdev->dev.msix_max)
+		if (irq == pdev->dev.irq ||
+		    (irq >= pdev->dev.msix && irq < pdev->dev.msix_max)) {
+			found = &pdev->dev;
 			break;
+		}
 	}
 	spin_unlock(&pci_lock);
-	if (pdev)
-		return &pdev->dev;
-	return (NULL);
+	return (found);
 }
 
 static inline unsigned long
@@ -316,6 +340,9 @@ pci_enable_device(struct pci_dev *pdev)
 static inline void
 pci_disable_device(struct pci_dev *pdev)
 {
+
+	pci_disable_io(pdev->dev.bsddev, SYS_RES_IOPORT);
+	pci_disable_io(pdev->dev.bsddev, SYS_RES_MEMORY);
 }
 
 static inline int
@@ -482,8 +509,12 @@ pci_write_config_dword(struct pci_dev *pdev, int where, u32 val)
 	return (0);
 }
 
-extern int pci_register_driver(struct pci_driver *pdrv);
-extern void pci_unregister_driver(struct pci_driver *pdrv);
+int	linux_pci_register_driver(struct pci_driver *pdrv);
+int	linux_pci_register_drm_driver(struct pci_driver *pdrv);
+void	linux_pci_unregister_driver(struct pci_driver *pdrv);
+
+#define	pci_register_driver(pdrv)	linux_pci_register_driver(pdrv)
+#define	pci_unregister_driver(pdrv)	linux_pci_unregister_driver(pdrv)
 
 struct msix_entry {
 	int entry;
diff --git a/sys/compat/linuxkpi/common/include/linux/pfn.h b/sys/compat/linuxkpi/common/include/linux/pfn.h
new file mode 100644
index 00000000000..162ca102c95
--- /dev/null
+++ b/sys/compat/linuxkpi/common/include/linux/pfn.h
@@ -0,0 +1,44 @@
+/*-
+ * Copyright (c) 2017 Mellanox Technologies, Ltd.
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice unmodified, this list of conditions, and the following
+ *    disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS OR
+ * IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+ * OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.
+ * IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT,
+ * INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT
+ * NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+ * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+ * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF
+ * THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ *
+ * $FreeBSD$
+ */
+
+#ifndef _LINUX_PFN_H_
+#define	_LINUX_PFN_H_
+
+#include <linux/types.h>
+
+typedef struct {
+	u64	val;
+} pfn_t;
+
+#define	PFN_ALIGN(x)	(((unsigned long)(x) + PAGE_SIZE - 1) & ~(PAGE_SIZE - 1))
+#define	PFN_UP(x)	(((x) + PAGE_SIZE - 1) >> PAGE_SHIFT)
+#define	PFN_DOWN(x)	((x) >> PAGE_SHIFT)
+#define	PFN_PHYS(x)	((phys_addr_t)(x) << PAGE_SHIFT)
+#define	PHYS_PFN(x)	((unsigned long)((x) >> PAGE_SHIFT))
+
+#endif					/* _LINUX_PFN_H_ */
diff --git a/sys/compat/linuxkpi/common/include/linux/pfn_t.h b/sys/compat/linuxkpi/common/include/linux/pfn_t.h
new file mode 100644
index 00000000000..bfa80b14ae9
--- /dev/null
+++ b/sys/compat/linuxkpi/common/include/linux/pfn_t.h
@@ -0,0 +1,56 @@
+/*-
+ * Copyright (c) 2017 Mellanox Technologies, Ltd.
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice unmodified, this list of conditions, and the following
+ *    disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS OR
+ * IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+ * OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.
+ * IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT,
+ * INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT
+ * NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+ * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+ * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF
+ * THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ *
+ * $FreeBSD$
+ */
+
+#ifndef _LINUX_PFN_T_H_
+#define	_LINUX_PFN_T_H_
+
+#include <linux/mm.h>
+
+CTASSERT(PAGE_SHIFT > 4);
+
+#define	PFN_FLAGS_MASK (((u64)(PAGE_SIZE - 1)) << (64 - PAGE_SHIFT))
+#define	PFN_SG_CHAIN (1ULL << (64 - 1))
+#define	PFN_SG_LAST (1ULL << (64 - 2))
+#define	PFN_DEV (1ULL << (64 - 3))
+#define	PFN_MAP (1ULL << (64 - 4))
+
+static inline pfn_t
+__pfn_to_pfn_t(unsigned long pfn, u64 flags)
+{
+	pfn_t pfn_t = { pfn | (flags & PFN_FLAGS_MASK) };
+
+	return (pfn_t);
+}
+
+static inline pfn_t
+pfn_to_pfn_t(unsigned long pfn)
+{
+	return (__pfn_to_pfn_t (pfn, 0));
+}
+
+#endif					/* _LINUX_PFN_T_H_ */
diff --git a/sys/compat/linuxkpi/common/include/linux/pid.h b/sys/compat/linuxkpi/common/include/linux/pid.h
new file mode 100644
index 00000000000..2c7e0eaf706
--- /dev/null
+++ b/sys/compat/linuxkpi/common/include/linux/pid.h
@@ -0,0 +1,65 @@
+/*-
+ * Copyright (c) 2017 Mellanox Technologies, Ltd.
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice unmodified, this list of conditions, and the following
+ *    disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS OR
+ * IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+ * OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.
+ * IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT,
+ * INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT
+ * NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+ * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+ * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF
+ * THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ *
+ * $FreeBSD$
+ */
+
+#ifndef	_LINUX_PID_H_
+#define	_LINUX_PID_H_
+
+#include <sys/param.h>
+#include <sys/systm.h>
+#include <sys/proc.h>
+
+enum pid_type {
+	PIDTYPE_PID,
+	PIDTYPE_PGID,
+	PIDTYPE_SID,
+	PIDTYPE_MAX
+};
+
+#define	pid_nr(n) (n)
+#define	pid_vnr(n) (n)
+#define	from_kuid_munged(a, uid) (uid)
+
+#define	pid_task(pid, type) ({			\
+	struct task_struct *__ts;		\
+	CTASSERT((type) == PIDTYPE_PID);	\
+	__ts = linux_pid_task(pid);		\
+	__ts;					\
+})
+
+#define	get_pid_task(pid, type) ({		\
+	struct task_struct *__ts;		\
+	CTASSERT((type) == PIDTYPE_PID);	\
+	__ts = linux_get_pid_task(pid);		\
+	__ts;					\
+})
+
+struct task_struct;
+extern struct task_struct *linux_pid_task(pid_t);
+extern struct task_struct *linux_get_pid_task(pid_t);
+
+#endif					/* _LINUX_PID_H_ */
diff --git a/sys/compat/linuxkpi/common/include/linux/poll.h b/sys/compat/linuxkpi/common/include/linux/poll.h
index bdcfd293396..33501165df2 100644
--- a/sys/compat/linuxkpi/common/include/linux/poll.h
+++ b/sys/compat/linuxkpi/common/include/linux/poll.h
@@ -40,10 +40,9 @@
 typedef struct poll_table_struct {
 } poll_table;
 
-static inline void
-poll_wait(struct linux_file *filp, wait_queue_head_t *wait_address, poll_table *p)
-{
-	selrecord(curthread, &filp->f_selinfo);
-}
+extern void linux_poll_wait(struct linux_file *, wait_queue_head_t *, poll_table *);
+#define	poll_wait(...) linux_poll_wait(__VA_ARGS__)
+
+extern void linux_poll_wakeup(struct linux_file *);
 
 #endif	/* _LINUX_POLL_H_ */
diff --git a/sys/compat/linuxkpi/common/include/linux/preempt.h b/sys/compat/linuxkpi/common/include/linux/preempt.h
new file mode 100644
index 00000000000..4a51e292ef6
--- /dev/null
+++ b/sys/compat/linuxkpi/common/include/linux/preempt.h
@@ -0,0 +1,40 @@
+/*-
+ * Copyright (c) 2017 Mellanox Technologies, Ltd.
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice unmodified, this list of conditions, and the following
+ *    disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS OR
+ * IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+ * OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.
+ * IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT,
+ * INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT
+ * NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+ * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+ * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF
+ * THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ *
+ * $FreeBSD$
+ */
+
+#ifndef _LINUX_PREEMPT_H_
+#define	_LINUX_PREEMPT_H_
+
+#include <linux/list.h>
+
+#define	in_interrupt() \
+	(curthread->td_intr_nesting_level || curthread->td_critnest)
+
+#define	preempt_disable()	critical_enter()
+#define	preempt_enable()	critical_exit()
+
+#endif					/* _LINUX_PREEMPT_H_ */
diff --git a/sys/compat/linuxkpi/common/include/linux/printk.h b/sys/compat/linuxkpi/common/include/linux/printk.h
index b1605940269..1480fc686a5 100644
--- a/sys/compat/linuxkpi/common/include/linux/printk.h
+++ b/sys/compat/linuxkpi/common/include/linux/printk.h
@@ -2,7 +2,7 @@
  * Copyright (c) 2010 Isilon Systems, Inc.
  * Copyright (c) 2010 iX Systems, Inc.
  * Copyright (c) 2010 Panasas, Inc.
- * Copyright (c) 2013, 2014 Mellanox Technologies, Ltd.
+ * Copyright (c) 2013-2017 Mellanox Technologies, Ltd.
  * All rights reserved.
  *
  * Redistribution and use in source and binary forms, with or without
@@ -28,8 +28,10 @@
  *
  * $FreeBSD$
  */
-#ifndef _FBSD_PRINTK_H_
-#define	_FBSD_PRINTK_H_
+#ifndef _LINUX_PRINTK_H_
+#define	_LINUX_PRINTK_H_
+
+#include <linux/kernel.h>
 
 /* GID printing macros */
 #define	GID_PRINT_FMT			"%.4x:%.4x:%.4x:%.4x:%.4x:%.4x:%.4x:%.4x"
@@ -38,4 +40,76 @@
 					htons(((u16 *)gid_raw)[4]), htons(((u16 *)gid_raw)[5]),\
 					htons(((u16 *)gid_raw)[6]), htons(((u16 *)gid_raw)[7])
 
-#endif					/* _FBSD_PRINTK_H */
+enum {
+	DUMP_PREFIX_NONE,
+	DUMP_PREFIX_ADDRESS,
+	DUMP_PREFIX_OFFSET
+};
+
+static inline void
+print_hex_dump(const char *level, const char *prefix_str,
+    const int prefix_type, const int rowsize, const int groupsize,
+    const void *buf, size_t len, const bool ascii)
+{
+	typedef const struct { long long value; } __packed *print_64p_t;
+	typedef const struct { uint32_t value; } __packed *print_32p_t;
+	typedef const struct { uint16_t value; } __packed *print_16p_t;
+	const void *buf_old = buf;
+	int row;
+
+	while (len > 0) {
+		if (level != NULL)
+			printf("%s", level);
+		if (prefix_str != NULL)
+			printf("%s ", prefix_str);
+
+		switch (prefix_type) {
+		case DUMP_PREFIX_ADDRESS:
+			printf("[%p] ", buf);
+			break;
+		case DUMP_PREFIX_OFFSET:
+			printf("[%p] ", (const char *)((const char *)buf -
+			    (const char *)buf_old));
+			break;
+		default:
+			break;
+		}
+		for (row = 0; row != rowsize; row++) {
+			if (groupsize == 8 && len > 7) {
+				printf("%016llx ", ((print_64p_t)buf)->value);
+				buf = (const uint8_t *)buf + 8;
+				len -= 8;
+			} else if (groupsize == 4 && len > 3) {
+				printf("%08x ", ((print_32p_t)buf)->value);
+				buf = (const uint8_t *)buf + 4;
+				len -= 4;
+			} else if (groupsize == 2 && len > 1) {
+				printf("%04x ", ((print_16p_t)buf)->value);
+				buf = (const uint8_t *)buf + 2;
+				len -= 2;
+			} else if (len > 0) {
+				printf("%02x ", *(const uint8_t *)buf);
+				buf = (const uint8_t *)buf + 1;
+				len--;
+			} else {
+				break;
+			}
+		}
+		printf("\n");
+	}
+}
+
+static inline void
+print_hex_dump_bytes(const char *prefix_str, const int prefix_type,
+    const void *buf, size_t len)
+{
+	print_hex_dump(NULL, prefix_str, prefix_type, 16, 1, buf, len, 0);
+}
+
+#define	printk_ratelimited(...) do {		\
+	static linux_ratelimit_t __ratelimited;	\
+	if (linux_ratelimited(&__ratelimited))	\
+		printk(__VA_ARGS__);		\
+} while (0)
+
+#endif					/* _LINUX_PRINTK_H_ */
diff --git a/sys/compat/linuxkpi/common/include/linux/random.h b/sys/compat/linuxkpi/common/include/linux/random.h
index 28475ed2275..04729319aaa 100644
--- a/sys/compat/linuxkpi/common/include/linux/random.h
+++ b/sys/compat/linuxkpi/common/include/linux/random.h
@@ -28,7 +28,8 @@
  *
  * $FreeBSD$
  */
-#ifndef	_LINUX_RANDOM_H_
+
+#ifndef _LINUX_RANDOM_H_
 #define	_LINUX_RANDOM_H_
 
 #include <sys/random.h>
@@ -37,8 +38,27 @@
 static inline void
 get_random_bytes(void *buf, int nbytes)
 {
+
 	if (read_random(buf, nbytes) == 0)
 		arc4rand(buf, nbytes, 0);
 }
 
-#endif	/* _LINUX_RANDOM_H_ */
+static inline u_int
+get_random_int(void)
+{
+	u_int val;
+
+	get_random_bytes(&val, sizeof(val));
+	return (val);
+}
+
+static inline u_long
+get_random_long(void)
+{
+	u_long val;
+
+	get_random_bytes(&val, sizeof(val));
+	return (val);
+}
+
+#endif /* _LINUX_RANDOM_H_ */
diff --git a/sys/compat/linuxkpi/common/include/linux/rculist.h b/sys/compat/linuxkpi/common/include/linux/rculist.h
new file mode 100644
index 00000000000..e4823de7a3b
--- /dev/null
+++ b/sys/compat/linuxkpi/common/include/linux/rculist.h
@@ -0,0 +1,85 @@
+/*-
+ * Copyright (c) 2015 François Tigeot
+ * Copyright (c) 2016-2017 Mellanox Technologies, Ltd.
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice unmodified, this list of conditions, and the following
+ *    disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS OR
+ * IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+ * OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.
+ * IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT,
+ * INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT
+ * NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+ * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+ * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF
+ * THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ *
+ * $FreeBSD$
+ */
+
+#ifndef _LINUX_RCULIST_H_
+#define	_LINUX_RCULIST_H_
+
+#include <linux/list.h>
+#include <linux/rcupdate.h>
+
+#define	hlist_first_rcu(head)	(*((struct hlist_node **)(&(head)->first)))
+#define	hlist_next_rcu(node)	(*((struct hlist_node **)(&(node)->next)))
+#define	hlist_pprev_rcu(node)	(*((struct hlist_node **)((node)->pprev)))
+
+static inline void
+hlist_add_behind_rcu(struct hlist_node *n, struct hlist_node *prev)
+{
+	n->next = prev->next;
+	n->pprev = &prev->next;
+	rcu_assign_pointer(hlist_next_rcu(prev), n);
+	if (n->next)
+		n->next->pprev = &n->next;
+}
+
+#define	hlist_for_each_entry_rcu(pos, head, member)	\
+	hlist_for_each_entry(pos, head, member)
+
+static inline void
+hlist_del_rcu(struct hlist_node *n)
+{
+	struct hlist_node *next = n->next;
+	struct hlist_node **pprev = n->pprev;
+
+	WRITE_ONCE(*pprev, next);
+	if (next)
+		next->pprev = pprev;
+}
+
+static inline void
+hlist_add_head_rcu(struct hlist_node *n, struct hlist_head *h)
+{
+	struct hlist_node *first = h->first;
+
+	n->next = first;
+	n->pprev = &h->first;
+	rcu_assign_pointer(hlist_first_rcu(h), n);
+	if (first)
+		first->pprev = &n->next;
+}
+
+static inline void
+hlist_del_init_rcu(struct hlist_node *n)
+{
+	if (!hlist_unhashed(n)) {
+		hlist_del_rcu(n);
+		n->pprev = NULL;
+	}
+}
+
+#endif					/* _LINUX_RCULIST_H_ */
diff --git a/sys/compat/linuxkpi/common/include/linux/rcupdate.h b/sys/compat/linuxkpi/common/include/linux/rcupdate.h
index 12f43bb23bb..b2dd2ae220a 100644
--- a/sys/compat/linuxkpi/common/include/linux/rcupdate.h
+++ b/sys/compat/linuxkpi/common/include/linux/rcupdate.h
@@ -1,5 +1,5 @@
 /*-
- * Copyright (c) 2016 Mellanox Technologies, Ltd.
+ * Copyright (c) 2016-2017 Mellanox Technologies, Ltd.
  * All rights reserved.
  *
  * Redistribution and use in source and binary forms, with or without
@@ -28,70 +28,73 @@
 #ifndef	_LINUX_RCUPDATE_H_
 #define	_LINUX_RCUPDATE_H_
 
-#include <sys/param.h>
-#include <sys/lock.h>
-#include <sys/sx.h>
-
-extern struct sx linux_global_rcu_lock;
-
-struct rcu_head {
-};
-
-typedef void (*rcu_callback_t)(struct rcu_head *);
-
-static inline void
-call_rcu(struct rcu_head *ptr, rcu_callback_t func)
-{
-	sx_xlock(&linux_global_rcu_lock);
-	func(ptr);
-	sx_xunlock(&linux_global_rcu_lock);
-}
-
-static inline void
-rcu_read_lock(void)
-{
-	sx_slock(&linux_global_rcu_lock);
-}
-
-static inline void
-rcu_read_unlock(void)
-{
-	sx_sunlock(&linux_global_rcu_lock);
-}
-
-static inline void
-rcu_barrier(void)
-{
-	sx_xlock(&linux_global_rcu_lock);
-	sx_xunlock(&linux_global_rcu_lock);
-}
-
-static inline void
-synchronize_rcu(void)
-{
-	sx_xlock(&linux_global_rcu_lock);
-	sx_xunlock(&linux_global_rcu_lock);
-}
-
-#define	hlist_add_head_rcu(n, h)		\
-do {						\
-  	sx_xlock(&linux_global_rcu_lock);	\
-	hlist_add_head(n, h);			\
-	sx_xunlock(&linux_global_rcu_lock);	\
+#include <linux/compiler.h>
+#include <linux/types.h>
+
+#include <machine/atomic.h>
+
+#define	LINUX_KFREE_RCU_OFFSET_MAX	4096	/* exclusive */
+
+#define	RCU_INITIALIZER(v)			\
+	((__typeof(*(v)) *)(v))
+
+#define	RCU_INIT_POINTER(p, v) do {		\
+	(p) = (v);				\
+} while (0)
+
+#define	call_rcu(ptr, func) do {		\
+	linux_call_rcu(ptr, func);		\
+} while (0)
+
+#define	rcu_barrier(void) do {			\
+	linux_rcu_barrier();			\
+} while (0)
+
+#define	rcu_read_lock(void) do {		\
+	linux_rcu_read_lock();			\
+} while (0)
+
+#define	rcu_read_unlock(void) do {		\
+	linux_rcu_read_unlock();		\
+} while (0)
+
+#define	synchronize_rcu(void) do {	\
+	linux_synchronize_rcu();	\
 } while (0)
 
-#define	hlist_del_init_rcu(n)			\
-do {						\
-    	sx_xlock(&linux_global_rcu_lock);	\
-	hlist_del_init(n);			\
-	sx_xunlock(&linux_global_rcu_lock);	\
+#define	synchronize_rcu_expedited(void) do {	\
+	linux_synchronize_rcu();		\
 } while (0)
 
-#define	hlist_del_rcu(n)			\
-do {						\
-    	sx_xlock(&linux_global_rcu_lock);	\
-	hlist_del(n);				\
-	sx_xunlock(&linux_global_rcu_lock);	\
+#define	kfree_rcu(ptr, rcu_head) do {				\
+	CTASSERT(offsetof(__typeof(*(ptr)), rcu_head) <		\
+	    LINUX_KFREE_RCU_OFFSET_MAX);			\
+	call_rcu(&(ptr)->rcu_head, (rcu_callback_t)(uintptr_t)	\
+	    offsetof(__typeof(*(ptr)), rcu_head));		\
 } while (0)
 
+#define	rcu_access_pointer(p)			\
+	((__typeof(*p) *)(READ_ONCE(p)))
+
+#define	rcu_dereference_protected(p, c)		\
+	((__typeof(*p) *)(p))
+
+#define	rcu_dereference(p)			\
+	rcu_dereference_protected(p, 0)
+
+#define	rcu_pointer_handoff(p) (p)
+
+#define	rcu_assign_pointer(p, v) do {				\
+	atomic_store_rel_ptr((volatile uintptr_t *)&(p),	\
+	    (uintptr_t)(v));					\
+} while (0)
+
+/* prototypes */
+
+extern void linux_call_rcu(struct rcu_head *ptr, rcu_callback_t func);
+extern void linux_rcu_barrier(void);
+extern void linux_rcu_read_lock(void);
+extern void linux_rcu_read_unlock(void);
+extern void linux_synchronize_rcu(void);
+
 #endif					/* _LINUX_RCUPDATE_H_ */
diff --git a/sys/compat/linuxkpi/common/include/linux/rwlock.h b/sys/compat/linuxkpi/common/include/linux/rwlock.h
index 54c53dc9498..4c9529e843e 100644
--- a/sys/compat/linuxkpi/common/include/linux/rwlock.h
+++ b/sys/compat/linuxkpi/common/include/linux/rwlock.h
@@ -34,6 +34,7 @@
 #include <sys/types.h>
 #include <sys/lock.h>
 #include <sys/rwlock.h>
+#include <sys/libkern.h>
 
 typedef struct {
 	struct rwlock rw;
diff --git a/sys/compat/linuxkpi/common/include/linux/rwsem.h b/sys/compat/linuxkpi/common/include/linux/rwsem.h
index 22ad4dc62a9..3042dcf9cb7 100644
--- a/sys/compat/linuxkpi/common/include/linux/rwsem.h
+++ b/sys/compat/linuxkpi/common/include/linux/rwsem.h
@@ -2,7 +2,7 @@
  * Copyright (c) 2010 Isilon Systems, Inc.
  * Copyright (c) 2010 iX Systems, Inc.
  * Copyright (c) 2010 Panasas, Inc.
- * Copyright (c) 2013, 2014 Mellanox Technologies, Ltd.
+ * Copyright (c) 2013-2017 Mellanox Technologies, Ltd.
  * All rights reserved.
  *
  * Redistribution and use in source and binary forms, with or without
@@ -34,6 +34,8 @@
 #include <sys/param.h>
 #include <sys/lock.h>
 #include <sys/sx.h>
+#include <sys/libkern.h>
+#include <sys/kernel.h>
 
 struct rw_semaphore {
 	struct sx sx;
@@ -45,15 +47,36 @@ struct rw_semaphore {
 #define	up_read(_rw)			sx_sunlock(&(_rw)->sx)
 #define	down_read_trylock(_rw)		!!sx_try_slock(&(_rw)->sx)
 #define	down_write_trylock(_rw)		!!sx_try_xlock(&(_rw)->sx)
+#define	down_write_killable(_rw)	!!sx_xlock_sig(&(_rw)->sx)
 #define	downgrade_write(_rw)		sx_downgrade(&(_rw)->sx)
 #define	down_read_nested(_rw, _sc)	down_read(_rw)
+#define	init_rwsem(_rw)			linux_init_rwsem(_rw, rwsem_name("lnxrwsem"))
+
+#ifdef WITNESS_ALL
+/* NOTE: the maximum WITNESS name is 64 chars */
+#define	__rwsem_name(name, file, line)		\
+	(((const char *){file ":" #line "-" name}) + 	\
+	(sizeof(file) > 16 ? sizeof(file) - 16 : 0))
+#else
+#define	__rwsem_name(name, file, line)	name
+#endif
+#define	_rwsem_name(...)		__rwsem_name(__VA_ARGS__)
+#define	rwsem_name(name)		_rwsem_name(name, __FILE__, __LINE__)
+
+#define	DECLARE_RWSEM(name)						\
+struct rw_semaphore name;						\
+static void name##_rwsem_init(void *arg)				\
+{									\
+	linux_init_rwsem(&name, rwsem_name(#name));			\
+}									\
+SYSINIT(name, SI_SUB_LOCK, SI_ORDER_SECOND, name##_rwsem_init, NULL)
 
 static inline void
-init_rwsem(struct rw_semaphore *rw)
+linux_init_rwsem(struct rw_semaphore *rw, const char *name)
 {
 
-	memset(&rw->sx, 0, sizeof(rw->sx));
-	sx_init_flags(&rw->sx, "lnxrwsem", SX_NOWITNESS);
+	memset(rw, 0, sizeof(*rw));
+	sx_init_flags(&rw->sx, name, SX_NOWITNESS);
 }
 
-#endif	/* _LINUX_RWSEM_H_ */
+#endif					/* _LINUX_RWSEM_H_ */
diff --git a/sys/compat/linuxkpi/common/include/linux/scatterlist.h b/sys/compat/linuxkpi/common/include/linux/scatterlist.h
index 5aa8d654b50..125b1acb7a3 100644
--- a/sys/compat/linuxkpi/common/include/linux/scatterlist.h
+++ b/sys/compat/linuxkpi/common/include/linux/scatterlist.h
@@ -2,7 +2,7 @@
  * Copyright (c) 2010 Isilon Systems, Inc.
  * Copyright (c) 2010 iX Systems, Inc.
  * Copyright (c) 2010 Panasas, Inc.
- * Copyright (c) 2013-2015 Mellanox Technologies, Ltd.
+ * Copyright (c) 2013-2017 Mellanox Technologies, Ltd.
  * Copyright (c) 2015 Matthew Dillon <dillon@backplane.com>
  * All rights reserved.
  *
@@ -34,18 +34,20 @@
 
 #include <linux/page.h>
 #include <linux/slab.h>
+#include <linux/mm.h>
 
 struct scatterlist {
-	union {
-		struct page *page;
-		struct scatterlist *sg;
-	}	sl_un;
+	unsigned long page_link;
+#define	SG_PAGE_LINK_CHAIN	0x1UL
+#define	SG_PAGE_LINK_LAST	0x2UL
+#define	SG_PAGE_LINK_MASK	0x3UL
+	unsigned int offset;
+	unsigned int length;
 	dma_addr_t address;
-	unsigned long offset;
-	uint32_t length;
-	uint32_t flags;
 };
 
+CTASSERT((sizeof(struct scatterlist) & SG_PAGE_LINK_MASK) == 0);
+
 struct sg_table {
 	struct scatterlist *sgl;
 	unsigned int nents;
@@ -56,58 +58,79 @@ struct sg_page_iter {
 	struct scatterlist *sg;
 	unsigned int sg_pgoffset;
 	unsigned int maxents;
+	struct {
+		unsigned int nents;
+		int	pg_advance;
+	} internal;
 };
 
 #define	SG_MAX_SINGLE_ALLOC	(PAGE_SIZE / sizeof(struct scatterlist))
 
+#define	SG_MAGIC		0x87654321UL
+
+#define	sg_is_chain(sg)		((sg)->page_link & SG_PAGE_LINK_CHAIN)
+#define	sg_is_last(sg)		((sg)->page_link & SG_PAGE_LINK_LAST)
+#define	sg_chain_ptr(sg)	\
+	((struct scatterlist *) ((sg)->page_link & ~SG_PAGE_LINK_MASK))
+
 #define	sg_dma_address(sg)	(sg)->address
 #define	sg_dma_len(sg)		(sg)->length
-#define	sg_page(sg)		(sg)->sl_un.page
-#define	sg_scatternext(sg)	(sg)->sl_un.sg
 
-#define	SG_END		0x01
-#define	SG_CHAIN	0x02
+#define	for_each_sg_page(sgl, iter, nents, pgoffset)			\
+	for (_sg_iter_init(sgl, iter, nents, pgoffset);			\
+	     (iter)->sg; _sg_iter_next(iter))
+
+#define	for_each_sg(sglist, sg, sgmax, iter)				\
+	for (iter = 0, sg = (sglist); iter < (sgmax); iter++, sg = sg_next(sg))
+
+typedef struct scatterlist *(sg_alloc_fn) (unsigned int, gfp_t);
+typedef void (sg_free_fn) (struct scatterlist *, unsigned int);
+
+static inline void
+sg_assign_page(struct scatterlist *sg, struct page *page)
+{
+	unsigned long page_link = sg->page_link & SG_PAGE_LINK_MASK;
+
+	sg->page_link = page_link | (unsigned long)page;
+}
 
 static inline void
 sg_set_page(struct scatterlist *sg, struct page *page, unsigned int len,
     unsigned int offset)
 {
-	sg_page(sg) = page;
-	sg_dma_len(sg) = len;
+	sg_assign_page(sg, page);
 	sg->offset = offset;
-	if (offset > PAGE_SIZE)
-		panic("sg_set_page: Invalid offset %d\n", offset);
+	sg->length = len;
 }
 
-static inline void
-sg_set_buf(struct scatterlist *sg, const void *buf, unsigned int buflen)
+static inline struct page *
+sg_page(struct scatterlist *sg)
 {
-	sg_set_page(sg, virt_to_page(buf), buflen,
-	    ((uintptr_t)buf) & (PAGE_SIZE - 1));
+	return ((struct page *)((sg)->page_link & ~SG_PAGE_LINK_MASK));
 }
 
 static inline void
-sg_init_table(struct scatterlist *sg, unsigned int nents)
+sg_set_buf(struct scatterlist *sg, const void *buf, unsigned int buflen)
 {
-	bzero(sg, sizeof(*sg) * nents);
-	sg[nents - 1].flags = SG_END;
+	sg_set_page(sg, virt_to_page(buf), buflen,
+	    ((uintptr_t)buf) & (PAGE_SIZE - 1));
 }
 
 static inline struct scatterlist *
 sg_next(struct scatterlist *sg)
 {
-	if (sg->flags & SG_END)
+	if (sg_is_last(sg))
 		return (NULL);
 	sg++;
-	if (sg->flags & SG_CHAIN)
-		sg = sg_scatternext(sg);
+	if (sg_is_chain(sg))
+		sg = sg_chain_ptr(sg);
 	return (sg);
 }
 
 static inline vm_paddr_t
 sg_phys(struct scatterlist *sg)
 {
-	return sg_page(sg)->phys_addr + sg->offset;
+	return (VM_PAGE_TO_PHYS(sg_page(sg)) + sg->offset);
 }
 
 static inline void
@@ -118,18 +141,45 @@ sg_chain(struct scatterlist *prv, unsigned int prv_nents,
 
 	sg->offset = 0;
 	sg->length = 0;
-	sg->flags = SG_CHAIN;
-	sg->sl_un.sg = sgl;
+	sg->page_link = ((unsigned long)sgl |
+	    SG_PAGE_LINK_CHAIN) & ~SG_PAGE_LINK_LAST;
 }
 
-static inline void 
+static inline void
 sg_mark_end(struct scatterlist *sg)
 {
-	sg->flags = SG_END;
+	sg->page_link |= SG_PAGE_LINK_LAST;
+	sg->page_link &= ~SG_PAGE_LINK_CHAIN;
+}
+
+static inline void
+sg_init_table(struct scatterlist *sg, unsigned int nents)
+{
+	bzero(sg, sizeof(*sg) * nents);
+	sg_mark_end(&sg[nents - 1]);
+}
+
+static struct scatterlist *
+sg_kmalloc(unsigned int nents, gfp_t gfp_mask)
+{
+	if (nents == SG_MAX_SINGLE_ALLOC) {
+		return ((void *)__get_free_page(gfp_mask));
+	} else
+		return (kmalloc(nents * sizeof(struct scatterlist), gfp_mask));
+}
+
+static inline void
+sg_kfree(struct scatterlist *sg, unsigned int nents)
+{
+	if (nents == SG_MAX_SINGLE_ALLOC) {
+		free_page((unsigned long)sg);
+	} else
+		kfree(sg);
 }
 
 static inline void
-__sg_free_table(struct sg_table *table, unsigned int max_ents)
+__sg_free_table(struct sg_table *table, unsigned int max_ents,
+    bool skip_first_chunk, sg_free_fn * free_fn)
 {
 	struct scatterlist *sgl, *next;
 
@@ -142,7 +192,7 @@ __sg_free_table(struct sg_table *table, unsigned int max_ents)
 		unsigned int sg_size;
 
 		if (alloc_size > max_ents) {
-			next = sgl[max_ents - 1].sl_un.sg;
+			next = sg_chain_ptr(&sgl[max_ents - 1]);
 			alloc_size = max_ents;
 			sg_size = alloc_size - 1;
 		} else {
@@ -151,7 +201,10 @@ __sg_free_table(struct sg_table *table, unsigned int max_ents)
 		}
 
 		table->orig_nents -= sg_size;
-		kfree(sgl);
+		if (skip_first_chunk)
+			skip_first_chunk = 0;
+		else
+			free_fn(sgl, alloc_size);
 		sgl = next;
 	}
 
@@ -161,12 +214,13 @@ __sg_free_table(struct sg_table *table, unsigned int max_ents)
 static inline void
 sg_free_table(struct sg_table *table)
 {
-	__sg_free_table(table, SG_MAX_SINGLE_ALLOC);
+	__sg_free_table(table, SG_MAX_SINGLE_ALLOC, 0, sg_kfree);
 }
 
 static inline int
 __sg_alloc_table(struct sg_table *table, unsigned int nents,
-    unsigned int max_ents, gfp_t gfp_mask)
+    unsigned int max_ents, struct scatterlist *first_chunk,
+    gfp_t gfp_mask, sg_alloc_fn *alloc_fn)
 {
 	struct scatterlist *sg, *prv;
 	unsigned int left;
@@ -174,7 +228,7 @@ __sg_alloc_table(struct sg_table *table, unsigned int nents,
 	memset(table, 0, sizeof(*table));
 
 	if (nents == 0)
-		return -EINVAL;
+		return (-EINVAL);
 	left = nents;
 	prv = NULL;
 	do {
@@ -189,12 +243,17 @@ __sg_alloc_table(struct sg_table *table, unsigned int nents,
 
 		left -= sg_size;
 
-		sg = kmalloc(alloc_size * sizeof(struct scatterlist), gfp_mask);
+		if (first_chunk) {
+			sg = first_chunk;
+			first_chunk = NULL;
+		} else {
+			sg = alloc_fn(alloc_size, gfp_mask);
+		}
 		if (unlikely(!sg)) {
 			if (prv)
 				table->nents = ++table->orig_nents;
 
-			return -ENOMEM;
+			return (-ENOMEM);
 		}
 		sg_init_table(sg, alloc_size);
 		table->nents = table->orig_nents += sg_size;
@@ -210,7 +269,7 @@ __sg_alloc_table(struct sg_table *table, unsigned int nents,
 		prv = sg;
 	} while (left);
 
-	return 0;
+	return (0);
 }
 
 static inline int
@@ -219,11 +278,70 @@ sg_alloc_table(struct sg_table *table, unsigned int nents, gfp_t gfp_mask)
 	int ret;
 
 	ret = __sg_alloc_table(table, nents, SG_MAX_SINGLE_ALLOC,
-	    gfp_mask);
+	    NULL, gfp_mask, sg_kmalloc);
 	if (unlikely(ret))
-		__sg_free_table(table, SG_MAX_SINGLE_ALLOC);
+		__sg_free_table(table, SG_MAX_SINGLE_ALLOC, 0, sg_kfree);
+
+	return (ret);
+}
+
+static inline int
+sg_alloc_table_from_pages(struct sg_table *sgt,
+    struct page **pages, unsigned int count,
+    unsigned long off, unsigned long size,
+    gfp_t gfp_mask)
+{
+	unsigned int i, segs, cur;
+	int rc;
+	struct scatterlist *s;
 
-	return ret;
+	for (segs = i = 1; i < count; ++i) {
+		if (page_to_pfn(pages[i]) != page_to_pfn(pages[i - 1]) + 1)
+			++segs;
+	}
+	if (__predict_false((rc = sg_alloc_table(sgt, segs, gfp_mask))))
+		return (rc);
+
+	cur = 0;
+	for_each_sg(sgt->sgl, s, sgt->orig_nents, i) {
+		unsigned long seg_size;
+		unsigned int j;
+
+		for (j = cur + 1; j < count; ++j)
+			if (page_to_pfn(pages[j]) !=
+			    page_to_pfn(pages[j - 1]) + 1)
+				break;
+
+		seg_size = ((j - cur) << PAGE_SHIFT) - off;
+		sg_set_page(s, pages[cur], min(size, seg_size), off);
+		size -= seg_size;
+		off = 0;
+		cur = j;
+	}
+	return (0);
+}
+
+
+static inline int
+sg_nents(struct scatterlist *sg)
+{
+	int nents;
+
+	for (nents = 0; sg; sg = sg_next(sg))
+		nents++;
+	return (nents);
+}
+
+static inline void
+__sg_page_iter_start(struct sg_page_iter *piter,
+    struct scatterlist *sglist, unsigned int nents,
+    unsigned long pgoffset)
+{
+	piter->internal.pg_advance = 0;
+	piter->internal.nents = nents;
+
+	piter->sg = sglist;
+	piter->sg_pgoffset = pgoffset;
 }
 
 static inline void
@@ -247,6 +365,34 @@ _sg_iter_next(struct sg_page_iter *iter)
 	iter->sg = sg;
 }
 
+static inline int
+sg_page_count(struct scatterlist *sg)
+{
+	return (PAGE_ALIGN(sg->offset + sg->length) >> PAGE_SHIFT);
+}
+
+static inline bool
+__sg_page_iter_next(struct sg_page_iter *piter)
+{
+	if (piter->internal.nents == 0)
+		return (0);
+	if (piter->sg == NULL)
+		return (0);
+
+	piter->sg_pgoffset += piter->internal.pg_advance;
+	piter->internal.pg_advance = 1;
+
+	while (piter->sg_pgoffset >= sg_page_count(piter->sg)) {
+		piter->sg_pgoffset -= sg_page_count(piter->sg);
+		piter->sg = sg_next(piter->sg);
+		if (--piter->internal.nents == 0)
+			return (0);
+		if (piter->sg == NULL)
+			return (0);
+	}
+	return (1);
+}
+
 static inline void
 _sg_iter_init(struct scatterlist *sgl, struct sg_page_iter *iter,
     unsigned int nents, unsigned long pgoffset)
@@ -266,14 +412,14 @@ _sg_iter_init(struct scatterlist *sgl, struct sg_page_iter *iter,
 static inline dma_addr_t
 sg_page_iter_dma_address(struct sg_page_iter *spi)
 {
-	return spi->sg->address + (spi->sg_pgoffset << PAGE_SHIFT);
+	return (spi->sg->address + (spi->sg_pgoffset << PAGE_SHIFT));
 }
 
-#define	for_each_sg_page(sgl, iter, nents, pgoffset)			\
-	for (_sg_iter_init(sgl, iter, nents, pgoffset);			\
-	     (iter)->sg; _sg_iter_next(iter))
+static inline struct page *
+sg_page_iter_page(struct sg_page_iter *piter)
+{
+	return (nth_page(sg_page(piter->sg), piter->sg_pgoffset));
+}
 
-#define	for_each_sg(sglist, sg, sgmax, _itr)				\
-	for (_itr = 0, sg = (sglist); _itr < (sgmax); _itr++, sg = sg_next(sg))
 
 #endif					/* _LINUX_SCATTERLIST_H_ */
diff --git a/sys/compat/linuxkpi/common/include/linux/sched.h b/sys/compat/linuxkpi/common/include/linux/sched.h
index c9f2a399904..9d5bca717b4 100644
--- a/sys/compat/linuxkpi/common/include/linux/sched.h
+++ b/sys/compat/linuxkpi/common/include/linux/sched.h
@@ -2,7 +2,7 @@
  * Copyright (c) 2010 Isilon Systems, Inc.
  * Copyright (c) 2010 iX Systems, Inc.
  * Copyright (c) 2010 Panasas, Inc.
- * Copyright (c) 2013-2016 Mellanox Technologies, Ltd.
+ * Copyright (c) 2013-2017 Mellanox Technologies, Ltd.
  * All rights reserved.
  *
  * Redistribution and use in source and binary forms, with or without
@@ -36,102 +36,128 @@
 #include <sys/proc.h>
 #include <sys/sched.h>
 #include <sys/sleepqueue.h>
+#include <sys/time.h>
 
-#define	MAX_SCHEDULE_TIMEOUT	LONG_MAX
+#include <linux/bitmap.h>
+#include <linux/compat.h>
+#include <linux/completion.h>
+#include <linux/mm_types.h>
+#include <linux/pid.h>
+#include <linux/slab.h>
+#include <linux/string.h>
+#include <linux/time.h>
 
-#define	TASK_RUNNING		0
-#define	TASK_INTERRUPTIBLE	1
-#define	TASK_UNINTERRUPTIBLE	2
-#define	TASK_DEAD		64
-#define	TASK_WAKEKILL		128
-#define	TASK_WAKING		256
+#include <asm/atomic.h>
 
-#define	TASK_SHOULD_STOP	1
-#define	TASK_STOPPED		2
+#define	MAX_SCHEDULE_TIMEOUT	INT_MAX
+
+#define	TASK_RUNNING		0x0000
+#define	TASK_INTERRUPTIBLE	0x0001
+#define	TASK_UNINTERRUPTIBLE	0x0002
+#define	TASK_NORMAL		(TASK_INTERRUPTIBLE | TASK_UNINTERRUPTIBLE)
+#define	TASK_WAKING		0x0100
+#define	TASK_PARKED		0x0200
+
+#define	TASK_COMM_LEN		(MAXCOMLEN + 1)
 
-/*
- * A task_struct is only provided for threads created by kthread() and
- * file operation callbacks.
- *
- * Using these routines outside the above mentioned contexts will
- * cause panics because no task_struct is assigned and td_retval[1] is
- * overwritten by syscalls.
- */
 struct task_struct {
-	struct	thread *task_thread;
-	int	(*task_fn)(void *data);
-	void	*task_data;
+	struct thread *task_thread;
+	struct mm_struct *mm;
+	linux_task_fn_t *task_fn;
+	void   *task_data;
 	int	task_ret;
-	int	state;
-	int	should_stop;
-	pid_t	pid;
+	atomic_t usage;
+	atomic_t state;
+	atomic_t kthread_flags;
+	pid_t	pid;	/* BSD thread ID */
 	const char    *comm;
-	void	*bsd_ioctl_data;
-	unsigned	bsd_ioctl_len;
+	void   *bsd_ioctl_data;
+	unsigned bsd_ioctl_len;
+	struct completion parked;
+	struct completion exited;
+	TAILQ_ENTRY(task_struct) rcu_entry;
+	int rcu_recurse;
 };
 
-#define	current			task_struct_get(curthread)
-#define	task_struct_get(x)	((struct task_struct *)(uintptr_t)(x)->td_retval[1])
-#define	task_struct_fill(x, y) do {		\
-  	(y)->task_thread = (x);			\
-	(y)->comm = (x)->td_name;		\
-	(y)->pid = (x)->td_tid;			\
-} while (0)
-#define	task_struct_set(x, y)	(x)->td_retval[1] = (uintptr_t)(y)
-
-/* ensure the task_struct pointer fits into the td_retval[1] field */
-CTASSERT(sizeof(((struct thread *)0)->td_retval[1]) >= sizeof(uintptr_t));
-
-#define	set_current_state(x)						\
-	atomic_store_rel_int((volatile int *)&current->state, (x))
-#define	__set_current_state(x)	current->state = (x)
-
-
-#define	schedule()							\
-do {									\
-	void *c;							\
-									\
-	if (cold || SCHEDULER_STOPPED())				\
-		break;							\
-	c = curthread;							\
-	sleepq_lock(c);							\
-	if (current->state == TASK_INTERRUPTIBLE ||			\
-	    current->state == TASK_UNINTERRUPTIBLE) {			\
-		sleepq_add(c, NULL, "task", SLEEPQ_SLEEP, 0);		\
-		sleepq_wait(c, 0);					\
-	} else {							\
-		sleepq_release(c);					\
-		sched_relinquish(curthread);				\
-	}								\
-} while (0)
+#define	current	({ \
+	struct thread *__td = curthread; \
+	linux_set_current(__td); \
+	((struct task_struct *)__td->td_lkpi_task); \
+})
+
+#define	task_pid_group_leader(task) (task)->task_thread->td_proc->p_pid
+#define	task_pid(task)		((task)->pid)
+#define	task_pid_nr(task)	((task)->pid)
+#define	get_pid(x)		(x)
+#define	put_pid(x)		do { } while (0)
+#define	current_euid()	(curthread->td_ucred->cr_uid)
+
+#define	set_task_state(task, x)		atomic_set(&(task)->state, (x))
+#define	__set_task_state(task, x)	((task)->state.counter = (x))
+#define	set_current_state(x)		set_task_state(current, x)
+#define	__set_current_state(x)		__set_task_state(current, x)
+
+static inline void
+get_task_struct(struct task_struct *task)
+{
+	atomic_inc(&task->usage);
+}
 
-#define	wake_up_process(x)						\
-do {									\
-	int wakeup_swapper;						\
-	void *c;							\
-									\
-	c = (x)->task_thread;						\
-	sleepq_lock(c);							\
-	(x)->state = TASK_RUNNING;					\
-	wakeup_swapper = sleepq_signal(c, SLEEPQ_SLEEP, 0, 0);		\
-	sleepq_release(c);						\
-	if (wakeup_swapper)						\
-		kick_proc0();						\
-} while (0)
+static inline void
+put_task_struct(struct task_struct *task)
+{
+	if (atomic_dec_and_test(&task->usage))
+		linux_free_current(task);
+}
 
 #define	cond_resched()	if (!cold)	sched_relinquish(curthread)
 
+#define	yield()		kern_yield(PRI_UNCHANGED)
 #define	sched_yield()	sched_relinquish(curthread)
 
-static inline long
-schedule_timeout(signed long timeout)
-{
-	if (timeout < 0)
-		return 0;
+#define	need_resched() (curthread->td_flags & TDF_NEEDRESCHED)
 
-	pause("lstim", timeout);
+bool linux_signal_pending(struct task_struct *task);
+bool linux_fatal_signal_pending(struct task_struct *task);
+bool linux_signal_pending_state(long state, struct task_struct *task);
+void linux_send_sig(int signo, struct task_struct *task);
+
+#define	signal_pending(task)		linux_signal_pending(task)
+#define	fatal_signal_pending(task)	linux_fatal_signal_pending(task)
+#define	signal_pending_state(state, task)		\
+	linux_signal_pending_state(state, task)
+#define	send_sig(signo, task, priv) do {		\
+	CTASSERT(priv == 0);				\
+	linux_send_sig(signo, task);			\
+} while (0)
+
+int linux_schedule_timeout(int timeout);
+
+#define	schedule()					\
+	(void)linux_schedule_timeout(MAX_SCHEDULE_TIMEOUT)
+#define	schedule_timeout(timeout)			\
+	linux_schedule_timeout(timeout)
+#define	schedule_timeout_killable(timeout)		\
+	schedule_timeout_uninterruptible(timeout)
+#define	schedule_timeout_interruptible(timeout) ({	\
+	set_current_state(TASK_INTERRUPTIBLE);		\
+	schedule_timeout(timeout);			\
+})
+#define	schedule_timeout_uninterruptible(timeout) ({	\
+	set_current_state(TASK_UNINTERRUPTIBLE);	\
+	schedule_timeout(timeout);			\
+})
+
+#define	io_schedule()			schedule()
+#define	io_schedule_timeout(timeout)	schedule_timeout(timeout)
+
+static inline uint64_t
+local_clock(void)
+{
+	struct timespec ts;
 
-	return 0;
+	nanotime(&ts);
+	return ((uint64_t)ts.tv_sec * NSEC_PER_SEC + ts.tv_nsec);
 }
 
 #endif	/* _LINUX_SCHED_H_ */
diff --git a/sys/compat/linuxkpi/common/include/linux/semaphore.h b/sys/compat/linuxkpi/common/include/linux/semaphore.h
index 022a0164840..59a35311a5c 100644
--- a/sys/compat/linuxkpi/common/include/linux/semaphore.h
+++ b/sys/compat/linuxkpi/common/include/linux/semaphore.h
@@ -34,6 +34,7 @@
 #include <sys/param.h>
 #include <sys/lock.h>
 #include <sys/sema.h>
+#include <sys/libkern.h>
 
 /*
  * XXX BSD semaphores are disused and slow.  They also do not provide a
diff --git a/sys/compat/linuxkpi/common/include/linux/slab.h b/sys/compat/linuxkpi/common/include/linux/slab.h
index d35f7a1ef89..a0fdd426435 100644
--- a/sys/compat/linuxkpi/common/include/linux/slab.h
+++ b/sys/compat/linuxkpi/common/include/linux/slab.h
@@ -2,7 +2,7 @@
  * Copyright (c) 2010 Isilon Systems, Inc.
  * Copyright (c) 2010 iX Systems, Inc.
  * Copyright (c) 2010 Panasas, Inc.
- * Copyright (c) 2013, 2014 Mellanox Technologies, Ltd.
+ * Copyright (c) 2013-2017 Mellanox Technologies, Ltd.
  * All rights reserved.
  *
  * Redistribution and use in source and binary forms, with or without
@@ -34,6 +34,7 @@
 #include <sys/param.h>
 #include <sys/systm.h>
 #include <sys/malloc.h>
+#include <sys/limits.h>
 #include <vm/uma.h>
 
 #include <linux/types.h>
@@ -41,20 +42,18 @@
 
 MALLOC_DECLARE(M_KMALLOC);
 
-#define	kmalloc(size, flags)		malloc((size), M_KMALLOC, (flags))
-#define	kvmalloc(size)			kmalloc((size), 0)
-#define	kzalloc(size, flags)		kmalloc((size), M_ZERO | ((flags) ? (flags) : M_NOWAIT))
-#define	kzalloc_node(size, flags, node)	kzalloc(size, flags)
-#define	kfree(ptr)			free(__DECONST(void *, (ptr)), M_KMALLOC)
+#define	kvmalloc(size)			kmalloc(size, 0)
+#define	kzalloc(size, flags)		kmalloc(size, (flags) | __GFP_ZERO)
+#define	kzalloc_node(size, flags, node)	kmalloc(size, (flags) | __GFP_ZERO)
 #define	kfree_const(ptr)		kfree(ptr)
-#define	krealloc(ptr, size, flags)	realloc((ptr), (size), M_KMALLOC, (flags))
-#define	kcalloc(n, size, flags)	        kmalloc((n) * (size), flags | M_ZERO)
-#define	vzalloc(size)			kzalloc(size, GFP_KERNEL | __GFP_NOWARN)
+#define	vzalloc(size)			__vmalloc(size, GFP_KERNEL | __GFP_NOWARN | __GFP_ZERO, 0)
 #define	vfree(arg)			kfree(arg)
 #define	kvfree(arg)			kfree(arg)
-#define	vmalloc(size)                   kmalloc(size, GFP_KERNEL)
-#define	vmalloc_node(size, node)        kmalloc(size, GFP_KERNEL)
-
+#define	vmalloc_node(size, node)        __vmalloc(size, GFP_KERNEL, 0)
+#define	vmalloc_user(size)              __vmalloc(size, GFP_KERNEL | __GFP_ZERO, 0)
+#define	vmalloc(size)                   __vmalloc(size, GFP_KERNEL, 0)
+#define	__kmalloc(...)			kmalloc(__VA_ARGS__)
+#define	kmalloc_node(chunk, flags, n)	kmalloc(chunk, flags)
 
 /*
  * Prefix some functions with linux_ to avoid namespace conflict
@@ -66,59 +65,105 @@ MALLOC_DECLARE(M_KMALLOC);
 #define	kmem_cache_free(...) 	linux_kmem_cache_free(__VA_ARGS__)
 #define	kmem_cache_destroy(...) linux_kmem_cache_destroy(__VA_ARGS__)
 
+typedef void linux_kmem_ctor_t (void *);
+
 struct linux_kmem_cache {
-	uma_zone_t	cache_zone;
-	void		(*cache_ctor)(void *);
+	uma_zone_t cache_zone;
+	linux_kmem_ctor_t *cache_ctor;
+	unsigned cache_flags;
+	unsigned cache_size;
 };
 
-#define	SLAB_HWCACHE_ALIGN	0x0001
+#define	SLAB_HWCACHE_ALIGN	(1 << 0)
+#define	SLAB_DESTROY_BY_RCU     (1 << 1)
+#define	SLAB_RECLAIM_ACCOUNT	(1 << 2)
 
-static inline int
-linux_kmem_ctor(void *mem, int size, void *arg, int flags)
+static inline gfp_t
+linux_check_m_flags(gfp_t flags)
 {
-	void (*ctor)(void *);
+	const gfp_t m = M_NOWAIT | M_WAITOK;
+
+	/* make sure either M_NOWAIT or M_WAITOK is set */
+	if ((flags & m) == 0)
+		flags |= M_NOWAIT;
+	else if ((flags & m) == m)
+		flags &= ~M_WAITOK;
+
+	/* mask away LinuxKPI specific flags */
+	return (flags & GFP_NATIVE_MASK);
+}
 
-	ctor = arg;
-	ctor(mem);
+static inline void *
+kmalloc(size_t size, gfp_t flags)
+{
+	return (malloc(size, M_KMALLOC, linux_check_m_flags(flags)));
+}
+
+static inline void *
+kcalloc(size_t n, size_t size, gfp_t flags)
+{
+	flags |= __GFP_ZERO;
+	return (mallocarray(n, size, M_KMALLOC, linux_check_m_flags(flags)));
+}
+
+static inline void *
+__vmalloc(size_t size, gfp_t flags, int other)
+{
+	return (malloc(size, M_KMALLOC, linux_check_m_flags(flags)));
+}
 
-	return (0);
+static inline void *
+vmalloc_32(size_t size)
+{
+	return (contigmalloc(size, M_KMALLOC, M_WAITOK, 0, UINT_MAX, 1, 1));
 }
 
-static inline struct kmem_cache *
-linux_kmem_cache_create(char *name, size_t size, size_t align, u_long flags,
-    void (*ctor)(void *))
+static inline void *
+kmalloc_array(size_t n, size_t size, gfp_t flags)
 {
-	struct kmem_cache *c;
-
-	c = malloc(sizeof(*c), M_KMALLOC, M_WAITOK);
-	if (align)
-		align--;
-	if (flags & SLAB_HWCACHE_ALIGN)
-		align = UMA_ALIGN_CACHE;
-	c->cache_zone = uma_zcreate(name, size, ctor ? linux_kmem_ctor : NULL,
-	    NULL, NULL, NULL, align, 0);
-	c->cache_ctor = ctor;
-
-	return c;
+	return (mallocarray(n, size, M_KMALLOC, linux_check_m_flags(flags)));
 }
 
 static inline void *
-linux_kmem_cache_alloc(struct kmem_cache *c, int flags)
+krealloc(void *ptr, size_t size, gfp_t flags)
 {
-	return uma_zalloc_arg(c->cache_zone, c->cache_ctor, flags);
+	return (realloc(ptr, size, M_KMALLOC, linux_check_m_flags(flags)));
 }
 
 static inline void
-linux_kmem_cache_free(struct kmem_cache *c, void *m)
+kfree(const void *ptr)
 {
-	uma_zfree(c->cache_zone, m);
+	free(__DECONST(void *, ptr), M_KMALLOC);
 }
 
+extern struct linux_kmem_cache *linux_kmem_cache_create(const char *name,
+    size_t size, size_t align, unsigned flags, linux_kmem_ctor_t *ctor);
+
+static inline void *
+linux_kmem_cache_alloc(struct linux_kmem_cache *c, gfp_t flags)
+{
+	return (uma_zalloc_arg(c->cache_zone, c,
+	    linux_check_m_flags(flags)));
+}
+
+static inline void *
+kmem_cache_zalloc(struct linux_kmem_cache *c, gfp_t flags)
+{
+	return (uma_zalloc_arg(c->cache_zone, c,
+	    linux_check_m_flags(flags | M_ZERO)));
+}
+
+extern void linux_kmem_cache_free_rcu(struct linux_kmem_cache *, void *);
+
 static inline void
-linux_kmem_cache_destroy(struct kmem_cache *c)
+linux_kmem_cache_free(struct linux_kmem_cache *c, void *m)
 {
-	uma_zdestroy(c->cache_zone);
-	free(c, M_KMALLOC);
+	if (unlikely(c->cache_flags & SLAB_DESTROY_BY_RCU))
+		linux_kmem_cache_free_rcu(c, m);
+	else
+		uma_zfree(c->cache_zone, m);
 }
 
-#endif	/* _LINUX_SLAB_H_ */
+extern void linux_kmem_cache_destroy(struct linux_kmem_cache *);
+
+#endif					/* _LINUX_SLAB_H_ */
diff --git a/sys/compat/linuxkpi/common/include/linux/smp.h b/sys/compat/linuxkpi/common/include/linux/smp.h
new file mode 100644
index 00000000000..3f568401554
--- /dev/null
+++ b/sys/compat/linuxkpi/common/include/linux/smp.h
@@ -0,0 +1,39 @@
+/*-
+ * Copyright (c) 2017 Mark Johnston <markj@FreeBSD.org>
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in
+ *    the documentation and/or other materials provided with the
+ *    distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR AND CONTRIBUTORS ``AS IS'' AND
+ * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED.  IN NO EVENT SHALL THE AUTHOR OR CONTRIBUTORS BE LIABLE
+ * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+ * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
+ * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
+ * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+ * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
+ * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
+ * SUCH DAMAGE.
+ *
+ * $FreeBSD$
+ */
+
+#ifndef _LINUX_SMP_H_
+#define	_LINUX_SMP_H_
+
+#define	on_each_cpu(cb, data, wait) ({				\
+	CTASSERT(wait);						\
+	linux_on_each_cpu(cb, data);				\
+})
+
+extern int	linux_on_each_cpu(void (*)(void *), void *);
+
+#endif /* _LINUX_SMP_H_ */
diff --git a/sys/compat/linuxkpi/common/include/linux/spinlock.h b/sys/compat/linuxkpi/common/include/linux/spinlock.h
index 97c83e0ed03..dbd7a5a9e5e 100644
--- a/sys/compat/linuxkpi/common/include/linux/spinlock.h
+++ b/sys/compat/linuxkpi/common/include/linux/spinlock.h
@@ -2,7 +2,7 @@
  * Copyright (c) 2010 Isilon Systems, Inc.
  * Copyright (c) 2010 iX Systems, Inc.
  * Copyright (c) 2010 Panasas, Inc.
- * Copyright (c) 2013, 2014 Mellanox Technologies, Ltd.
+ * Copyright (c) 2013-2017 Mellanox Technologies, Ltd.
  * All rights reserved.
  *
  * Redistribution and use in source and binary forms, with or without
@@ -35,36 +35,126 @@
 #include <sys/kernel.h>
 #include <sys/lock.h>
 #include <sys/mutex.h>
+#include <sys/kdb.h>
 
 #include <linux/compiler.h>
-#include <linux/kernel.h>
 #include <linux/rwlock.h>
+#include <linux/bottom_half.h>
 
 typedef struct {
 	struct mtx m;
 } spinlock_t;
 
-#define	spin_lock(_l)		mtx_lock(&(_l)->m)
-#define	spin_unlock(_l)		mtx_unlock(&(_l)->m)
-#define	spin_trylock(_l)	mtx_trylock(&(_l)->m)
-#define	spin_lock_nested(_l, _n) mtx_lock_flags(&(_l)->m, MTX_DUPOK)
-#define	spin_lock_irq(lock)	spin_lock(lock)
-#define	spin_unlock_irq(lock)	spin_unlock(lock)
-#define	spin_lock_irqsave(lock, flags)   				\
-    do {(flags) = 0; spin_lock(lock); } while (0)
-#define	spin_unlock_irqrestore(lock, flags)				\
-    do { spin_unlock(lock); } while (0)
+/*
+ * By defining CONFIG_SPIN_SKIP LinuxKPI spinlocks and asserts will be
+ * skipped during panic(). By default it is disabled due to
+ * performance reasons.
+ */
+#ifdef CONFIG_SPIN_SKIP
+#define	SPIN_SKIP(void)	unlikely(SCHEDULER_STOPPED() || kdb_active)
+#else
+#define	SPIN_SKIP(void) 0
+#endif
+
+#define	spin_lock(_l) do {			\
+	if (SPIN_SKIP())			\
+		break;				\
+	mtx_lock(&(_l)->m);			\
+	local_bh_disable();			\
+} while (0)
+
+#define	spin_lock_bh(_l) do {			\
+	spin_lock(_l);				\
+} while (0)
+
+#define	spin_lock_irq(_l) do {			\
+	spin_lock(_l);				\
+} while (0)
+
+#define	spin_unlock(_l)	do {			\
+	if (SPIN_SKIP())			\
+		break;				\
+	local_bh_enable();			\
+	mtx_unlock(&(_l)->m);			\
+} while (0)
+
+#define	spin_unlock_bh(_l) do {			\
+	spin_unlock(_l);			\
+} while (0)
+
+#define	spin_unlock_irq(_l) do {		\
+	spin_unlock(_l);			\
+} while (0)
+
+#define	spin_trylock(_l) ({			\
+	int __ret;				\
+	if (SPIN_SKIP()) {			\
+		__ret = 1;			\
+	} else {				\
+		__ret = mtx_trylock(&(_l)->m);	\
+		if (likely(__ret != 0))		\
+			local_bh_disable();	\
+	}					\
+	__ret;					\
+})
+
+#define	spin_lock_nested(_l, _n) do {		\
+	if (SPIN_SKIP())			\
+		break;				\
+	mtx_lock_flags(&(_l)->m, MTX_DUPOK);	\
+	local_bh_disable();			\
+} while (0)
+
+#define	spin_lock_irqsave(_l, flags) do {	\
+	(flags) = 0;				\
+	spin_lock(_l);				\
+} while (0)
+
+#define	spin_lock_irqsave_nested(_l, flags, _n) do {	\
+	(flags) = 0;					\
+	spin_lock_nested(_l, _n);			\
+} while (0)
+
+#define	spin_unlock_irqrestore(_l, flags) do {		\
+	spin_unlock(_l);				\
+} while (0)
+
+#ifdef WITNESS_ALL
+/* NOTE: the maximum WITNESS name is 64 chars */
+#define	__spin_lock_name(name, file, line)		\
+	(((const char *){file ":" #line "-" name}) + 	\
+	(sizeof(file) > 16 ? sizeof(file) - 16 : 0))
+#else
+#define	__spin_lock_name(name, file, line)	name
+#endif
+#define	_spin_lock_name(...)		__spin_lock_name(__VA_ARGS__)
+#define	spin_lock_name(name)		_spin_lock_name(name, __FILE__, __LINE__)
+
+#define	spin_lock_init(lock)	linux_spin_lock_init(lock, spin_lock_name("lnxspin"))
+
+static inline void
+linux_spin_lock_init(spinlock_t *lock, const char *name)
+{
+
+	memset(lock, 0, sizeof(*lock));
+	mtx_init(&lock->m, name, NULL, MTX_DEF | MTX_NOWITNESS);
+}
 
 static inline void
-spin_lock_init(spinlock_t *lock)
+spin_lock_destroy(spinlock_t *lock)
 {
 
-	memset(&lock->m, 0, sizeof(lock->m));
-	mtx_init(&lock->m, "lnxspin", NULL, MTX_DEF | MTX_NOWITNESS);
+       mtx_destroy(&lock->m);
 }
 
-#define	DEFINE_SPINLOCK(lock)						\
-	spinlock_t lock;						\
-	MTX_SYSINIT(lock, &(lock).m, "lnxspin", MTX_DEF)
+#define	DEFINE_SPINLOCK(lock)					\
+	spinlock_t lock;					\
+	MTX_SYSINIT(lock, &(lock).m, spin_lock_name("lnxspin"), MTX_DEF)
+
+#define	assert_spin_locked(_l) do {		\
+	if (SPIN_SKIP())			\
+		break;				\
+	mtx_assert(&(_l)->m, MA_OWNED);		\
+} while (0)
 
-#endif	/* _LINUX_SPINLOCK_H_ */
+#endif					/* _LINUX_SPINLOCK_H_ */
diff --git a/sys/compat/linuxkpi/common/include/linux/srcu.h b/sys/compat/linuxkpi/common/include/linux/srcu.h
index c20215b0958..f89fc82a8e1 100644
--- a/sys/compat/linuxkpi/common/include/linux/srcu.h
+++ b/sys/compat/linuxkpi/common/include/linux/srcu.h
@@ -1,5 +1,5 @@
 /*-
- * Copyright (c) 2015 Mellanox Technologies, Ltd.
+ * Copyright (c) 2015-2017 Mellanox Technologies, Ltd.
  * All rights reserved.
  *
  * Redistribution and use in source and binary forms, with or without
@@ -25,48 +25,22 @@
  *
  * $FreeBSD$
  */
+
 #ifndef	_LINUX_SRCU_H_
 #define	_LINUX_SRCU_H_
 
-#include <sys/param.h>
-#include <sys/lock.h>
-#include <sys/sx.h>
-
 struct srcu_struct {
-	struct sx sx;
 };
 
-static inline int
-init_srcu_struct(struct srcu_struct *srcu)
-{
-	sx_init(&srcu->sx, "SleepableRCU");
-	return (0);
-}
-
-static inline void
-cleanup_srcu_struct(struct srcu_struct *srcu)
-{
-	sx_destroy(&srcu->sx);
-}
-
-static inline int
-srcu_read_lock(struct srcu_struct *srcu)
-{
-	sx_slock(&srcu->sx);
-	return (0);
-}
+#define	srcu_dereference(ptr,srcu)	((__typeof(*(ptr)) *)(ptr))
 
-static inline void
-srcu_read_unlock(struct srcu_struct *srcu, int key)
-{
-	sx_sunlock(&srcu->sx);
-}
+/* prototypes */
 
-static inline void
-synchronize_srcu(struct srcu_struct *srcu)
-{
-	sx_xlock(&srcu->sx);
-	sx_xunlock(&srcu->sx);
-}
+extern int srcu_read_lock(struct srcu_struct *);
+extern void srcu_read_unlock(struct srcu_struct *, int index);
+extern void synchronize_srcu(struct srcu_struct *);
+extern void srcu_barrier(struct srcu_struct *);
+extern int init_srcu_struct(struct srcu_struct *);
+extern void cleanup_srcu_struct(struct srcu_struct *);
 
 #endif					/* _LINUX_SRCU_H_ */
diff --git a/sys/compat/linuxkpi/common/include/linux/string.h b/sys/compat/linuxkpi/common/include/linux/string.h
index b480faf5c2b..a47eb4299eb 100644
--- a/sys/compat/linuxkpi/common/include/linux/string.h
+++ b/sys/compat/linuxkpi/common/include/linux/string.h
@@ -2,7 +2,7 @@
  * Copyright (c) 2010 Isilon Systems, Inc.
  * Copyright (c) 2010 iX Systems, Inc.
  * Copyright (c) 2010 Panasas, Inc.
- * Copyright (c) 2013-2016 Mellanox Technologies, Ltd.
+ * Copyright (c) 2013-2017 Mellanox Technologies, Ltd.
  * All rights reserved.
  *
  * Redistribution and use in source and binary forms, with or without
@@ -31,29 +31,111 @@
 #ifndef	_LINUX_STRING_H_
 #define	_LINUX_STRING_H_
 
+#include <sys/ctype.h>
+
 #include <linux/types.h>
 #include <linux/gfp.h>
 #include <linux/slab.h>
+#include <linux/uaccess.h>
+#include <linux/err.h>
 
 #include <sys/libkern.h>
 
 #define	strnicmp(...) strncasecmp(__VA_ARGS__)
 
+static inline int
+match_string(const char *const *table, int n, const char *key)
+{
+	int i;
+
+	for (i = 0; i != n && table[i] != NULL; i++) {
+		if (strcmp(table[i], key) == 0)
+			return (i);
+	}
+	return (-EINVAL);
+}
+
+static inline void *
+memdup_user(const void *ptr, size_t len)
+{
+	void *retval;
+	int error;
+
+	retval = malloc(len, M_KMALLOC, M_WAITOK);
+	error = linux_copyin(ptr, retval, len);
+	if (error != 0) {
+		free(retval, M_KMALLOC);
+		return (ERR_PTR(error));
+	}
+	return (retval);
+}
+
 static inline void *
 kmemdup(const void *src, size_t len, gfp_t gfp)
 {
 	void *dst;
 
 	dst = kmalloc(len, gfp);
-	if (dst)
+	if (dst != NULL)
 		memcpy(dst, src, len);
 	return (dst);
 }
 
+static inline char *
+kstrdup(const char *string, gfp_t gfp)
+{
+	char *retval;
+	size_t len;
+
+	len = strlen(string) + 1;
+	retval = kmalloc(len, gfp);
+	if (retval != NULL)
+		memcpy(retval, string, len);
+	return (retval);
+}
+
+static inline char *
+kstrndup(const char *string, size_t len, gfp_t gfp)
+{
+	char *retval;
+
+	retval = kmalloc(len + 1, gfp);
+	if (retval != NULL)
+		strncpy(retval, string, len);
+	return (retval);
+}
+
 static inline const char *
 kstrdup_const(const char *src, gfp_t gfp)
 {
 	return (kmemdup(src, strlen(src) + 1, gfp));
 }
 
-#endif	/* _LINUX_STRING_H_ */
+static inline char *
+skip_spaces(const char *str)
+{
+	while (isspace(*str))
+		++str;
+	return (__DECONST(char *, str));
+}
+
+static inline void *
+memchr_inv(const void *start, int c, size_t length)
+{
+	const u8 *ptr;
+	const u8 *end;
+	u8 ch;
+
+	ch = c;
+	ptr = start;
+	end = ptr + length;
+
+	while (ptr != end) {
+		if (*ptr != ch)
+			return (__DECONST(void *, ptr));
+		ptr++;
+	}
+	return (NULL);
+}
+
+#endif					/* _LINUX_STRING_H_ */
diff --git a/sys/compat/linuxkpi/common/include/linux/sysfs.h b/sys/compat/linuxkpi/common/include/linux/sysfs.h
index 68c6e9224d9..ea7acff8a45 100644
--- a/sys/compat/linuxkpi/common/include/linux/sysfs.h
+++ b/sys/compat/linuxkpi/common/include/linux/sysfs.h
@@ -54,14 +54,21 @@ struct attribute_group {
 	.attr = { .name = __stringify(_name), .mode = _mode },		\
         .show = _show, .store  = _store,				\
 }
-
-#define	__ATTR_RO(_name) {						\
-	.attr = { .name = __stringify(_name), .mode = 0444 },		\
-	.show   = _name##_show,						\
-}
+#define	__ATTR_RO(_name)	__ATTR(_name, 0444, _name##_show, NULL)
+#define	__ATTR_WO(_name)	__ATTR(_name, 0200, NULL, _name##_store)
+#define	__ATTR_RW(_name)	__ATTR(_name, 0644, _name##_show, _name##_store)
 
 #define	__ATTR_NULL	{ .attr = { .name = NULL } }
 
+#define	ATTRIBUTE_GROUPS(_name)						\
+	static struct attribute_group _name##_group = {			\
+		.attrs = _name##_attrs,					\
+	};								\
+	static struct attribute_group *_name##_groups[] = {		\
+		&_name##_group,						\
+		NULL,							\
+	};
+
 /*
  * Handle our generic '\0' terminated 'C' string.
  * Two cases:
@@ -126,7 +133,7 @@ static inline int
 sysfs_create_file(struct kobject *kobj, const struct attribute *attr)
 {
 
-	sysctl_add_oid(NULL, SYSCTL_CHILDREN(kobj->oidp), OID_AUTO,
+	SYSCTL_ADD_OID(NULL, SYSCTL_CHILDREN(kobj->oidp), OID_AUTO,
 	    attr->name, CTLTYPE_STRING|CTLFLAG_RW|CTLFLAG_MPSAFE, kobj,
 	    (uintptr_t)attr, sysctl_handle_attr, "A", "");
 
@@ -158,7 +165,7 @@ sysfs_create_group(struct kobject *kobj, const struct attribute_group *grp)
 	oidp = SYSCTL_ADD_NODE(NULL, SYSCTL_CHILDREN(kobj->oidp),
 	    OID_AUTO, grp->name, CTLFLAG_RD|CTLFLAG_MPSAFE, NULL, grp->name);
 	for (attr = grp->attrs; *attr != NULL; attr++) {
-		sysctl_add_oid(NULL, SYSCTL_CHILDREN(oidp), OID_AUTO,
+		SYSCTL_ADD_OID(NULL, SYSCTL_CHILDREN(oidp), OID_AUTO,
 		    (*attr)->name, CTLTYPE_STRING|CTLFLAG_RW|CTLFLAG_MPSAFE,
 		    kobj, (uintptr_t)*attr, sysctl_handle_attr, "A", "");
 	}
diff --git a/sys/compat/linuxkpi/common/include/linux/timer.h b/sys/compat/linuxkpi/common/include/linux/timer.h
index a794c13873a..d046b6e141a 100644
--- a/sys/compat/linuxkpi/common/include/linux/timer.h
+++ b/sys/compat/linuxkpi/common/include/linux/timer.h
@@ -41,34 +41,41 @@ struct timer_list {
 	struct callout timer_callout;
 	void    (*function) (unsigned long);
 	unsigned long data;
-	unsigned long expires;
+	int expires;
 };
 
 extern unsigned long linux_timer_hz_mask;
 
-#define	setup_timer(timer, func, dat)					\
-do {									\
+#define	TIMER_IRQSAFE	0x0001
+
+#define	setup_timer(timer, func, dat) do {				\
 	(timer)->function = (func);					\
 	(timer)->data = (dat);						\
 	callout_init(&(timer)->timer_callout, 1);			\
 } while (0)
 
-#define	init_timer(timer)						\
-do {									\
+#define	__setup_timer(timer, func, dat, flags) do {			\
+	CTASSERT(((flags) & ~TIMER_IRQSAFE) == 0);			\
+	setup_timer(timer, func, dat);					\
+} while (0)
+
+#define	init_timer(timer) do {						\
 	(timer)->function = NULL;					\
 	(timer)->data = 0;						\
 	callout_init(&(timer)->timer_callout, 1);			\
 } while (0)
 
-extern void mod_timer(struct timer_list *, unsigned long);
+extern void mod_timer(struct timer_list *, int);
 extern void add_timer(struct timer_list *);
+extern void add_timer_on(struct timer_list *, int cpu);
 
-#define	del_timer(timer)	callout_stop(&(timer)->timer_callout)
-#define	del_timer_sync(timer)	callout_drain(&(timer)->timer_callout)
+#define	del_timer(timer)	(void)callout_stop(&(timer)->timer_callout)
+#define	del_timer_sync(timer)	(void)callout_drain(&(timer)->timer_callout)
 #define	timer_pending(timer)	callout_pending(&(timer)->timer_callout)
-#define	round_jiffies(j) \
-	((unsigned long)(((j) + linux_timer_hz_mask) & ~linux_timer_hz_mask))
-#define	round_jiffies_relative(j) \
-	round_jiffies(j)
+#define	round_jiffies(j)	\
+	((int)(((j) + linux_timer_hz_mask) & ~linux_timer_hz_mask))
+#define	round_jiffies_relative(j) round_jiffies(j)
+#define	round_jiffies_up(j)	round_jiffies(j)
+#define	round_jiffies_up_relative(j) round_jiffies_up(j)
 
 #endif					/* _LINUX_TIMER_H_ */
diff --git a/sys/compat/linuxkpi/common/include/linux/types.h b/sys/compat/linuxkpi/common/include/linux/types.h
index c9c37284a70..8852a4d3101 100644
--- a/sys/compat/linuxkpi/common/include/linux/types.h
+++ b/sys/compat/linuxkpi/common/include/linux/types.h
@@ -2,7 +2,7 @@
  * Copyright (c) 2010 Isilon Systems, Inc.
  * Copyright (c) 2010 iX Systems, Inc.
  * Copyright (c) 2010 Panasas, Inc.
- * Copyright (c) 2013, 2014 Mellanox Technologies, Ltd.
+ * Copyright (c) 2013-2017 Mellanox Technologies, Ltd.
  * All rights reserved.
  *
  * Redistribution and use in source and binary forms, with or without
@@ -57,10 +57,22 @@ typedef unsigned int    uint;
 typedef unsigned gfp_t;
 typedef uint64_t loff_t;
 typedef vm_paddr_t resource_size_t;
+typedef uint16_t __bitwise__ __sum16;
+typedef unsigned long pgoff_t;
 
 typedef u64 phys_addr_t;
 
 #define	DECLARE_BITMAP(n, bits)						\
 	unsigned long n[howmany(bits, sizeof(long) * 8)]
 
+typedef unsigned long irq_hw_number_t;
+
+struct rcu_head {
+	void *raw[2];
+} __aligned(sizeof(void *));
+
+typedef void (*rcu_callback_t)(struct rcu_head *head);
+typedef void (*call_rcu_func_t)(struct rcu_head *head, rcu_callback_t func);
+typedef int linux_task_fn_t(void *data);
+
 #endif	/* _LINUX_TYPES_H_ */
diff --git a/sys/compat/linuxkpi/common/include/linux/uaccess.h b/sys/compat/linuxkpi/common/include/linux/uaccess.h
index 2cfd950ab4d..c046e1c063a 100644
--- a/sys/compat/linuxkpi/common/include/linux/uaccess.h
+++ b/sys/compat/linuxkpi/common/include/linux/uaccess.h
@@ -29,11 +29,22 @@
  *
  * $FreeBSD$
  */
+
 #ifndef	_LINUX_UACCESS_H_
 #define	_LINUX_UACCESS_H_
 
+#include <sys/param.h>
+#include <sys/lock.h>
+#include <sys/proc.h>
+
+#include <vm/vm.h>
+#include <vm/vm_extern.h>
+
 #include <linux/compiler.h>
 
+#define	VERIFY_READ	VM_PROT_READ
+#define	VERIFY_WRITE	VM_PROT_WRITE
+
 #define	__get_user(_x, _p) ({					\
 	int __err;						\
 	__typeof(*(_p)) __x;					\
@@ -48,25 +59,33 @@
 })
 #define	get_user(_x, _p)	linux_copyin((_p), &(_x), sizeof(*(_p)))
 #define	put_user(_x, _p)	linux_copyout(&(_x), (_p), sizeof(*(_p)))
+#define	clear_user(...)		linux_clear_user(__VA_ARGS__)
+#define	access_ok(...)		linux_access_ok(__VA_ARGS__)
 
 extern int linux_copyin(const void *uaddr, void *kaddr, size_t len);
 extern int linux_copyout(const void *kaddr, void *uaddr, size_t len);
+extern size_t linux_clear_user(void *uaddr, size_t len);
+extern int linux_access_ok(int rw, const void *uaddr, size_t len);
 
 /*
- * NOTE: The returned value from pagefault_disable() must be stored
- * and passed to pagefault_enable(). Else possible recursion on the
- * state can be lost.
+ * NOTE: Each pagefault_disable() call must have a corresponding
+ * pagefault_enable() call in the same scope. The former creates a new
+ * block and defines a temporary variable, and the latter uses the
+ * temporary variable and closes the block. Failure to balance the
+ * calls will result in a compile-time error.
  */
-static inline int __must_check
-pagefault_disable(void)
-{
-	return (vm_fault_disable_pagefaults());
-}
+#define	pagefault_disable(void) do {		\
+	int __saved_pflags =			\
+	    vm_fault_disable_pagefaults()
+
+#define	pagefault_enable(void)				\
+	vm_fault_enable_pagefaults(__saved_pflags);	\
+} while (0)
 
-static inline void
-pagefault_enable(int save)
+static inline bool
+pagefault_disabled(void)
 {
-	vm_fault_enable_pagefaults(save);
+	return ((curthread->td_pflags & TDP_NOFAULTING) != 0);
 }
 
-#endif	/* _LINUX_UACCESS_H_ */
+#endif					/* _LINUX_UACCESS_H_ */
diff --git a/sys/compat/linuxkpi/common/include/linux/wait.h b/sys/compat/linuxkpi/common/include/linux/wait.h
index 7ae6464c6d4..ea4586f2e2e 100644
--- a/sys/compat/linuxkpi/common/include/linux/wait.h
+++ b/sys/compat/linuxkpi/common/include/linux/wait.h
@@ -3,6 +3,7 @@
  * Copyright (c) 2010 iX Systems, Inc.
  * Copyright (c) 2010 Panasas, Inc.
  * Copyright (c) 2013, 2014 Mellanox Technologies, Ltd.
+ * Copyright (c) 2017 Mark Johnston <markj@FreeBSD.org>
  * All rights reserved.
  *
  * Redistribution and use in source and binary forms, with or without
@@ -28,162 +29,241 @@
  *
  * $FreeBSD$
  */
-#ifndef	_LINUX_WAIT_H_
+
+#ifndef _LINUX_WAIT_H_
 #define	_LINUX_WAIT_H_
 
 #include <linux/compiler.h>
-#include <linux/spinlock.h>
-#include <linux/sched.h>
 #include <linux/list.h>
-#include <linux/jiffies.h>
+#include <linux/spinlock.h>
+
+#include <asm/atomic.h>
 
 #include <sys/param.h>
 #include <sys/systm.h>
-#include <sys/sleepqueue.h>
-#include <sys/kernel.h>
-#include <sys/proc.h>
 
-typedef struct {
-} wait_queue_t;
+#define	SKIP_SLEEP() (SCHEDULER_STOPPED() || kdb_active)
 
-typedef struct {
-	unsigned int	wchan;
-} wait_queue_head_t;
+#define	might_sleep()							\
+	WITNESS_WARN(WARN_GIANTOK | WARN_SLEEPOK, NULL, "might_sleep()")
 
-#define	init_waitqueue_head(x) \
-    do { } while (0)
+struct wait_queue;
+struct wait_queue_head;
 
-static inline void
-__wake_up(wait_queue_head_t *q, int all)
-{
-	int wakeup_swapper;
-	void *c;
-
-	c = &q->wchan;
-	sleepq_lock(c);
-	if (all)
-		wakeup_swapper = sleepq_broadcast(c, SLEEPQ_SLEEP, 0, 0);
-	else
-		wakeup_swapper = sleepq_signal(c, SLEEPQ_SLEEP, 0, 0);
-	sleepq_release(c);
-	if (wakeup_swapper)
-		kick_proc0();
-}
+typedef struct wait_queue wait_queue_t;
+typedef struct wait_queue_head wait_queue_head_t;
+
+typedef int wait_queue_func_t(wait_queue_t *, unsigned int, int, void *);
+
+/*
+ * Many API consumers directly reference these fields and those of
+ * wait_queue_head.
+ */
+struct wait_queue {
+	unsigned int flags;	/* always 0 */
+	void *private;
+	wait_queue_func_t *func;
+	struct list_head task_list;
+};
+
+struct wait_queue_head {
+	spinlock_t lock;
+	struct list_head task_list;
+};
+
+/*
+ * This function is referenced by at least one DRM driver, so it may not be
+ * renamed and furthermore must be the default wait queue callback.
+ */
+extern wait_queue_func_t autoremove_wake_function;
 
-#define	wake_up(q)				__wake_up(q, 0)
-#define	wake_up_nr(q, nr)			__wake_up(q, 1)
-#define	wake_up_all(q)				__wake_up(q, 1)
-#define	wake_up_interruptible(q)		__wake_up(q, 0)
-#define	wake_up_interruptible_nr(q, nr)		__wake_up(q, 1)
-#define	wake_up_interruptible_all(q, nr)	__wake_up(q, 1)
-
-#define	wait_event(q, cond)						\
-do {									\
-	void *c = &(q).wchan;						\
-	if (!(cond)) {							\
-		for (;;) {						\
-			if (SCHEDULER_STOPPED())			\
-				break;					\
-			sleepq_lock(c);					\
-			if (cond) {					\
-				sleepq_release(c);			\
-				break;					\
-			}						\
-			sleepq_add(c, NULL, "completion", SLEEPQ_SLEEP, 0); \
-			sleepq_wait(c, 0);				\
-		}							\
-	}								\
+#define	DEFINE_WAIT(name)						\
+	wait_queue_t name = {						\
+		.private = current,					\
+		.func = autoremove_wake_function,			\
+		.task_list = LINUX_LIST_HEAD_INIT(name.task_list)	\
+	}
+
+#define	DECLARE_WAITQUEUE(name, task)					\
+	wait_queue_t name = {						\
+		.private = task,					\
+		.task_list = LINUX_LIST_HEAD_INIT(name.task_list)	\
+	}
+
+#define	DECLARE_WAIT_QUEUE_HEAD(name)					\
+	wait_queue_head_t name = {					\
+		.task_list = LINUX_LIST_HEAD_INIT(name.task_list),	\
+	};								\
+	MTX_SYSINIT(name, &(name).lock.m, spin_lock_name("wqhead"), MTX_DEF)
+
+#define	init_waitqueue_head(wqh) do {					\
+	mtx_init(&(wqh)->lock.m, spin_lock_name("wqhead"),		\
+	    NULL, MTX_DEF | MTX_NEW | MTX_NOWITNESS);			\
+	INIT_LIST_HEAD(&(wqh)->task_list);				\
 } while (0)
 
-#define	wait_event_interruptible(q, cond)				\
-({									\
-	void *c = &(q).wchan;						\
-	int _error;							\
-									\
-	_error = 0;							\
-	if (!(cond)) {							\
-		for (; _error == 0;) {					\
-			if (SCHEDULER_STOPPED())			\
-				break;					\
-			sleepq_lock(c);					\
-			if (cond) {					\
-				sleepq_release(c);			\
-				break;					\
-			}						\
-			sleepq_add(c, NULL, "completion",		\
-			    SLEEPQ_SLEEP | SLEEPQ_INTERRUPTIBLE, 0);	\
-			if (sleepq_wait_sig(c, 0))			\
-				_error = -ERESTARTSYS;			\
-		}							\
-	}								\
-	-_error;							\
+void linux_wake_up(wait_queue_head_t *, unsigned int, int, bool);
+
+#define	wake_up(wqh)							\
+	linux_wake_up(wqh, TASK_NORMAL, 1, false)
+#define	wake_up_all(wqh)						\
+	linux_wake_up(wqh, TASK_NORMAL, 0, false)
+#define	wake_up_locked(wqh)						\
+	linux_wake_up(wqh, TASK_NORMAL, 1, true)
+#define	wake_up_all_locked(wqh)						\
+	linux_wake_up(wqh, TASK_NORMAL, 0, true)
+#define	wake_up_interruptible(wqh)					\
+	linux_wake_up(wqh, TASK_INTERRUPTIBLE, 1, false)
+#define	wake_up_interruptible_all(wqh)					\
+	linux_wake_up(wqh, TASK_INTERRUPTIBLE, 0, false)
+
+int linux_wait_event_common(wait_queue_head_t *, wait_queue_t *, int,
+    unsigned int, spinlock_t *);
+
+/*
+ * Returns -ERESTARTSYS for a signal, 0 if cond is false after timeout, 1 if
+ * cond is true after timeout, remaining jiffies (> 0) if cond is true before
+ * timeout.
+ */
+#define	__wait_event_common(wqh, cond, timeout, state, lock) ({	\
+	DEFINE_WAIT(__wq);					\
+	const int __timeout = ((int)(timeout)) < 1 ? 1 : (timeout);	\
+	int __start = ticks;					\
+	int __ret = 0;						\
+								\
+	for (;;) {						\
+		linux_prepare_to_wait(&(wqh), &__wq, state);	\
+		if (cond)					\
+			break;					\
+		__ret = linux_wait_event_common(&(wqh), &__wq,	\
+		    __timeout, state, lock);			\
+		if (__ret != 0)					\
+			break;					\
+	}							\
+	linux_finish_wait(&(wqh), &__wq);			\
+	if (__timeout != MAX_SCHEDULE_TIMEOUT) {		\
+		if (__ret == -EWOULDBLOCK)			\
+			__ret = !!(cond);			\
+		else if (__ret != -ERESTARTSYS) {		\
+			__ret = __timeout + __start - ticks;	\
+			/* range check return value */		\
+			if (__ret < 1)				\
+				__ret = 1;			\
+			else if (__ret > __timeout)		\
+				__ret = __timeout;		\
+		}						\
+	}							\
+	__ret;							\
 })
 
-#define	wait_event_interruptible_timeout(q, cond, timeout)		\
-({									\
-	void *c = &(q).wchan;						\
-	long end = jiffies + timeout;					\
-	int __ret = 0;	 						\
-	int __rc = 0;							\
-									\
-	if (!(cond)) {							\
-		for (; __rc == 0;) {					\
-			if (SCHEDULER_STOPPED())			\
-				break;					\
-			sleepq_lock(c);					\
-			if (cond) {					\
-				sleepq_release(c);			\
-				__ret = 1;				\
-				break;					\
-			}						\
-			sleepq_add(c, NULL, "completion",		\
-			SLEEPQ_SLEEP | SLEEPQ_INTERRUPTIBLE, 0);	\
-			sleepq_set_timeout(c, linux_timer_jiffies_until(end));\
-			__rc = sleepq_timedwait_sig (c, 0);		\
-			if (__rc != 0) {				\
-				/* check for timeout or signal. 	\
-				 * 0 if the condition evaluated to false\
-				 * after the timeout elapsed,  1 if the \
-				 * condition evaluated to true after the\
-				 * timeout elapsed.			\
-				 */					\
-				if (__rc == EWOULDBLOCK)		\
-					__ret = (cond);			\
-				 else					\
-					__ret = -ERESTARTSYS;		\
-			}						\
+#define	wait_event(wqh, cond) do {					\
+	(void) __wait_event_common(wqh, cond, MAX_SCHEDULE_TIMEOUT,	\
+	    TASK_UNINTERRUPTIBLE, NULL);				\
+} while (0)
+
+#define	wait_event_timeout(wqh, cond, timeout) ({			\
+	__wait_event_common(wqh, cond, timeout, TASK_UNINTERRUPTIBLE,	\
+	    NULL);							\
+})
+
+#define	wait_event_interruptible(wqh, cond) ({				\
+	__wait_event_common(wqh, cond, MAX_SCHEDULE_TIMEOUT,		\
+	    TASK_INTERRUPTIBLE, NULL);					\
+})
+
+#define	wait_event_interruptible_timeout(wqh, cond, timeout) ({		\
+	__wait_event_common(wqh, cond, timeout, TASK_INTERRUPTIBLE,	\
+	    NULL);							\
+})
+
+/*
+ * Wait queue is already locked.
+ */
+#define	wait_event_interruptible_locked(wqh, cond) ({			\
+	int __ret;							\
 									\
-		}							\
-	} else {							\
-		/* return remaining jiffies (at least 1) if the 	\
-		 * condition evaluated to true before the timeout	\
-		 * elapsed.						\
-		 */							\
-		__ret = (end - jiffies);				\
-		if( __ret < 1 )						\
-			__ret = 1;					\
-	}								\
+	spin_unlock(&(wqh).lock);					\
+	__ret = __wait_event_common(wqh, cond, MAX_SCHEDULE_TIMEOUT,	\
+	    TASK_INTERRUPTIBLE, NULL);					\
+	spin_lock(&(wqh).lock);						\
 	__ret;								\
 })
 
+/*
+ * Hold the (locked) spinlock when testing the cond.
+ */
+#define	wait_event_interruptible_lock_irq(wqh, cond, lock) ({		\
+	__wait_event_common(wqh, cond, MAX_SCHEDULE_TIMEOUT,		\
+	    TASK_INTERRUPTIBLE, &(lock));				\
+})
 
-static inline int
-waitqueue_active(wait_queue_head_t *q)
+static inline void
+__add_wait_queue(wait_queue_head_t *wqh, wait_queue_t *wq)
+{
+	list_add(&wq->task_list, &wqh->task_list);
+}
+
+static inline void
+add_wait_queue(wait_queue_head_t *wqh, wait_queue_t *wq)
 {
-	return 0;	/* XXX: not really implemented */
+
+	spin_lock(&wqh->lock);
+	__add_wait_queue(wqh, wq);
+	spin_unlock(&wqh->lock);
 }
 
-#define DEFINE_WAIT(name)	\
-	wait_queue_t name = {}
+static inline void
+__add_wait_queue_tail(wait_queue_head_t *wqh, wait_queue_t *wq)
+{
+	list_add_tail(&wq->task_list, &wqh->task_list);
+}
 
 static inline void
-prepare_to_wait(wait_queue_head_t *q, wait_queue_t *wait, int state)
+__remove_wait_queue(wait_queue_head_t *wqh, wait_queue_t *wq)
 {
+	list_del(&wq->task_list);
 }
 
 static inline void
-finish_wait(wait_queue_head_t *q, wait_queue_t *wait)
+remove_wait_queue(wait_queue_head_t *wqh, wait_queue_t *wq)
 {
+
+	spin_lock(&wqh->lock);
+	__remove_wait_queue(wqh, wq);
+	spin_unlock(&wqh->lock);
 }
 
-#endif	/* _LINUX_WAIT_H_ */
+bool linux_waitqueue_active(wait_queue_head_t *);
+
+#define	waitqueue_active(wqh)		linux_waitqueue_active(wqh)
+
+void linux_prepare_to_wait(wait_queue_head_t *, wait_queue_t *, int);
+void linux_finish_wait(wait_queue_head_t *, wait_queue_t *);
+
+#define	prepare_to_wait(wqh, wq, state)	linux_prepare_to_wait(wqh, wq, state)
+#define	finish_wait(wqh, wq)		linux_finish_wait(wqh, wq)
+
+void linux_wake_up_bit(void *, int);
+int linux_wait_on_bit_timeout(unsigned long *, int, unsigned int, int);
+void linux_wake_up_atomic_t(atomic_t *);
+int linux_wait_on_atomic_t(atomic_t *, unsigned int);
+
+#define	wake_up_bit(word, bit)		linux_wake_up_bit(word, bit)
+#define	wait_on_bit_timeout(word, bit, state, timeout)			\
+	linux_wait_on_bit_timeout(word, bit, state, timeout)
+#define	wake_up_atomic_t(a)		linux_wake_up_atomic_t(a)
+/*
+ * All existing callers have a cb that just schedule()s. To avoid adding
+ * complexity, just emulate that internally. The prototype is different so that
+ * callers must be manually modified; a cb that does something other than call
+ * schedule() will require special treatment.
+ */
+#define	wait_on_atomic_t(a, state)	linux_wait_on_atomic_t(a, state)
+
+struct task_struct;
+bool linux_wake_up_state(struct task_struct *, unsigned int);
+
+#define	wake_up_process(task)		linux_wake_up_state(task, TASK_NORMAL)
+#define	wake_up_state(task, state)	linux_wake_up_state(task, state)
+
+#endif /* _LINUX_WAIT_H_ */
diff --git a/sys/compat/linuxkpi/common/include/linux/workqueue.h b/sys/compat/linuxkpi/common/include/linux/workqueue.h
index 7eb2301175a..152a526c4e1 100644
--- a/sys/compat/linuxkpi/common/include/linux/workqueue.h
+++ b/sys/compat/linuxkpi/common/include/linux/workqueue.h
@@ -2,7 +2,7 @@
  * Copyright (c) 2010 Isilon Systems, Inc.
  * Copyright (c) 2010 iX Systems, Inc.
  * Copyright (c) 2010 Panasas, Inc.
- * Copyright (c) 2013-2015 Mellanox Technologies, Ltd.
+ * Copyright (c) 2013-2017 Mellanox Technologies, Ltd.
  * All rights reserved.
  *
  * Redistribution and use in source and binary forms, with or without
@@ -38,179 +38,195 @@
 
 #include <asm/atomic.h>
 
+#include <sys/param.h>
+#include <sys/kernel.h>
 #include <sys/taskqueue.h>
+#include <sys/mutex.h>
+
+#define	WORK_CPU_UNBOUND MAXCPU
+#define	WQ_UNBOUND (1 << 0)
+#define	WQ_HIGHPRI (1 << 1)
+
+struct work_struct;
+typedef void (*work_func_t)(struct work_struct *);
+
+struct work_exec {
+	TAILQ_ENTRY(work_exec) entry;
+	struct work_struct *target;
+};
 
 struct workqueue_struct {
-	struct taskqueue	*taskqueue;
-	atomic_t		draining;
+	struct taskqueue *taskqueue;
+	struct mtx exec_mtx;
+	TAILQ_HEAD(, work_exec) exec_head;
+	atomic_t draining;
 };
 
+#define	WQ_EXEC_LOCK(wq) mtx_lock(&(wq)->exec_mtx)
+#define	WQ_EXEC_UNLOCK(wq) mtx_unlock(&(wq)->exec_mtx)
+
 struct work_struct {
-	struct	task 		work_task;
-	struct	taskqueue	*taskqueue;
-	void			(*fn)(struct work_struct *);
+	struct task work_task;
+	struct workqueue_struct *work_queue;
+	work_func_t func;
+	atomic_t state;
 };
 
-typedef __typeof(((struct work_struct *)0)->fn) work_func_t;
+#define	DECLARE_WORK(name, fn)						\
+	struct work_struct name;					\
+	static void name##_init(void *arg)				\
+	{								\
+		INIT_WORK(&name, fn);					\
+	}								\
+	SYSINIT(name, SI_SUB_LOCK, SI_ORDER_SECOND, name##_init, NULL)
 
 struct delayed_work {
-	struct work_struct	work;
-	struct callout		timer;
+	struct work_struct work;
+	struct {
+		struct callout callout;
+		struct mtx mtx;
+		int	expires;
+	} timer;
 };
 
-extern void linux_work_fn(void *, int);
-extern void linux_flush_fn(void *, int);
-extern void linux_delayed_work_fn(void *);
-extern struct workqueue_struct *linux_create_workqueue_common(const char *, int);
-extern void destroy_workqueue(struct workqueue_struct *);
+#define	DECLARE_DELAYED_WORK(name, fn)					\
+	struct delayed_work name;					\
+	static void name##_init(void *arg)				\
+	{								\
+		linux_init_delayed_work(&name, fn);			\
+	}								\
+	SYSINIT(name, SI_SUB_LOCK, SI_ORDER_SECOND, name##_init, NULL)
 
 static inline struct delayed_work *
 to_delayed_work(struct work_struct *work)
 {
-
- 	return container_of(work, struct delayed_work, work);
+	return (container_of(work, struct delayed_work, work));
 }
 
-#define	INIT_WORK(work, func) 	 					\
+#define	INIT_WORK(work, fn) 	 					\
 do {									\
-	(work)->fn = (func);						\
-	(work)->taskqueue = NULL;					\
-	TASK_INIT(&(work)->work_task, 0, linux_work_fn, (work));		\
+	(work)->func = (fn);						\
+	(work)->work_queue = NULL;					\
+	atomic_set(&(work)->state, 0);					\
+	TASK_INIT(&(work)->work_task, 0, linux_work_fn, (work));	\
 } while (0)
 
-#define	INIT_DELAYED_WORK(_work, func)					\
-do {									\
-	INIT_WORK(&(_work)->work, func);				\
-	callout_init(&(_work)->timer, 1);				\
-} while (0)
+#define	INIT_WORK_ONSTACK(work, fn) \
+	INIT_WORK(work, fn)
 
-#define	INIT_DEFERRABLE_WORK(...) INIT_DELAYED_WORK(__VA_ARGS__)
+#define	INIT_DELAYED_WORK(dwork, fn) \
+	linux_init_delayed_work(dwork, fn)
 
-#define	schedule_work(work)						\
-do {									\
-	(work)->taskqueue = taskqueue_thread;				\
-	taskqueue_enqueue(taskqueue_thread, &(work)->work_task);	\
-} while (0)
+#define	INIT_DEFERRABLE_WORK(dwork, fn) \
+	INIT_DELAYED_WORK(dwork, fn)
 
-#define	flush_scheduled_work()	flush_taskqueue(taskqueue_thread)
+#define	flush_scheduled_work() \
+	taskqueue_drain_all(system_wq->taskqueue)
 
-static inline int
-queue_work(struct workqueue_struct *wq, struct work_struct *work)
-{
-	work->taskqueue = wq->taskqueue;
-	/* Check for draining */
-	if (atomic_read(&wq->draining) != 0)
-		return (!work->work_task.ta_pending);
-	/* Return opposite value to align with Linux logic */
-	return (!taskqueue_enqueue(wq->taskqueue, &work->work_task));
-}
+#define	queue_work(wq, work) \
+	linux_queue_work_on(WORK_CPU_UNBOUND, wq, work)
 
-static inline int
-queue_delayed_work(struct workqueue_struct *wq, struct delayed_work *work,
-    unsigned long delay)
-{
-	int pending;
-
-	work->work.taskqueue = wq->taskqueue;
-	if (atomic_read(&wq->draining) != 0) {
-	  	pending = work->work.work_task.ta_pending;
-	} else if (delay != 0) {
-		pending = work->work.work_task.ta_pending;
-		callout_reset(&work->timer, delay, linux_delayed_work_fn, work);
-	} else {
-		callout_stop(&work->timer);
-		pending = taskqueue_enqueue(work->work.taskqueue,
-		    &work->work.work_task);
-	}
-	return (!pending);
-}
+#define	schedule_work(work) \
+	linux_queue_work_on(WORK_CPU_UNBOUND, system_wq, work)
 
-static inline bool
-schedule_delayed_work(struct delayed_work *dwork,
-    unsigned long delay)
-{
-	struct workqueue_struct wq;
+#define	queue_delayed_work(wq, dwork, delay) \
+	linux_queue_delayed_work_on(WORK_CPU_UNBOUND, wq, dwork, delay)
 
-	wq.taskqueue = taskqueue_thread;
-	atomic_set(&wq.draining, 0);
-	return (queue_delayed_work(&wq, dwork, delay));
-}
+#define	schedule_delayed_work_on(cpu, dwork, delay) \
+	linux_queue_delayed_work_on(cpu, system_wq, dwork, delay)
+
+#define	queue_work_on(cpu, wq, work) \
+	linux_queue_work_on(cpu, wq, work)
+
+#define	schedule_delayed_work(dwork, delay) \
+	linux_queue_delayed_work_on(WORK_CPU_UNBOUND, system_wq, dwork, delay)
 
-#define	create_singlethread_workqueue(name)				\
+#define	queue_delayed_work_on(cpu, wq, dwork, delay) \
+	linux_queue_delayed_work_on(cpu, wq, dwork, delay)
+
+#define	create_singlethread_workqueue(name) \
 	linux_create_workqueue_common(name, 1)
 
-#define	create_workqueue(name)						\
-	linux_create_workqueue_common(name, MAXCPU)
+#define	create_workqueue(name) \
+	linux_create_workqueue_common(name, mp_ncpus)
 
-#define	alloc_ordered_workqueue(name, flags)				\
+#define	alloc_ordered_workqueue(name, flags) \
 	linux_create_workqueue_common(name, 1)
 
-#define	alloc_workqueue(name, flags, max_active)			\
+#define	alloc_workqueue(name, flags, max_active) \
 	linux_create_workqueue_common(name, max_active)
 
-#define	flush_workqueue(wq)	flush_taskqueue((wq)->taskqueue)
+#define	flush_workqueue(wq) \
+	taskqueue_drain_all((wq)->taskqueue)
 
-static inline void
-flush_taskqueue(struct taskqueue *tq)
-{
-	struct task flushtask;
+#define	drain_workqueue(wq) do {		\
+	atomic_inc(&(wq)->draining);		\
+	taskqueue_drain_all((wq)->taskqueue);	\
+	atomic_dec(&(wq)->draining);		\
+} while (0)
 
-	PHOLD(curproc);
-	TASK_INIT(&flushtask, 0, linux_flush_fn, NULL);
-	taskqueue_enqueue(tq, &flushtask);
-	taskqueue_drain(tq, &flushtask);
-	PRELE(curproc);
-}
+#define	mod_delayed_work(wq, dwork, delay) ({		\
+	bool __retval;					\
+	__retval = linux_cancel_delayed_work(dwork);	\
+	linux_queue_delayed_work_on(WORK_CPU_UNBOUND,	\
+	    wq, dwork, delay);				\
+	__retval;					\
+})
 
-static inline void
-drain_workqueue(struct workqueue_struct *wq)
-{
-	atomic_inc(&wq->draining);
-	flush_taskqueue(wq->taskqueue);
-	atomic_dec(&wq->draining);
-}
+#define	delayed_work_pending(dwork) \
+	linux_work_pending(&(dwork)->work)
 
-static inline int
-cancel_work_sync(struct work_struct *work)
-{
-	if (work->taskqueue &&
-	    taskqueue_cancel(work->taskqueue, &work->work_task, NULL))
-		taskqueue_drain(work->taskqueue, &work->work_task);
-	return 0;
-}
+#define	cancel_delayed_work(dwork) \
+	linux_cancel_delayed_work(dwork)
 
-/*
- * This may leave work running on another CPU as it does on Linux.
- */
-static inline int
-cancel_delayed_work(struct delayed_work *work)
-{
+#define	cancel_work_sync(work) \
+	linux_cancel_work_sync(work)
 
-	callout_stop(&work->timer);
-	if (work->work.taskqueue)
-		return (taskqueue_cancel(work->work.taskqueue,
-		    &work->work.work_task, NULL) == 0);
-	return 0;
-}
+#define	cancel_delayed_work_sync(dwork) \
+	linux_cancel_delayed_work_sync(dwork)
 
-static inline int
-cancel_delayed_work_sync(struct delayed_work *work)
-{
+#define	flush_work(work) \
+	linux_flush_work(work)
 
-        callout_drain(&work->timer);
-        if (work->work.taskqueue &&
-            taskqueue_cancel(work->work.taskqueue, &work->work.work_task, NULL))
-                taskqueue_drain(work->work.taskqueue, &work->work.work_task);
-        return 0;
-}
+#define	flush_delayed_work(dwork) \
+	linux_flush_delayed_work(dwork)
 
-static inline bool
-mod_delayed_work(struct workqueue_struct *wq, struct delayed_work *dwork,
-    unsigned long delay)
-{
-	cancel_delayed_work(dwork);
-	queue_delayed_work(wq, dwork, delay);
-	return false;
-}
+#define	work_pending(work) \
+	linux_work_pending(work)
+
+#define	work_busy(work) \
+	linux_work_busy(work)
+
+#define	destroy_work_on_stack(work) \
+	do { } while (0)
+
+#define	destroy_delayed_work_on_stack(dwork) \
+	do { } while (0)
 
-#endif	/* _LINUX_WORKQUEUE_H_ */
+#define	destroy_workqueue(wq) \
+	linux_destroy_workqueue(wq)
+
+/* prototypes */
+
+extern struct workqueue_struct *system_wq;
+extern struct workqueue_struct *system_long_wq;
+extern struct workqueue_struct *system_unbound_wq;
+extern struct workqueue_struct *system_power_efficient_wq;
+
+extern void linux_init_delayed_work(struct delayed_work *, work_func_t);
+extern void linux_work_fn(void *, int);
+extern void linux_delayed_work_fn(void *, int);
+extern struct workqueue_struct *linux_create_workqueue_common(const char *, int);
+extern void linux_destroy_workqueue(struct workqueue_struct *);
+extern bool linux_queue_work_on(int cpu, struct workqueue_struct *, struct work_struct *);
+extern bool linux_queue_delayed_work_on(int cpu, struct workqueue_struct *,
+    struct delayed_work *, unsigned delay);
+extern bool linux_cancel_delayed_work(struct delayed_work *);
+extern bool linux_cancel_work_sync(struct work_struct *);
+extern bool linux_cancel_delayed_work_sync(struct delayed_work *);
+extern bool linux_flush_work(struct work_struct *);
+extern bool linux_flush_delayed_work(struct delayed_work *);
+extern bool linux_work_pending(struct work_struct *);
+extern bool linux_work_busy(struct work_struct *);
+
+#endif					/* _LINUX_WORKQUEUE_H_ */
diff --git a/sys/compat/linuxkpi/common/include/linux/ww_mutex.h b/sys/compat/linuxkpi/common/include/linux/ww_mutex.h
new file mode 100644
index 00000000000..0c816f19889
--- /dev/null
+++ b/sys/compat/linuxkpi/common/include/linux/ww_mutex.h
@@ -0,0 +1,142 @@
+/*-
+ * Copyright (c) 2017 Mellanox Technologies, Ltd.
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice unmodified, this list of conditions, and the following
+ *    disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS OR
+ * IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+ * OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.
+ * IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT,
+ * INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT
+ * NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+ * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+ * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF
+ * THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ *
+ * $FreeBSD$
+ */
+#ifndef	_LINUX_WW_MUTEX_H_
+#define	_LINUX_WW_MUTEX_H_
+
+#include <sys/param.h>
+#include <sys/proc.h>
+#include <sys/condvar.h>
+#include <sys/kernel.h>
+
+#include <linux/mutex.h>
+
+struct ww_class {
+	const char *mutex_name;
+};
+
+struct ww_acquire_ctx {
+};
+
+struct ww_mutex {
+	struct mutex base;
+	struct cv condvar;
+};
+
+#define	DEFINE_WW_CLASS(name)					\
+	struct ww_class name = {				\
+		.mutex_name = mutex_name(#name "_mutex")	\
+	}
+
+#define	DEFINE_WW_MUTEX(name, ww_class)					\
+	struct ww_mutex name;						\
+	static void name##_init(void *arg)				\
+	{								\
+		ww_mutex_init(&name, &ww_class);			\
+	}								\
+	SYSINIT(name, SI_SUB_LOCK, SI_ORDER_SECOND, name##_init, NULL)
+
+#define	ww_mutex_is_locked(_m) \
+	sx_xlocked(&(_m)->base.sx)
+
+#define	ww_mutex_lock_slow(_m, _x) \
+	ww_mutex_lock(_m, _x)
+
+#define	ww_mutex_lock_slow_interruptible(_m, _x) \
+	ww_mutex_lock_interruptible(_m, _x)
+
+static inline int __must_check
+ww_mutex_trylock(struct ww_mutex *lock)
+{
+	return (mutex_trylock(&lock->base));
+}
+
+extern int linux_ww_mutex_lock_sub(struct ww_mutex *, int catch_signal);
+
+static inline int
+ww_mutex_lock(struct ww_mutex *lock, struct ww_acquire_ctx *ctx)
+{
+	if (MUTEX_SKIP())
+		return (0);
+	else if ((struct thread *)SX_OWNER(lock->base.sx.sx_lock) == curthread)
+		return (-EALREADY);
+	else
+		return (linux_ww_mutex_lock_sub(lock, 0));
+}
+
+static inline int
+ww_mutex_lock_interruptible(struct ww_mutex *lock, struct ww_acquire_ctx *ctx)
+{
+	if (MUTEX_SKIP())
+		return (0);
+	else if ((struct thread *)SX_OWNER(lock->base.sx.sx_lock) == curthread)
+		return (-EALREADY);
+	else
+		return (linux_ww_mutex_lock_sub(lock, 1));
+}
+
+extern void linux_ww_mutex_unlock_sub(struct ww_mutex *);
+
+static inline void
+ww_mutex_unlock(struct ww_mutex *lock)
+{
+	if (MUTEX_SKIP())
+		return;
+	else
+		linux_ww_mutex_unlock_sub(lock);
+}
+
+static inline void
+ww_mutex_destroy(struct ww_mutex *lock)
+{
+	cv_destroy(&lock->condvar);
+	mutex_destroy(&lock->base);
+}
+
+static inline void
+ww_acquire_init(struct ww_acquire_ctx *ctx, struct ww_class *ww_class)
+{
+}
+
+static inline void
+ww_mutex_init(struct ww_mutex *lock, struct ww_class *ww_class)
+{
+	linux_mutex_init(&lock->base, ww_class->mutex_name, SX_NOWITNESS);
+	cv_init(&lock->condvar, "lkpi-ww");
+}
+
+static inline void
+ww_acquire_fini(struct ww_acquire_ctx *ctx)
+{
+}
+
+static inline void
+ww_acquire_done(struct ww_acquire_ctx *ctx)
+{
+}
+
+#endif					/* _LINUX_WW_MUTEX_H_ */
diff --git a/sys/compat/linuxkpi/common/include/net/ip.h b/sys/compat/linuxkpi/common/include/net/ip.h
index 7fbe9d99e2a..1c278734b2a 100644
--- a/sys/compat/linuxkpi/common/include/net/ip.h
+++ b/sys/compat/linuxkpi/common/include/net/ip.h
@@ -43,10 +43,11 @@
 #include <netinet/in.h>
 #include <netinet/in_pcb.h>
 
-static inline void inet_get_local_port_range(int *low, int *high)
+static inline void
+inet_get_local_port_range(struct vnet *vnet, int *low, int *high)
 {
 #ifdef INET
-	CURVNET_SET_QUIET(TD_TO_VNET(curthread));
+	CURVNET_SET_QUIET(vnet);
 	*low = V_ipport_firstauto;
 	*high = V_ipport_lastauto;
 	CURVNET_RESTORE();
diff --git a/sys/compat/linuxkpi/common/include/net/ipv6.h b/sys/compat/linuxkpi/common/include/net/ipv6.h
index 7e078f378cd..4e48da42c44 100644
--- a/sys/compat/linuxkpi/common/include/net/ipv6.h
+++ b/sys/compat/linuxkpi/common/include/net/ipv6.h
@@ -35,8 +35,12 @@
 #include <netinet/in.h>
 #include <linux/types.h>
 
-#define	ipv6_addr_loopback IN6_IS_ADDR_LOOPBACK
-#define	ipv6_addr_copy(dst, src)					\
+#define	IPV6_DEFAULT_HOPLIMIT 64
+
+#define	ipv6_addr_loopback(addr) IN6_IS_ADDR_LOOPBACK(addr)
+#define	ipv6_addr_any(addr) IN6_IS_ADDR_UNSPECIFIED(addr)
+
+#define	ipv6_addr_copy(dst, src)			\
 	memcpy((dst), (src), sizeof(struct in6_addr))
 
 static inline void
diff --git a/sys/compat/linuxkpi/common/src/linux_compat.c b/sys/compat/linuxkpi/common/src/linux_compat.c
index d146f3c971d..57a6774464e 100644
--- a/sys/compat/linuxkpi/common/src/linux_compat.c
+++ b/sys/compat/linuxkpi/common/src/linux_compat.c
@@ -2,7 +2,7 @@
  * Copyright (c) 2010 Isilon Systems, Inc.
  * Copyright (c) 2010 iX Systems, Inc.
  * Copyright (c) 2010 Panasas, Inc.
- * Copyright (c) 2013-2016 Mellanox Technologies, Ltd.
+ * Copyright (c) 2013-2017 Mellanox Technologies, Ltd.
  * All rights reserved.
  *
  * Redistribution and use in source and binary forms, with or without
@@ -48,6 +48,9 @@ __FBSDID("$FreeBSD$");
 
 #include <vm/vm.h>
 #include <vm/pmap.h>
+#include <vm/vm_object.h>
+#include <vm/vm_page.h>
+#include <vm/vm_pager.h>
 
 #include <machine/stdarg.h>
 
@@ -68,15 +71,18 @@ __FBSDID("$FreeBSD$");
 #include <linux/vmalloc.h>
 #include <linux/netdevice.h>
 #include <linux/timer.h>
-#include <linux/workqueue.h>
-#include <linux/rcupdate.h>
 #include <linux/interrupt.h>
 #include <linux/uaccess.h>
-#include <linux/kernel.h>
 #include <linux/list.h>
+#include <linux/kthread.h>
+#include <linux/kernel.h>
 #include <linux/compat.h>
+#include <linux/poll.h>
+#include <linux/smp.h>
 
-#include <vm/vm_pager.h>
+#if defined(__i386__) || defined(__amd64__)
+#include <asm/smp.h>
+#endif
 
 SYSCTL_NODE(_compat, OID_AUTO, linuxkpi, CTLFLAG_RW, 0, "LinuxKPI parameters");
 
@@ -89,13 +95,14 @@ MALLOC_DEFINE(M_KMALLOC, "linux", "Linux kmalloc compat");
 #undef cdev
 #define	RB_ROOT(head)	(head)->rbh_root
 
+static struct vm_area_struct *linux_cdev_handle_find(void *handle);
+
 struct kobject linux_class_root;
 struct device linux_root_device;
 struct class linux_class_misc;
 struct list_head pci_drivers;
 struct list_head pci_devices;
 spinlock_t pci_lock;
-struct sx linux_global_rcu_lock;
 
 unsigned long linux_timer_hz_mask;
 
@@ -383,42 +390,299 @@ kobject_init_and_add(struct kobject *kobj, const struct kobj_type *ktype,
 	return kobject_add_complete(kobj, parent);
 }
 
-void
-linux_set_current(struct thread *td, struct task_struct *t)
+static void
+linux_file_dtor(void *cdp)
 {
-	memset(t, 0, sizeof(*t));
-	task_struct_fill(td, t);
-	task_struct_set(td, t);
+	struct linux_file *filp;
+
+	linux_set_current(curthread);
+	filp = cdp;
+	filp->f_op->release(filp->f_vnode, filp);
+	vdrop(filp->f_vnode);
+	kfree(filp);
 }
 
-void
-linux_clear_current(struct thread *td)
+static void
+linux_kq_lock(void *arg)
+{
+	spinlock_t *s = arg;
+
+	spin_lock(s);
+}
+static void
+linux_kq_unlock(void *arg)
+{
+	spinlock_t *s = arg;
+
+	spin_unlock(s);
+}
+
+static void
+linux_kq_lock_owned(void *arg)
 {
-	task_struct_set(td, NULL);
+#ifdef INVARIANTS
+	spinlock_t *s = arg;
+
+	mtx_assert(&s->m, MA_OWNED);
+#endif
 }
 
 static void
-linux_file_dtor(void *cdp)
+linux_kq_lock_unowned(void *arg)
+{
+#ifdef INVARIANTS
+	spinlock_t *s = arg;
+
+	mtx_assert(&s->m, MA_NOTOWNED);
+#endif
+}
+
+static void
+linux_dev_kqfilter_poll(struct linux_file *, int);
+
+struct linux_file *
+linux_file_alloc(void)
 {
 	struct linux_file *filp;
-	struct task_struct t;
-	struct thread *td;
 
-	td = curthread;
-	filp = cdp;
-	linux_set_current(td, &t);
-	filp->f_op->release(filp->f_vnode, filp);
-	linux_clear_current(td);
-	vdrop(filp->f_vnode);
-	kfree(filp);
+	filp = kzalloc(sizeof(*filp), GFP_KERNEL);
+
+	/* set initial refcount */
+	filp->f_count = 1;
+
+	/* setup fields needed by kqueue support */
+	spin_lock_init(&filp->f_kqlock);
+	knlist_init(&filp->f_selinfo.si_note, &filp->f_kqlock,
+	    linux_kq_lock, linux_kq_unlock,
+	    linux_kq_lock_owned, linux_kq_lock_unowned);
+
+	return (filp);
+}
+
+void
+linux_file_free(struct linux_file *filp)
+{
+	if (filp->_file == NULL) {
+		if (filp->f_shmem != NULL)
+			vm_object_deallocate(filp->f_shmem);
+		kfree(filp);
+	} else {
+		/*
+		 * The close method of the character device or file
+		 * will free the linux_file structure:
+		 */
+		_fdrop(filp->_file, curthread);
+	}
+}
+
+static int
+linux_cdev_pager_fault(vm_object_t vm_obj, vm_ooffset_t offset, int prot,
+    vm_page_t *mres)
+{
+	struct vm_area_struct *vmap;
+
+	vmap = linux_cdev_handle_find(vm_obj->handle);
+
+	MPASS(vmap != NULL);
+	MPASS(vmap->vm_private_data == vm_obj->handle);
+
+	if (likely(vmap->vm_ops != NULL && offset < vmap->vm_len)) {
+		vm_paddr_t paddr = IDX_TO_OFF(vmap->vm_pfn) + offset;
+		vm_page_t page;
+
+		if (((*mres)->flags & PG_FICTITIOUS) != 0) {
+			/*
+			 * If the passed in result page is a fake
+			 * page, update it with the new physical
+			 * address.
+			 */
+			page = *mres;
+			vm_page_updatefake(page, paddr, vm_obj->memattr);
+		} else {
+			/*
+			 * Replace the passed in "mres" page with our
+			 * own fake page and free up the all of the
+			 * original pages.
+			 */
+			VM_OBJECT_WUNLOCK(vm_obj);
+			page = vm_page_getfake(paddr, vm_obj->memattr);
+			VM_OBJECT_WLOCK(vm_obj);
+
+			vm_page_replace_checked(page, vm_obj,
+			    (*mres)->pindex, *mres);
+
+			vm_page_lock(*mres);
+			vm_page_free(*mres);
+			vm_page_unlock(*mres);
+			*mres = page;
+		}
+		page->valid = VM_PAGE_BITS_ALL;
+		return (VM_PAGER_OK);
+	}
+	return (VM_PAGER_FAIL);
+}
+
+static int
+linux_cdev_pager_populate(vm_object_t vm_obj, vm_pindex_t pidx, int fault_type,
+    vm_prot_t max_prot, vm_pindex_t *first, vm_pindex_t *last)
+{
+	struct vm_area_struct *vmap;
+	int err;
+
+	linux_set_current(curthread);
+
+	/* get VM area structure */
+	vmap = linux_cdev_handle_find(vm_obj->handle);
+	MPASS(vmap != NULL);
+	MPASS(vmap->vm_private_data == vm_obj->handle);
+
+	VM_OBJECT_WUNLOCK(vm_obj);
+
+	down_write(&vmap->vm_mm->mmap_sem);
+	if (unlikely(vmap->vm_ops == NULL)) {
+		err = VM_FAULT_SIGBUS;
+	} else {
+		struct vm_fault vmf;
+
+		/* fill out VM fault structure */
+		vmf.virtual_address = (void *)((uintptr_t)pidx << PAGE_SHIFT);
+		vmf.flags = (fault_type & VM_PROT_WRITE) ? FAULT_FLAG_WRITE : 0;
+		vmf.pgoff = 0;
+		vmf.page = NULL;
+
+		vmap->vm_pfn_count = 0;
+		vmap->vm_pfn_pcount = &vmap->vm_pfn_count;
+		vmap->vm_obj = vm_obj;
+
+		err = vmap->vm_ops->fault(vmap, &vmf);
+
+		while (vmap->vm_pfn_count == 0 && err == VM_FAULT_NOPAGE) {
+			kern_yield(PRI_USER);
+			err = vmap->vm_ops->fault(vmap, &vmf);
+		}
+	}
+
+	/* translate return code */
+	switch (err) {
+	case VM_FAULT_OOM:
+		err = VM_PAGER_AGAIN;
+		break;
+	case VM_FAULT_SIGBUS:
+		err = VM_PAGER_BAD;
+		break;
+	case VM_FAULT_NOPAGE:
+		/*
+		 * By contract the fault handler will return having
+		 * busied all the pages itself. If pidx is already
+		 * found in the object, it will simply xbusy the first
+		 * page and return with vm_pfn_count set to 1.
+		 */
+		*first = vmap->vm_pfn_first;
+		*last = *first + vmap->vm_pfn_count - 1;
+		err = VM_PAGER_OK;
+		break;
+	default:
+		err = VM_PAGER_ERROR;
+		break;
+	}
+	up_write(&vmap->vm_mm->mmap_sem);
+	VM_OBJECT_WLOCK(vm_obj);
+	return (err);
+}
+
+static struct rwlock linux_vma_lock;
+static TAILQ_HEAD(, vm_area_struct) linux_vma_head =
+    TAILQ_HEAD_INITIALIZER(linux_vma_head);
+
+static void
+linux_cdev_handle_free(struct vm_area_struct *vmap)
+{
+	/* Drop reference on vm_file */
+	if (vmap->vm_file != NULL)
+		fput(vmap->vm_file);
+
+	/* Drop reference on mm_struct */
+	mmput(vmap->vm_mm);
+
+	kfree(vmap);
 }
 
+static void
+linux_cdev_handle_remove(struct vm_area_struct *vmap)
+{
+	rw_wlock(&linux_vma_lock);
+	TAILQ_REMOVE(&linux_vma_head, vmap, vm_entry);
+	rw_wunlock(&linux_vma_lock);
+}
+
+static struct vm_area_struct *
+linux_cdev_handle_find(void *handle)
+{
+	struct vm_area_struct *vmap;
+
+	rw_rlock(&linux_vma_lock);
+	TAILQ_FOREACH(vmap, &linux_vma_head, vm_entry) {
+		if (vmap->vm_private_data == handle)
+			break;
+	}
+	rw_runlock(&linux_vma_lock);
+	return (vmap);
+}
+
+static int
+linux_cdev_pager_ctor(void *handle, vm_ooffset_t size, vm_prot_t prot,
+		      vm_ooffset_t foff, struct ucred *cred, u_short *color)
+{
+
+	MPASS(linux_cdev_handle_find(handle) != NULL);
+	*color = 0;
+	return (0);
+}
+
+static void
+linux_cdev_pager_dtor(void *handle)
+{
+	const struct vm_operations_struct *vm_ops;
+	struct vm_area_struct *vmap;
+
+	vmap = linux_cdev_handle_find(handle);
+	MPASS(vmap != NULL);
+
+	/*
+	 * Remove handle before calling close operation to prevent
+	 * other threads from reusing the handle pointer.
+	 */
+	linux_cdev_handle_remove(vmap);
+
+	down_write(&vmap->vm_mm->mmap_sem);
+	vm_ops = vmap->vm_ops;
+	if (likely(vm_ops != NULL))
+		vm_ops->close(vmap);
+	up_write(&vmap->vm_mm->mmap_sem);
+
+	linux_cdev_handle_free(vmap);
+}
+
+static struct cdev_pager_ops linux_cdev_pager_ops[2] = {
+  {
+	/* OBJT_MGTDEVICE */
+	.cdev_pg_populate	= linux_cdev_pager_populate,
+	.cdev_pg_ctor	= linux_cdev_pager_ctor,
+	.cdev_pg_dtor	= linux_cdev_pager_dtor
+  },
+  {
+	/* OBJT_DEVICE */
+	.cdev_pg_fault	= linux_cdev_pager_fault,
+	.cdev_pg_ctor	= linux_cdev_pager_ctor,
+	.cdev_pg_dtor	= linux_cdev_pager_dtor
+  },
+};
+
 static int
 linux_dev_open(struct cdev *dev, int oflags, int devtype, struct thread *td)
 {
 	struct linux_cdev *ldev;
 	struct linux_file *filp;
-	struct task_struct t;
 	struct file *file;
 	int error;
 
@@ -426,16 +690,21 @@ linux_dev_open(struct cdev *dev, int oflags, int devtype, struct thread *td)
 	ldev = dev->si_drv1;
 	if (ldev == NULL)
 		return (ENODEV);
-	filp = kzalloc(sizeof(*filp), GFP_KERNEL);
+
+	filp = linux_file_alloc();
 	filp->f_dentry = &filp->f_dentry_store;
 	filp->f_op = ldev->ops;
 	filp->f_flags = file->f_flag;
 	vhold(file->f_vnode);
 	filp->f_vnode = file->f_vnode;
-	linux_set_current(td, &t);
+	filp->_file = file;
+
+	linux_set_current(td);
+
 	if (filp->f_op->open) {
 		error = -filp->f_op->open(file->f_vnode, filp);
 		if (error) {
+			vdrop(filp->f_vnode);
 			kfree(filp);
 			goto done;
 		}
@@ -443,30 +712,25 @@ linux_dev_open(struct cdev *dev, int oflags, int devtype, struct thread *td)
 	error = devfs_set_cdevpriv(filp, linux_file_dtor);
 	if (error) {
 		filp->f_op->release(file->f_vnode, filp);
+		vdrop(filp->f_vnode);
 		kfree(filp);
 	}
 done:
-	linux_clear_current(td);
 	return (error);
 }
 
 static int
 linux_dev_close(struct cdev *dev, int fflag, int devtype, struct thread *td)
 {
-	struct linux_cdev *ldev;
 	struct linux_file *filp;
 	struct file *file;
 	int error;
 
 	file = td->td_fpop;
-	ldev = dev->si_drv1;
-	if (ldev == NULL)
-		return (0);
 	if ((error = devfs_get_cdevpriv((void **)&filp)) != 0)
 		return (error);
 	filp->f_flags = file->f_flag;
-        devfs_clear_cdevpriv();
-        
+	devfs_clear_cdevpriv();
 
 	return (0);
 }
@@ -531,25 +795,79 @@ linux_copyout(const void *kaddr, void *uaddr, size_t len)
 	return (-copyout(kaddr, uaddr, len));
 }
 
+size_t
+linux_clear_user(void *_uaddr, size_t _len)
+{
+	uint8_t *uaddr = _uaddr;
+	size_t len = _len;
+
+	/* make sure uaddr is aligned before going into the fast loop */
+	while (((uintptr_t)uaddr & 7) != 0 && len > 7) {
+		if (subyte(uaddr, 0))
+			return (_len);
+		uaddr++;
+		len--;
+	}
+
+	/* zero 8 bytes at a time */
+	while (len > 7) {
+#ifdef __LP64__
+		if (suword64(uaddr, 0))
+			return (_len);
+#else
+		if (suword32(uaddr, 0))
+			return (_len);
+		if (suword32(uaddr + 4, 0))
+			return (_len);
+#endif
+		uaddr += 8;
+		len -= 8;
+	}
+
+	/* zero fill end, if any */
+	while (len > 0) {
+		if (subyte(uaddr, 0))
+			return (_len);
+		uaddr++;
+		len--;
+	}
+	return (0);
+}
+
+int
+linux_access_ok(int rw, const void *uaddr, size_t len)
+{
+	uintptr_t saddr;
+	uintptr_t eaddr;
+
+	/* get start and end address */
+	saddr = (uintptr_t)uaddr;
+	eaddr = (uintptr_t)uaddr + len;
+
+	/* verify addresses are valid for userspace */
+	return ((saddr == eaddr) ||
+	    (eaddr > saddr && eaddr <= VM_MAXUSER_ADDRESS));
+}
+
 static int
 linux_dev_ioctl(struct cdev *dev, u_long cmd, caddr_t data, int fflag,
     struct thread *td)
 {
-	struct linux_cdev *ldev;
 	struct linux_file *filp;
-	struct task_struct t;
 	struct file *file;
 	unsigned size;
 	int error;
 
 	file = td->td_fpop;
-	ldev = dev->si_drv1;
-	if (ldev == NULL)
-		return (0);
 	if ((error = devfs_get_cdevpriv((void **)&filp)) != 0)
 		return (error);
 	filp->f_flags = file->f_flag;
-	linux_set_current(td, &t);
+
+	/* the LinuxKPI supports blocking and non-blocking I/O */
+	if (cmd == FIONBIO || cmd == FIOASYNC)
+		return (0);
+
+	linux_set_current(td);
 	size = IOCPARM_LEN(cmd);
 	/* refer to logic in sys_ioctl() */
 	if (size > 0) {
@@ -559,28 +877,48 @@ linux_dev_ioctl(struct cdev *dev, u_long cmd, caddr_t data, int fflag,
 		 * Background: Linux code expects a user-space address
 		 * while FreeBSD supplies a kernel-space address.
 		 */
-		t.bsd_ioctl_data = data;
-		t.bsd_ioctl_len = size;
+		current->bsd_ioctl_data = data;
+		current->bsd_ioctl_len = size;
 		data = (void *)LINUX_IOCTL_MIN_PTR;
 	} else {
 		/* fetch user-space pointer */
 		data = *(void **)data;
 	}
-	if (filp->f_op->unlocked_ioctl)
+#if defined(__amd64__)
+	if (td->td_proc->p_elf_machine == EM_386) {
+		/* try the compat IOCTL handler first */
+		if (filp->f_op->compat_ioctl != NULL)
+			error = -filp->f_op->compat_ioctl(filp, cmd, (u_long)data);
+		else
+			error = ENOTTY;
+
+		/* fallback to the regular IOCTL handler, if any */
+		if (error == ENOTTY && filp->f_op->unlocked_ioctl != NULL)
+			error = -filp->f_op->unlocked_ioctl(filp, cmd, (u_long)data);
+	} else
+#endif
+	if (filp->f_op->unlocked_ioctl != NULL)
 		error = -filp->f_op->unlocked_ioctl(filp, cmd, (u_long)data);
 	else
 		error = ENOTTY;
-	linux_clear_current(td);
+	if (size > 0) {
+		current->bsd_ioctl_data = NULL;
+		current->bsd_ioctl_len = 0;
+	}
 
+	if (error == EWOULDBLOCK) {
+		/* update kqfilter status, if any */
+		linux_dev_kqfilter_poll(filp,
+		    LINUX_KQ_FLAG_HAS_READ | LINUX_KQ_FLAG_HAS_WRITE);
+	} else if (error == ERESTARTSYS)
+		error = ERESTART;
 	return (error);
 }
 
 static int
 linux_dev_read(struct cdev *dev, struct uio *uio, int ioflag)
 {
-	struct linux_cdev *ldev;
 	struct linux_file *filp;
-	struct task_struct t;
 	struct thread *td;
 	struct file *file;
 	ssize_t bytes;
@@ -588,16 +926,13 @@ linux_dev_read(struct cdev *dev, struct uio *uio, int ioflag)
 
 	td = curthread;
 	file = td->td_fpop;
-	ldev = dev->si_drv1;
-	if (ldev == NULL)
-		return (0);
 	if ((error = devfs_get_cdevpriv((void **)&filp)) != 0)
 		return (error);
 	filp->f_flags = file->f_flag;
 	/* XXX no support for I/O vectors currently */
 	if (uio->uio_iovcnt != 1)
 		return (EOPNOTSUPP);
-	linux_set_current(td, &t);
+	linux_set_current(td);
 	if (filp->f_op->read) {
 		bytes = filp->f_op->read(filp, uio->uio_iov->iov_base,
 		    uio->uio_iov->iov_len, &uio->uio_offset);
@@ -606,11 +941,16 @@ linux_dev_read(struct cdev *dev, struct uio *uio, int ioflag)
 			    ((uint8_t *)uio->uio_iov->iov_base) + bytes;
 			uio->uio_iov->iov_len -= bytes;
 			uio->uio_resid -= bytes;
-		} else
+		} else {
 			error = -bytes;
+			if (error == ERESTARTSYS)
+				error = ERESTART;
+		}
 	} else
 		error = ENXIO;
-	linux_clear_current(td);
+
+	/* update kqfilter status, if any */
+	linux_dev_kqfilter_poll(filp, LINUX_KQ_FLAG_HAS_READ);
 
 	return (error);
 }
@@ -618,9 +958,7 @@ linux_dev_read(struct cdev *dev, struct uio *uio, int ioflag)
 static int
 linux_dev_write(struct cdev *dev, struct uio *uio, int ioflag)
 {
-	struct linux_cdev *ldev;
 	struct linux_file *filp;
-	struct task_struct t;
 	struct thread *td;
 	struct file *file;
 	ssize_t bytes;
@@ -628,16 +966,13 @@ linux_dev_write(struct cdev *dev, struct uio *uio, int ioflag)
 
 	td = curthread;
 	file = td->td_fpop;
-	ldev = dev->si_drv1;
-	if (ldev == NULL)
-		return (0);
 	if ((error = devfs_get_cdevpriv((void **)&filp)) != 0)
 		return (error);
 	filp->f_flags = file->f_flag;
 	/* XXX no support for I/O vectors currently */
 	if (uio->uio_iovcnt != 1)
 		return (EOPNOTSUPP);
-	linux_set_current(td, &t);
+	linux_set_current(td);
 	if (filp->f_op->write) {
 		bytes = filp->f_op->write(filp, uio->uio_iov->iov_base,
 		    uio->uio_iov->iov_len, &uio->uio_offset);
@@ -646,96 +981,418 @@ linux_dev_write(struct cdev *dev, struct uio *uio, int ioflag)
 			    ((uint8_t *)uio->uio_iov->iov_base) + bytes;
 			uio->uio_iov->iov_len -= bytes;
 			uio->uio_resid -= bytes;
-		} else
+		} else {
 			error = -bytes;
+			if (error == ERESTARTSYS)
+				error = ERESTART;
+		}
 	} else
 		error = ENXIO;
-	linux_clear_current(td);
+
+	/* update kqfilter status, if any */
+	linux_dev_kqfilter_poll(filp, LINUX_KQ_FLAG_HAS_WRITE);
 
 	return (error);
 }
 
+#define	LINUX_POLL_TABLE_NORMAL ((poll_table *)1)
+
 static int
 linux_dev_poll(struct cdev *dev, int events, struct thread *td)
 {
-	struct linux_cdev *ldev;
 	struct linux_file *filp;
-	struct task_struct t;
 	struct file *file;
 	int revents;
-	int error;
+
+	if (devfs_get_cdevpriv((void **)&filp) != 0)
+		goto error;
 
 	file = td->td_fpop;
-	ldev = dev->si_drv1;
-	if (ldev == NULL)
-		return (0);
-	if ((error = devfs_get_cdevpriv((void **)&filp)) != 0)
-		return (error);
 	filp->f_flags = file->f_flag;
-	linux_set_current(td, &t);
-	if (filp->f_op->poll)
-		revents = filp->f_op->poll(filp, NULL) & events;
+	linux_set_current(td);
+	if (filp->f_op->poll != NULL)
+		revents = filp->f_op->poll(filp, LINUX_POLL_TABLE_NORMAL) & events;
 	else
 		revents = 0;
-	linux_clear_current(td);
 
 	return (revents);
+error:
+	return (events & (POLLHUP|POLLIN|POLLRDNORM|POLLOUT|POLLWRNORM));
+}
+
+/*
+ * This function atomically updates the poll wakeup state and returns
+ * the previous state at the time of update.
+ */
+static uint8_t
+linux_poll_wakeup_state(atomic_t *v, const uint8_t *pstate)
+{
+	int c, old;
+
+	c = v->counter;
+
+	while ((old = atomic_cmpxchg(v, c, pstate[c])) != c)
+		c = old;
+
+	return (c);
+}
+
+
+static int
+linux_poll_wakeup_callback(wait_queue_t *wq, unsigned int wq_state, int flags, void *key)
+{
+	static const uint8_t state[LINUX_FWQ_STATE_MAX] = {
+		[LINUX_FWQ_STATE_INIT] = LINUX_FWQ_STATE_INIT, /* NOP */
+		[LINUX_FWQ_STATE_NOT_READY] = LINUX_FWQ_STATE_NOT_READY, /* NOP */
+		[LINUX_FWQ_STATE_QUEUED] = LINUX_FWQ_STATE_READY,
+		[LINUX_FWQ_STATE_READY] = LINUX_FWQ_STATE_READY, /* NOP */
+	};
+	struct linux_file *filp = container_of(wq, struct linux_file, f_wait_queue.wq);
+
+	switch (linux_poll_wakeup_state(&filp->f_wait_queue.state, state)) {
+	case LINUX_FWQ_STATE_QUEUED:
+		linux_poll_wakeup(filp);
+		return (1);
+	default:
+		return (0);
+	}
+}
+
+void
+linux_poll_wait(struct linux_file *filp, wait_queue_head_t *wqh, poll_table *p)
+{
+	static const uint8_t state[LINUX_FWQ_STATE_MAX] = {
+		[LINUX_FWQ_STATE_INIT] = LINUX_FWQ_STATE_NOT_READY,
+		[LINUX_FWQ_STATE_NOT_READY] = LINUX_FWQ_STATE_NOT_READY, /* NOP */
+		[LINUX_FWQ_STATE_QUEUED] = LINUX_FWQ_STATE_QUEUED, /* NOP */
+		[LINUX_FWQ_STATE_READY] = LINUX_FWQ_STATE_QUEUED,
+	};
+
+	/* check if we are called inside the select system call */
+	if (p == LINUX_POLL_TABLE_NORMAL)
+		selrecord(curthread, &filp->f_selinfo);
+
+	switch (linux_poll_wakeup_state(&filp->f_wait_queue.state, state)) {
+	case LINUX_FWQ_STATE_INIT:
+		/* NOTE: file handles can only belong to one wait-queue */
+		filp->f_wait_queue.wqh = wqh;
+		filp->f_wait_queue.wq.func = &linux_poll_wakeup_callback;
+		add_wait_queue(wqh, &filp->f_wait_queue.wq);
+		atomic_set(&filp->f_wait_queue.state, LINUX_FWQ_STATE_QUEUED);
+		break;
+	default:
+		break;
+	}
+}
+
+static void
+linux_poll_wait_dequeue(struct linux_file *filp)
+{
+	static const uint8_t state[LINUX_FWQ_STATE_MAX] = {
+		[LINUX_FWQ_STATE_INIT] = LINUX_FWQ_STATE_INIT,	/* NOP */
+		[LINUX_FWQ_STATE_NOT_READY] = LINUX_FWQ_STATE_INIT,
+		[LINUX_FWQ_STATE_QUEUED] = LINUX_FWQ_STATE_INIT,
+		[LINUX_FWQ_STATE_READY] = LINUX_FWQ_STATE_INIT,
+	};
+
+	seldrain(&filp->f_selinfo);
+
+	switch (linux_poll_wakeup_state(&filp->f_wait_queue.state, state)) {
+	case LINUX_FWQ_STATE_NOT_READY:
+	case LINUX_FWQ_STATE_QUEUED:
+	case LINUX_FWQ_STATE_READY:
+		remove_wait_queue(filp->f_wait_queue.wqh, &filp->f_wait_queue.wq);
+		break;
+	default:
+		break;
+	}
+}
+
+void
+linux_poll_wakeup(struct linux_file *filp)
+{
+	/* this function should be NULL-safe */
+	if (filp == NULL)
+		return;
+
+	selwakeup(&filp->f_selinfo);
+
+	spin_lock(&filp->f_kqlock);
+	filp->f_kqflags |= LINUX_KQ_FLAG_NEED_READ |
+	    LINUX_KQ_FLAG_NEED_WRITE;
+
+	/* make sure the "knote" gets woken up */
+	KNOTE_LOCKED(&filp->f_selinfo.si_note, 1);
+	spin_unlock(&filp->f_kqlock);
+}
+
+static void
+linux_dev_kqfilter_detach(struct knote *kn)
+{
+	struct linux_file *filp = kn->kn_hook;
+
+	spin_lock(&filp->f_kqlock);
+	knlist_remove(&filp->f_selinfo.si_note, kn, 1);
+	spin_unlock(&filp->f_kqlock);
+}
+
+static int
+linux_dev_kqfilter_read_event(struct knote *kn, long hint)
+{
+	struct linux_file *filp = kn->kn_hook;
+
+	mtx_assert(&filp->f_kqlock.m, MA_OWNED);
+
+	return ((filp->f_kqflags & LINUX_KQ_FLAG_NEED_READ) ? 1 : 0);
+}
+
+static int
+linux_dev_kqfilter_write_event(struct knote *kn, long hint)
+{
+	struct linux_file *filp = kn->kn_hook;
+
+	mtx_assert(&filp->f_kqlock.m, MA_OWNED);
+
+	return ((filp->f_kqflags & LINUX_KQ_FLAG_NEED_WRITE) ? 1 : 0);
+}
+
+static struct filterops linux_dev_kqfiltops_read = {
+	.f_isfd = 1,
+	.f_detach = linux_dev_kqfilter_detach,
+	.f_event = linux_dev_kqfilter_read_event,
+};
+
+static struct filterops linux_dev_kqfiltops_write = {
+	.f_isfd = 1,
+	.f_detach = linux_dev_kqfilter_detach,
+	.f_event = linux_dev_kqfilter_write_event,
+};
+
+static void
+linux_dev_kqfilter_poll(struct linux_file *filp, int kqflags)
+{
+	int temp;
+
+	if (filp->f_kqflags & kqflags) {
+		/* get the latest polling state */
+		temp = filp->f_op->poll(filp, NULL);
+
+		spin_lock(&filp->f_kqlock);
+		/* clear kqflags */
+		filp->f_kqflags &= ~(LINUX_KQ_FLAG_NEED_READ |
+		    LINUX_KQ_FLAG_NEED_WRITE);
+		/* update kqflags */
+		if (temp & (POLLIN | POLLOUT)) {
+			if (temp & POLLIN)
+				filp->f_kqflags |= LINUX_KQ_FLAG_NEED_READ;
+			if (temp & POLLOUT)
+				filp->f_kqflags |= LINUX_KQ_FLAG_NEED_WRITE;
+
+			/* make sure the "knote" gets woken up */
+			KNOTE_LOCKED(&filp->f_selinfo.si_note, 0);
+		}
+		spin_unlock(&filp->f_kqlock);
+	}
+}
+
+static int
+linux_dev_kqfilter(struct cdev *dev, struct knote *kn)
+{
+	struct linux_file *filp;
+	struct file *file;
+	struct thread *td;
+	int error;
+
+	td = curthread;
+	file = td->td_fpop;
+	if ((error = devfs_get_cdevpriv((void **)&filp)) != 0)
+		return (error);
+	filp->f_flags = file->f_flag;
+	if (filp->f_op->poll == NULL)
+		return (EINVAL);
+
+	spin_lock(&filp->f_kqlock);
+	switch (kn->kn_filter) {
+	case EVFILT_READ:
+		filp->f_kqflags |= LINUX_KQ_FLAG_HAS_READ;
+		kn->kn_fop = &linux_dev_kqfiltops_read;
+		kn->kn_hook = filp;
+		knlist_add(&filp->f_selinfo.si_note, kn, 1);
+		break;
+	case EVFILT_WRITE:
+		filp->f_kqflags |= LINUX_KQ_FLAG_HAS_WRITE;
+		kn->kn_fop = &linux_dev_kqfiltops_write;
+		kn->kn_hook = filp;
+		knlist_add(&filp->f_selinfo.si_note, kn, 1);
+		break;
+	default:
+		error = EINVAL;
+		break;
+	}
+	spin_unlock(&filp->f_kqlock);
+
+	if (error == 0) {
+		linux_set_current(td);
+
+		/* update kqfilter status, if any */
+		linux_dev_kqfilter_poll(filp,
+		    LINUX_KQ_FLAG_HAS_READ | LINUX_KQ_FLAG_HAS_WRITE);
+	}
+	return (error);
 }
 
 static int
 linux_dev_mmap_single(struct cdev *dev, vm_ooffset_t *offset,
     vm_size_t size, struct vm_object **object, int nprot)
 {
-	struct linux_cdev *ldev;
+	struct vm_area_struct *vmap;
+	struct mm_struct *mm;
 	struct linux_file *filp;
 	struct thread *td;
-	struct task_struct t;
 	struct file *file;
-	struct vm_area_struct vma;
+	vm_memattr_t attr;
 	int error;
 
 	td = curthread;
 	file = td->td_fpop;
-	ldev = dev->si_drv1;
-	if (ldev == NULL)
-		return (ENODEV);
 	if ((error = devfs_get_cdevpriv((void **)&filp)) != 0)
 		return (error);
 	filp->f_flags = file->f_flag;
-	linux_set_current(td, &t);
-	vma.vm_start = 0;
-	vma.vm_end = size;
-	vma.vm_pgoff = *offset / PAGE_SIZE;
-	vma.vm_pfn = 0;
-	vma.vm_page_prot = VM_MEMATTR_DEFAULT;
-	if (filp->f_op->mmap) {
-		error = -filp->f_op->mmap(filp, &vma);
-		if (error == 0) {
-			struct sglist *sg;
-
-			sg = sglist_alloc(1, M_WAITOK);
-			sglist_append_phys(sg,
-			    (vm_paddr_t)vma.vm_pfn << PAGE_SHIFT, vma.vm_len);
-			*object = vm_pager_allocate(OBJT_SG, sg, vma.vm_len,
-			    nprot, 0, td->td_ucred);
-		        if (*object == NULL) {
-				sglist_free(sg);
-				error = EINVAL;
-				goto done;
+
+	if (filp->f_op->mmap == NULL)
+		return (ENODEV);
+
+	linux_set_current(td);
+
+	/*
+	 * The same VM object might be shared by multiple processes
+	 * and the mm_struct is usually freed when a process exits.
+	 *
+	 * The atomic reference below makes sure the mm_struct is
+	 * available as long as the vmap is in the linux_vma_head.
+	 */
+	mm = current->mm;
+	if (atomic_inc_not_zero(&mm->mm_users) == 0)
+		return (EINVAL);
+
+	vmap = kzalloc(sizeof(*vmap), GFP_KERNEL);
+	vmap->vm_start = 0;
+	vmap->vm_end = size;
+	vmap->vm_pgoff = *offset / PAGE_SIZE;
+	vmap->vm_pfn = 0;
+	vmap->vm_flags = vmap->vm_page_prot = (nprot & VM_PROT_ALL);
+	vmap->vm_ops = NULL;
+	vmap->vm_file = get_file(filp);
+	vmap->vm_mm = mm;
+
+	if (unlikely(down_write_killable(&vmap->vm_mm->mmap_sem))) {
+		error = EINTR;
+	} else {
+		error = -filp->f_op->mmap(filp, vmap);
+		up_write(&vmap->vm_mm->mmap_sem);
+	}
+
+	if (error != 0) {
+		linux_cdev_handle_free(vmap);
+		return (error);
+	}
+
+	attr = pgprot2cachemode(vmap->vm_page_prot);
+
+	if (vmap->vm_ops != NULL) {
+		struct vm_area_struct *ptr;
+		void *vm_private_data;
+		bool vm_no_fault;
+
+		if (vmap->vm_ops->open == NULL ||
+		    vmap->vm_ops->close == NULL ||
+		    vmap->vm_private_data == NULL) {
+			/* free allocated VM area struct */
+			linux_cdev_handle_free(vmap);
+			return (EINVAL);
+		}
+
+		vm_private_data = vmap->vm_private_data;
+
+		rw_wlock(&linux_vma_lock);
+		TAILQ_FOREACH(ptr, &linux_vma_head, vm_entry) {
+			if (ptr->vm_private_data == vm_private_data)
+				break;
+		}
+		/* check if there is an existing VM area struct */
+		if (ptr != NULL) {
+			/* check if the VM area structure is invalid */
+			if (ptr->vm_ops == NULL ||
+			    ptr->vm_ops->open == NULL ||
+			    ptr->vm_ops->close == NULL) {
+				error = ESTALE;
+				vm_no_fault = 1;
+			} else {
+				error = EEXIST;
+				vm_no_fault = (ptr->vm_ops->fault == NULL);
 			}
-			*offset = 0;
-			if (vma.vm_page_prot != VM_MEMATTR_DEFAULT) {
-				VM_OBJECT_WLOCK(*object);
-				vm_object_set_memattr(*object,
-				    vma.vm_page_prot);
-				VM_OBJECT_WUNLOCK(*object);
+		} else {
+			/* insert VM area structure into list */
+			TAILQ_INSERT_TAIL(&linux_vma_head, vmap, vm_entry);
+			error = 0;
+			vm_no_fault = (vmap->vm_ops->fault == NULL);
+		}
+		rw_wunlock(&linux_vma_lock);
+
+		if (error != 0) {
+			/* free allocated VM area struct */
+			linux_cdev_handle_free(vmap);
+			/* check for stale VM area struct */
+			if (error != EEXIST)
+				return (error);
+		}
+
+		/* check if there is no fault handler */
+		if (vm_no_fault) {
+			*object = cdev_pager_allocate(vm_private_data, OBJT_DEVICE,
+			    &linux_cdev_pager_ops[1], size, nprot, *offset,
+			    curthread->td_ucred);
+		} else {
+			*object = cdev_pager_allocate(vm_private_data, OBJT_MGTDEVICE,
+			    &linux_cdev_pager_ops[0], size, nprot, *offset,
+			    curthread->td_ucred);
+		}
+
+		/* check if allocating the VM object failed */
+		if (*object == NULL) {
+			if (error == 0) {
+				/* remove VM area struct from list */
+				linux_cdev_handle_remove(vmap);
+				/* free allocated VM area struct */
+				linux_cdev_handle_free(vmap);
 			}
+			return (EINVAL);
 		}
-	} else
-		error = ENODEV;
-done:
-	linux_clear_current(td);
-	return (error);
+	} else {
+		struct sglist *sg;
+
+		sg = sglist_alloc(1, M_WAITOK);
+		sglist_append_phys(sg,
+		    (vm_paddr_t)vmap->vm_pfn << PAGE_SHIFT, vmap->vm_len);
+
+		*object = vm_pager_allocate(OBJT_SG, sg, vmap->vm_len,
+		    nprot, 0, curthread->td_ucred);
+
+		linux_cdev_handle_free(vmap);
+
+		if (*object == NULL) {
+			sglist_free(sg);
+			return (EINVAL);
+		}
+	}
+
+	if (attr != VM_MEMATTR_DEFAULT) {
+		VM_OBJECT_WLOCK(*object);
+		vm_object_set_memattr(*object, attr);
+		VM_OBJECT_WUNLOCK(*object);
+	}
+	*offset = 0;
+	return (0);
 }
 
 struct cdevsw linuxcdevsw = {
@@ -748,6 +1405,8 @@ struct cdevsw linuxcdevsw = {
 	.d_ioctl = linux_dev_ioctl,
 	.d_mmap_single = linux_dev_mmap_single,
 	.d_poll = linux_dev_poll,
+	.d_kqfilter = linux_dev_kqfilter,
+	.d_name = "lkpidev",
 };
 
 static int
@@ -755,7 +1414,6 @@ linux_file_read(struct file *file, struct uio *uio, struct ucred *active_cred,
     int flags, struct thread *td)
 {
 	struct linux_file *filp;
-	struct task_struct t;
 	ssize_t bytes;
 	int error;
 
@@ -765,7 +1423,7 @@ linux_file_read(struct file *file, struct uio *uio, struct ucred *active_cred,
 	/* XXX no support for I/O vectors currently */
 	if (uio->uio_iovcnt != 1)
 		return (EOPNOTSUPP);
-	linux_set_current(td, &t);
+	linux_set_current(td);
 	if (filp->f_op->read) {
 		bytes = filp->f_op->read(filp, uio->uio_iov->iov_base,
 		    uio->uio_iov->iov_len, &uio->uio_offset);
@@ -778,7 +1436,6 @@ linux_file_read(struct file *file, struct uio *uio, struct ucred *active_cred,
 			error = -bytes;
 	} else
 		error = ENXIO;
-	linux_clear_current(td);
 
 	return (error);
 }
@@ -788,17 +1445,15 @@ linux_file_poll(struct file *file, int events, struct ucred *active_cred,
     struct thread *td)
 {
 	struct linux_file *filp;
-	struct task_struct t;
 	int revents;
 
 	filp = (struct linux_file *)file->f_data;
 	filp->f_flags = file->f_flag;
-	linux_set_current(td, &t);
-	if (filp->f_op->poll)
-		revents = filp->f_op->poll(filp, NULL) & events;
+	linux_set_current(td);
+	if (filp->f_op->poll != NULL)
+		revents = filp->f_op->poll(filp, LINUX_POLL_TABLE_NORMAL) & events;
 	else
 		revents = 0;
-	linux_clear_current(td);
 
 	return (revents);
 }
@@ -807,14 +1462,13 @@ static int
 linux_file_close(struct file *file, struct thread *td)
 {
 	struct linux_file *filp;
-	struct task_struct t;
 	int error;
 
 	filp = (struct linux_file *)file->f_data;
 	filp->f_flags = file->f_flag;
-	linux_set_current(td, &t);
+	linux_set_current(td);
+	linux_poll_wait_dequeue(filp);
 	error = -filp->f_op->release(NULL, filp);
-	linux_clear_current(td);
 	funsetown(&filp->f_sigio);
 	kfree(filp);
 
@@ -826,14 +1480,13 @@ linux_file_ioctl(struct file *fp, u_long cmd, void *data, struct ucred *cred,
     struct thread *td)
 {
 	struct linux_file *filp;
-	struct task_struct t;
 	int error;
 
 	filp = (struct linux_file *)fp->f_data;
 	filp->f_flags = fp->f_flag;
 	error = 0;
 
-	linux_set_current(td, &t);
+	linux_set_current(td);
 	switch (cmd) {
 	case FIONBIO:
 		break;
@@ -855,7 +1508,6 @@ linux_file_ioctl(struct file *fp, u_long cmd, void *data, struct ucred *cred,
 		error = ENOTTY;
 		break;
 	}
-	linux_clear_current(td);
 	return (error);
 }
 
@@ -875,6 +1527,21 @@ linux_file_fill_kinfo(struct file *fp, struct kinfo_file *kif,
 	return (0);
 }
 
+unsigned int
+linux_iminor(struct inode *inode)
+{
+	struct linux_cdev *ldev;
+
+	if (inode == NULL || inode->v_rdev == NULL ||
+	    inode->v_rdev->si_devsw != &linuxcdevsw)
+		return (-1U);
+	ldev = inode->v_rdev->si_drv1;
+	if (ldev == NULL)
+		return (-1U);
+
+	return (minor(ldev->dev));
+}
+
 struct fileops linuxfileops = {
 	.fo_read = linux_file_read,
 	.fo_write = invfo_rdwr,
@@ -939,7 +1606,7 @@ vmmap_remove(void *addr)
 	return (vmmap);
 }
 
-#if defined(__i386__) || defined(__amd64__)
+#if defined(__i386__) || defined(__amd64__) || defined(__powerpc__)
 void *
 _ioremap_attr(vm_paddr_t phys_addr, unsigned long size, int attr)
 {
@@ -962,7 +1629,7 @@ iounmap(void *addr)
 	vmmap = vmmap_remove(addr);
 	if (vmmap == NULL)
 		return;
-#if defined(__i386__) || defined(__amd64__)
+#if defined(__i386__) || defined(__amd64__) || defined(__powerpc__)
 	pmap_unmapdev((vm_offset_t)addr, vmmap->vm_size);
 #endif
 	kfree(vmmap);
@@ -1034,12 +1701,14 @@ linux_timer_callback_wrapper(void *context)
 {
 	struct timer_list *timer;
 
+	linux_set_current(curthread);
+
 	timer = context;
 	timer->function(timer->data);
 }
 
 void
-mod_timer(struct timer_list *timer, unsigned long expires)
+mod_timer(struct timer_list *timer, int expires)
 {
 
 	timer->expires = expires;
@@ -1057,6 +1726,15 @@ add_timer(struct timer_list *timer)
 	    &linux_timer_callback_wrapper, timer);
 }
 
+void
+add_timer_on(struct timer_list *timer, int cpu)
+{
+
+	callout_reset_on(&timer->timer_callout,
+	    linux_timer_jiffies_until(timer->expires),
+	    &linux_timer_callback_wrapper, timer, cpu);
+}
+
 static void
 linux_timer_init(void *arg)
 {
@@ -1092,10 +1770,10 @@ linux_complete_common(struct completion *c, int all)
 /*
  * Indefinite wait for done != 0 with or without signals.
  */
-long
+int
 linux_wait_for_common(struct completion *c, int flags)
 {
-	long error;
+	int error;
 
 	if (SCHEDULER_STOPPED())
 		return (0);
@@ -1132,10 +1810,11 @@ linux_wait_for_common(struct completion *c, int flags)
 /*
  * Time limited wait for done != 0 with or without signals.
  */
-long
-linux_wait_for_timeout_common(struct completion *c, long timeout, int flags)
+int
+linux_wait_for_timeout_common(struct completion *c, int timeout, int flags)
 {
-	long end = jiffies + timeout, error;
+	int end = jiffies + timeout;
+	int error;
 	int ret;
 
 	if (SCHEDULER_STOPPED())
@@ -1207,50 +1886,6 @@ linux_completion_done(struct completion *c)
 	return (isdone);
 }
 
-void
-linux_delayed_work_fn(void *arg)
-{
-	struct delayed_work *work;
-
-	work = arg;
-	taskqueue_enqueue(work->work.taskqueue, &work->work.work_task);
-}
-
-void
-linux_work_fn(void *context, int pending)
-{
-	struct work_struct *work;
-
-	work = context;
-	work->fn(work);
-}
-
-void
-linux_flush_fn(void *context, int pending)
-{
-}
-
-struct workqueue_struct *
-linux_create_workqueue_common(const char *name, int cpus)
-{
-	struct workqueue_struct *wq;
-
-	wq = kmalloc(sizeof(*wq), M_WAITOK);
-	wq->taskqueue = taskqueue_create(name, M_WAITOK,
-	    taskqueue_thread_enqueue,  &wq->taskqueue);
-	atomic_set(&wq->draining, 0);
-	taskqueue_start_threads(&wq->taskqueue, cpus, PWAIT, "%s", name);
-
-	return (wq);
-}
-
-void
-destroy_workqueue(struct workqueue_struct *wq)
-{
-	taskqueue_free(wq->taskqueue);
-	kfree(wq);
-}
-
 static void
 linux_cdev_release(struct kobject *kobj)
 {
@@ -1431,20 +2066,48 @@ linux_irq_handler(void *ent)
 {
 	struct irq_ent *irqe;
 
+	linux_set_current(curthread);
+
 	irqe = ent;
 	irqe->handler(irqe->irq, irqe->arg);
 }
 
+#if defined(__i386__) || defined(__amd64__)
+int
+linux_wbinvd_on_all_cpus(void)
+{
+
+	pmap_invalidate_cache();
+	return (0);
+}
+#endif
+
+int
+linux_on_each_cpu(void callback(void *), void *data)
+{
+
+	smp_rendezvous(smp_no_rendezvous_barrier, callback,
+	    smp_no_rendezvous_barrier, data);
+	return (0);
+}
+
+int
+linux_in_atomic(void)
+{
+
+	return ((curthread->td_pflags & TDP_NOFAULTING) != 0);
+}
+
 struct linux_cdev *
 linux_find_cdev(const char *name, unsigned major, unsigned minor)
 {
-	int unit = MKDEV(major, minor);
+	dev_t dev = MKDEV(major, minor);
 	struct cdev *cdev;
 
 	dev_lock();
 	LIST_FOREACH(cdev, &linuxcdevsw.d_devs, si_list) {
 		struct linux_cdev *ldev = cdev->si_drv1;
-		if (dev2unit(cdev) == unit &&
+		if (ldev->dev == dev &&
 		    strcmp(kobject_name(&ldev->kobj), name) == 0) {
 			break;
 		}
@@ -1524,7 +2187,7 @@ linux_compat_init(void *arg)
 #if defined(__i386__) || defined(__amd64__)
 	linux_cpu_has_clflush = (cpu_feature & CPUID_CLFSH);
 #endif
-	sx_init(&linux_global_rcu_lock, "LinuxGlobalRCU");
+	rw_init(&linux_vma_lock, "lkpi-vma-lock");
 
 	rootoid = SYSCTL_ADD_ROOT_NODE(NULL,
 	    OID_AUTO, "sys", CTLFLAG_RD|CTLFLAG_MPSAFE, NULL, "sys");
@@ -1556,8 +2219,9 @@ linux_compat_uninit(void *arg)
 	linux_kobject_kfree_name(&linux_root_device.kobj);
 	linux_kobject_kfree_name(&linux_class_misc.kobj);
 
-	synchronize_rcu();
-	sx_destroy(&linux_global_rcu_lock);
+	mtx_destroy(&vmmaplock);
+	spin_lock_destroy(&pci_lock);
+	rw_destroy(&linux_vma_lock);
 }
 SYSUNINIT(linux_compat, SI_SUB_DRIVERS, SI_ORDER_SECOND, linux_compat_uninit, NULL);
 
diff --git a/sys/compat/linuxkpi/common/src/linux_current.c b/sys/compat/linuxkpi/common/src/linux_current.c
new file mode 100644
index 00000000000..6cac640ef73
--- /dev/null
+++ b/sys/compat/linuxkpi/common/src/linux_current.c
@@ -0,0 +1,248 @@
+/*-
+ * Copyright (c) 2017 Hans Petter Selasky
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice unmodified, this list of conditions, and the following
+ *    disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS OR
+ * IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+ * OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.
+ * IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT,
+ * INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT
+ * NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+ * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+ * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF
+ * THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include <sys/cdefs.h>
+__FBSDID("$FreeBSD$");
+
+#include <linux/compat.h>
+#include <linux/completion.h>
+#include <linux/mm.h>
+#include <linux/kthread.h>
+
+#include <sys/kernel.h>
+#include <sys/eventhandler.h>
+#include <sys/malloc.h>
+
+static eventhandler_tag linuxkpi_thread_dtor_tag;
+
+static MALLOC_DEFINE(M_LINUX_CURRENT, "linuxcurrent", "LinuxKPI task structure");
+
+int
+linux_alloc_current(struct thread *td, int flags)
+{
+	struct proc *proc;
+	struct thread *td_other;
+	struct task_struct *ts;
+	struct task_struct *ts_other;
+	struct mm_struct *mm;
+	struct mm_struct *mm_other;
+
+	MPASS(td->td_lkpi_task == NULL);
+
+	ts = malloc(sizeof(*ts), M_LINUX_CURRENT, flags | M_ZERO);
+	if (ts == NULL)
+		return (ENOMEM);
+
+	mm = malloc(sizeof(*mm), M_LINUX_CURRENT, flags | M_ZERO);
+	if (mm == NULL) {
+		free(ts, M_LINUX_CURRENT);
+		return (ENOMEM);
+	}
+
+	/* setup new task structure */
+	atomic_set(&ts->kthread_flags, 0);
+	ts->task_thread = td;
+	ts->comm = td->td_name;
+	ts->pid = td->td_tid;
+	atomic_set(&ts->usage, 1);
+	atomic_set(&ts->state, TASK_RUNNING);
+	init_completion(&ts->parked);
+	init_completion(&ts->exited);
+
+	proc = td->td_proc;
+
+	/* check if another thread already has a mm_struct */
+	PROC_LOCK(proc);
+	FOREACH_THREAD_IN_PROC(proc, td_other) {
+		ts_other = td_other->td_lkpi_task;
+		if (ts_other == NULL)
+			continue;
+
+		mm_other = ts_other->mm;
+		if (mm_other == NULL)
+			continue;
+
+		/* try to share other mm_struct */
+		if (atomic_inc_not_zero(&mm_other->mm_users)) {
+			/* set mm_struct pointer */
+			ts->mm = mm_other;
+			break;
+		}
+	}
+
+	/* use allocated mm_struct as a fallback */
+	if (ts->mm == NULL) {
+		/* setup new mm_struct */
+		init_rwsem(&mm->mmap_sem);
+		atomic_set(&mm->mm_count, 1);
+		atomic_set(&mm->mm_users, 1);
+		/* set mm_struct pointer */
+		ts->mm = mm;
+		/* clear pointer to not free memory */
+		mm = NULL;
+	}
+
+	/* store pointer to task struct */
+	td->td_lkpi_task = ts;
+	PROC_UNLOCK(proc);
+
+	/* free mm_struct pointer, if any */
+	free(mm, M_LINUX_CURRENT);
+
+	return (0);
+}
+
+struct mm_struct *
+linux_get_task_mm(struct task_struct *task)
+{
+	struct mm_struct *mm;
+
+	mm = task->mm;
+	if (mm != NULL) {
+		atomic_inc(&mm->mm_users);
+		return (mm);
+	}
+	return (NULL);
+}
+
+void
+linux_mm_dtor(struct mm_struct *mm)
+{
+	free(mm, M_LINUX_CURRENT);
+}
+
+void
+linux_free_current(struct task_struct *ts)
+{
+	mmput(ts->mm);
+	free(ts, M_LINUX_CURRENT);
+}
+
+static void
+linuxkpi_thread_dtor(void *arg __unused, struct thread *td)
+{
+	struct task_struct *ts;
+
+	ts = td->td_lkpi_task;
+	if (ts == NULL)
+		return;
+
+	td->td_lkpi_task = NULL;
+	put_task_struct(ts);
+}
+
+struct task_struct *
+linux_pid_task(pid_t pid)
+{
+	struct thread *td;
+	struct proc *p;
+
+	/* try to find corresponding thread */
+	td = tdfind(pid, -1);
+	if (td != NULL) {
+		struct task_struct *ts = td->td_lkpi_task;
+		PROC_UNLOCK(td->td_proc);
+		return (ts);
+	}
+
+	/* try to find corresponding procedure */
+	p = pfind(pid);
+	if (p != NULL) {
+		FOREACH_THREAD_IN_PROC(p, td) {
+			struct task_struct *ts = td->td_lkpi_task;
+			if (ts != NULL) {
+				PROC_UNLOCK(p);
+				return (ts);
+			}
+		}
+		PROC_UNLOCK(p);
+	}
+	return (NULL);
+}
+
+struct task_struct *
+linux_get_pid_task(pid_t pid)
+{
+	struct thread *td;
+	struct proc *p;
+
+	/* try to find corresponding thread */
+	td = tdfind(pid, -1);
+	if (td != NULL) {
+		struct task_struct *ts = td->td_lkpi_task;
+		if (ts != NULL)
+			get_task_struct(ts);
+		PROC_UNLOCK(td->td_proc);
+		return (ts);
+	}
+
+	/* try to find corresponding procedure */
+	p = pfind(pid);
+	if (p != NULL) {
+		FOREACH_THREAD_IN_PROC(p, td) {
+			struct task_struct *ts = td->td_lkpi_task;
+			if (ts != NULL) {
+				get_task_struct(ts);
+				PROC_UNLOCK(p);
+				return (ts);
+			}
+		}
+		PROC_UNLOCK(p);
+	}
+	return (NULL);
+}
+
+static void
+linux_current_init(void *arg __unused)
+{
+	linuxkpi_thread_dtor_tag = EVENTHANDLER_REGISTER(thread_dtor,
+	    linuxkpi_thread_dtor, NULL, EVENTHANDLER_PRI_ANY);
+}
+SYSINIT(linux_current, SI_SUB_EVENTHANDLER, SI_ORDER_SECOND, linux_current_init, NULL);
+
+static void
+linux_current_uninit(void *arg __unused)
+{
+	struct proc *p;
+	struct task_struct *ts;
+	struct thread *td;
+
+	sx_slock(&allproc_lock);
+	FOREACH_PROC_IN_SYSTEM(p) {
+		PROC_LOCK(p);
+		FOREACH_THREAD_IN_PROC(p, td) {
+			if ((ts = td->td_lkpi_task) != NULL) {
+				td->td_lkpi_task = NULL;
+				put_task_struct(ts);
+			}
+		}
+		PROC_UNLOCK(p);
+	}
+	sx_sunlock(&allproc_lock);
+
+	EVENTHANDLER_DEREGISTER(thread_dtor, linuxkpi_thread_dtor_tag);
+}
+SYSUNINIT(linux_current, SI_SUB_EVENTHANDLER, SI_ORDER_SECOND, linux_current_uninit, NULL);
diff --git a/sys/compat/linuxkpi/common/src/linux_hrtimer.c b/sys/compat/linuxkpi/common/src/linux_hrtimer.c
new file mode 100644
index 00000000000..c6502569c8a
--- /dev/null
+++ b/sys/compat/linuxkpi/common/src/linux_hrtimer.c
@@ -0,0 +1,104 @@
+/*-
+ * Copyright (c) 2017 Mark Johnston <markj@FreeBSD.org>
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice unmodified, this list of conditions, and the following
+ *    disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS OR
+ * IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+ * OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.
+ * IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT,
+ * INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT
+ * NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+ * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+ * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF
+ * THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include <sys/cdefs.h>
+__FBSDID("$FreeBSD$");
+
+#include <sys/param.h>
+#include <sys/systm.h>
+#include <sys/lock.h>
+#include <sys/mutex.h>
+#include <sys/time.h>
+
+#include <machine/cpu.h>
+
+#include <linux/hrtimer.h>
+
+static void
+hrtimer_call_handler(void *arg)
+{
+	struct hrtimer *hrtimer;
+	enum hrtimer_restart ret;
+
+	hrtimer = arg;
+	ret = hrtimer->function(hrtimer);
+	MPASS(ret == HRTIMER_NORESTART);
+	callout_deactivate(&hrtimer->callout);
+}
+
+bool
+linux_hrtimer_active(struct hrtimer *hrtimer)
+{
+	bool ret;
+
+	mtx_lock(&hrtimer->mtx);
+	ret = callout_active(&hrtimer->callout);
+	mtx_unlock(&hrtimer->mtx);
+	return (ret);
+}
+
+/*
+ * Cancel active hrtimer.
+ * Return 1 if timer was active and cancellation succeeded, or 0 otherwise.
+ */
+int
+linux_hrtimer_cancel(struct hrtimer *hrtimer)
+{
+
+	return (callout_drain(&hrtimer->callout) > 0);
+}
+
+void
+linux_hrtimer_init(struct hrtimer *hrtimer)
+{
+
+	hrtimer->function = NULL;
+	mtx_init(&hrtimer->mtx, "hrtimer", NULL, MTX_DEF | MTX_RECURSE);
+	callout_init_mtx(&hrtimer->callout, &hrtimer->mtx, 0);
+}
+
+void
+linux_hrtimer_set_expires(struct hrtimer *hrtimer __unused,
+    ktime_t time __unused)
+{
+}
+
+void
+linux_hrtimer_start(struct hrtimer *hrtimer, ktime_t time)
+{
+
+	linux_hrtimer_start_range_ns(hrtimer, time, 0);
+}
+
+void
+linux_hrtimer_start_range_ns(struct hrtimer *hrtimer, ktime_t time, int64_t nsec)
+{
+
+	mtx_lock(&hrtimer->mtx);
+	callout_reset_sbt(&hrtimer->callout, nstosbt(time.tv64), nstosbt(nsec),
+	    hrtimer_call_handler, hrtimer, 0);
+	mtx_unlock(&hrtimer->mtx);
+}
diff --git a/sys/compat/linuxkpi/common/src/linux_idr.c b/sys/compat/linuxkpi/common/src/linux_idr.c
index 99e45e3dcd3..5bc26ed4f9e 100644
--- a/sys/compat/linuxkpi/common/src/linux_idr.c
+++ b/sys/compat/linuxkpi/common/src/linux_idr.c
@@ -2,7 +2,7 @@
  * Copyright (c) 2010 Isilon Systems, Inc.
  * Copyright (c) 2010 iX Systems, Inc.
  * Copyright (c) 2010 Panasas, Inc.
- * Copyright (c) 2013-2016 Mellanox Technologies, Ltd.
+ * Copyright (c) 2013-2017 Mellanox Technologies, Ltd.
  * All rights reserved.
  *
  * Redistribution and use in source and binary forms, with or without
@@ -40,12 +40,23 @@ __FBSDID("$FreeBSD$");
 
 #include <machine/stdarg.h>
 
-#include <linux/bitops.h>
+#include <linux/bitmap.h>
 #include <linux/kobject.h>
 #include <linux/slab.h>
 #include <linux/idr.h>
 #include <linux/err.h>
 
+#define	MAX_IDR_LEVEL	((MAX_IDR_SHIFT + IDR_BITS - 1) / IDR_BITS)
+#define	MAX_IDR_FREE	(MAX_IDR_LEVEL * 2)
+
+struct linux_idr_cache {
+	spinlock_t lock;
+	struct idr_layer *head;
+	unsigned count;
+};
+
+static DPCPU_DEFINE(struct linux_idr_cache, linux_idr_cache);
+
 /*
  * IDR Implementation.
  *
@@ -55,6 +66,96 @@ __FBSDID("$FreeBSD$");
  */
 static MALLOC_DEFINE(M_IDR, "idr", "Linux IDR compat");
 
+static struct idr_layer *
+idr_preload_dequeue_locked(struct linux_idr_cache *lic)
+{
+	struct idr_layer *retval;
+
+	/* check if wrong thread is trying to dequeue */
+	if (mtx_owned(&lic->lock.m) == 0)
+		return (NULL);
+
+	retval = lic->head;
+	if (likely(retval != NULL)) {
+		lic->head = retval->ary[0];
+		lic->count--;
+		retval->ary[0] = NULL;
+	}
+	return (retval);
+}
+
+static void
+idr_preload_init(void *arg)
+{
+	int cpu;
+
+	CPU_FOREACH(cpu) {
+		struct linux_idr_cache *lic =
+		    DPCPU_ID_PTR(cpu, linux_idr_cache);
+
+		spin_lock_init(&lic->lock);
+	}
+}
+SYSINIT(idr_preload_init, SI_SUB_CPU, SI_ORDER_ANY, idr_preload_init, NULL);
+
+static void
+idr_preload_uninit(void *arg)
+{
+	int cpu;
+
+	CPU_FOREACH(cpu) {
+		struct idr_layer *cacheval;
+		struct linux_idr_cache *lic =
+		    DPCPU_ID_PTR(cpu, linux_idr_cache);
+
+		while (1) {
+			spin_lock(&lic->lock);
+			cacheval = idr_preload_dequeue_locked(lic);
+			spin_unlock(&lic->lock);
+
+			if (cacheval == NULL)
+				break;
+			free(cacheval, M_IDR);
+		}
+		spin_lock_destroy(&lic->lock);
+	}
+}
+SYSUNINIT(idr_preload_uninit, SI_SUB_LOCK, SI_ORDER_FIRST, idr_preload_uninit, NULL);
+
+void
+idr_preload(gfp_t gfp_mask)
+{
+	struct linux_idr_cache *lic;
+	struct idr_layer *cacheval;
+
+	sched_pin();
+
+	lic = &DPCPU_GET(linux_idr_cache);
+
+	/* fill up cache */
+	spin_lock(&lic->lock);
+	while (lic->count < MAX_IDR_FREE) {
+		spin_unlock(&lic->lock);
+		cacheval = malloc(sizeof(*cacheval), M_IDR, M_ZERO | gfp_mask);
+		spin_lock(&lic->lock);
+		if (cacheval == NULL)
+			break;
+		cacheval->ary[0] = lic->head;
+		lic->head = cacheval;
+		lic->count++;
+	}
+}
+
+void
+idr_preload_end(void)
+{
+	struct linux_idr_cache *lic;
+
+	lic = &DPCPU_GET(linux_idr_cache);
+	spin_unlock(&lic->lock);
+	sched_unpin();
+}
+
 static inline int
 idr_max(struct idr *idr)
 {
@@ -280,20 +381,32 @@ idr_pre_get(struct idr *idr, gfp_t gfp_mask)
 	return (1);
 }
 
-static inline struct idr_layer *
-idr_get(struct idr *idr)
+static struct idr_layer *
+idr_free_list_get(struct idr *idp)
 {
 	struct idr_layer *il;
 
-	il = idr->free;
-	if (il) {
-		idr->free = il->ary[0];
+	if ((il = idp->free) != NULL) {
+		idp->free = il->ary[0];
 		il->ary[0] = NULL;
-		return (il);
 	}
-	il = malloc(sizeof(*il), M_IDR, M_ZERO | M_NOWAIT);
-	if (il != NULL)
+	return (il);
+}
+
+static inline struct idr_layer *
+idr_get(struct idr *idp)
+{
+	struct idr_layer *il;
+
+	if ((il = idr_free_list_get(idp)) != NULL) {
+		MPASS(il->bitmap != 0);
+	} else if ((il = malloc(sizeof(*il), M_IDR, M_ZERO | M_NOWAIT)) != NULL) {
+		bitmap_fill(&il->bitmap, IDR_SIZE);
+	} else if ((il = idr_preload_dequeue_locked(&DPCPU_GET(linux_idr_cache))) != NULL) {
 		bitmap_fill(&il->bitmap, IDR_SIZE);
+	} else {
+		return (NULL);
+	}
 	return (il);
 }
 
diff --git a/sys/compat/linuxkpi/common/src/linux_kthread.c b/sys/compat/linuxkpi/common/src/linux_kthread.c
new file mode 100644
index 00000000000..19808261507
--- /dev/null
+++ b/sys/compat/linuxkpi/common/src/linux_kthread.c
@@ -0,0 +1,168 @@
+/*-
+ * Copyright (c) 2017 Hans Petter Selasky
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice unmodified, this list of conditions, and the following
+ *    disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS OR
+ * IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+ * OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.
+ * IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT,
+ * INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT
+ * NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+ * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+ * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF
+ * THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include <sys/cdefs.h>
+__FBSDID("$FreeBSD$");
+
+#include <linux/compat.h>
+#include <linux/kthread.h>
+#include <linux/sched.h>
+#include <linux/wait.h>
+
+#include <sys/bus.h>
+#include <sys/interrupt.h>
+#include <sys/priority.h>
+
+enum {
+	KTHREAD_SHOULD_STOP_MASK = (1 << 0),
+	KTHREAD_SHOULD_PARK_MASK = (1 << 1),
+	KTHREAD_IS_PARKED_MASK = (1 << 2),
+};
+
+bool
+linux_kthread_should_stop_task(struct task_struct *task)
+{
+
+	return (atomic_read(&task->kthread_flags) & KTHREAD_SHOULD_STOP_MASK);
+}
+
+bool
+linux_kthread_should_stop(void)
+{
+
+	return (atomic_read(&current->kthread_flags) & KTHREAD_SHOULD_STOP_MASK);
+}
+
+int
+linux_kthread_stop(struct task_struct *task)
+{
+	int retval;
+
+	/*
+	 * Assume task is still alive else caller should not call
+	 * kthread_stop():
+	 */
+	atomic_or(KTHREAD_SHOULD_STOP_MASK, &task->kthread_flags);
+	kthread_unpark(task);
+	wake_up_process(task);
+	wait_for_completion(&task->exited);
+
+	/*
+	 * Get return code and free task structure:
+	 */
+	retval = task->task_ret;
+	put_task_struct(task);
+
+	return (retval);
+}
+
+int
+linux_kthread_park(struct task_struct *task)
+{
+
+	atomic_or(KTHREAD_SHOULD_PARK_MASK, &task->kthread_flags);
+	wake_up_process(task);
+	wait_for_completion(&task->parked);
+	return (0);
+}
+
+void
+linux_kthread_parkme(void)
+{
+	struct task_struct *task;
+
+	task = current;
+	set_task_state(task, TASK_PARKED | TASK_UNINTERRUPTIBLE);
+	while (linux_kthread_should_park()) {
+		while ((atomic_fetch_or(KTHREAD_IS_PARKED_MASK,
+		    &task->kthread_flags) & KTHREAD_IS_PARKED_MASK) == 0)
+			complete(&task->parked);
+		schedule();
+		set_task_state(task, TASK_PARKED | TASK_UNINTERRUPTIBLE);
+	}
+	atomic_andnot(KTHREAD_IS_PARKED_MASK, &task->kthread_flags);
+	set_task_state(task, TASK_RUNNING);
+}
+
+bool
+linux_kthread_should_park(void)
+{
+	struct task_struct *task;
+
+	task = current;
+	return (atomic_read(&task->kthread_flags) & KTHREAD_SHOULD_PARK_MASK);
+}
+
+void
+linux_kthread_unpark(struct task_struct *task)
+{
+
+	atomic_andnot(KTHREAD_SHOULD_PARK_MASK, &task->kthread_flags);
+	if ((atomic_fetch_andnot(KTHREAD_IS_PARKED_MASK, &task->kthread_flags) &
+	    KTHREAD_IS_PARKED_MASK) != 0)
+		wake_up_state(task, TASK_PARKED);
+}
+
+struct task_struct *
+linux_kthread_setup_and_run(struct thread *td, linux_task_fn_t *task_fn, void *arg)
+{
+	struct task_struct *task;
+
+	linux_set_current(td);
+
+	task = td->td_lkpi_task;
+	task->task_fn = task_fn;
+	task->task_data = arg;
+
+	thread_lock(td);
+	/* make sure the scheduler priority is raised */
+	sched_prio(td, PI_SWI(SWI_NET));
+	/* put thread into run-queue */
+	sched_add(td, SRQ_BORING);
+	thread_unlock(td);
+
+	return (task);
+}
+
+void
+linux_kthread_fn(void *arg __unused)
+{
+	struct task_struct *task = current;
+
+	if (linux_kthread_should_stop_task(task) == 0)
+		task->task_ret = task->task_fn(task->task_data);
+
+	if (linux_kthread_should_stop_task(task) != 0) {
+		struct thread *td = curthread;
+
+		/* let kthread_stop() free data */
+		td->td_lkpi_task = NULL;
+
+		/* wakeup kthread_stop() */
+		complete(&task->exited);
+	}
+	kthread_exit();
+}
diff --git a/sys/compat/linuxkpi/common/src/linux_lock.c b/sys/compat/linuxkpi/common/src/linux_lock.c
new file mode 100644
index 00000000000..ff91514274b
--- /dev/null
+++ b/sys/compat/linuxkpi/common/src/linux_lock.c
@@ -0,0 +1,136 @@
+/*-
+ * Copyright (c) 2017 Mellanox Technologies, Ltd.
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice unmodified, this list of conditions, and the following
+ *    disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS OR
+ * IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+ * OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.
+ * IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT,
+ * INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT
+ * NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+ * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+ * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF
+ * THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ *
+ * $FreeBSD$
+ */
+
+#include <sys/queue.h>
+
+#include <linux/ww_mutex.h>
+
+struct ww_mutex_thread {
+	TAILQ_ENTRY(ww_mutex_thread) entry;
+	struct thread *thread;
+	struct ww_mutex *lock;
+};
+
+static TAILQ_HEAD(, ww_mutex_thread) ww_mutex_head;
+static struct mtx ww_mutex_global;
+
+static void
+linux_ww_init(void *arg)
+{
+	TAILQ_INIT(&ww_mutex_head);
+	mtx_init(&ww_mutex_global, "lkpi-ww-mtx", NULL, MTX_DEF);
+}
+
+SYSINIT(ww_init, SI_SUB_LOCK, SI_ORDER_SECOND, linux_ww_init, NULL);
+
+static void
+linux_ww_uninit(void *arg)
+{
+	mtx_destroy(&ww_mutex_global);
+}
+
+SYSUNINIT(ww_uninit, SI_SUB_LOCK, SI_ORDER_SECOND, linux_ww_uninit, NULL);
+
+static inline void
+linux_ww_lock(void)
+{
+	mtx_lock(&ww_mutex_global);
+}
+
+static inline void
+linux_ww_unlock(void)
+{
+	mtx_unlock(&ww_mutex_global);
+}
+
+/* lock a mutex with deadlock avoidance */
+int
+linux_ww_mutex_lock_sub(struct ww_mutex *lock, int catch_signal)
+{
+	struct ww_mutex_thread entry;
+	struct ww_mutex_thread *other;
+	int retval = 0;
+
+	linux_ww_lock();
+	if (unlikely(sx_try_xlock(&lock->base.sx) == 0)) {
+		entry.thread = curthread;
+		entry.lock = lock;
+		TAILQ_INSERT_TAIL(&ww_mutex_head, &entry, entry);
+
+		do {
+			struct thread *owner = (struct thread *)
+			    SX_OWNER(lock->base.sx.sx_lock);
+
+			/* scan for deadlock */
+			TAILQ_FOREACH(other, &ww_mutex_head, entry) {
+				/* skip own thread */
+				if (other == &entry)
+					continue;
+				/*
+				 * If another thread is owning our
+				 * lock and is at the same time trying
+				 * to acquire a lock this thread owns,
+				 * that means deadlock.
+				 */
+				if (other->thread == owner &&
+				    (struct thread *)SX_OWNER(
+				    other->lock->base.sx.sx_lock) == curthread) {
+					retval = -EDEADLK;
+					goto done;
+				}
+			}
+			if (catch_signal) {
+				if (cv_wait_sig(&lock->condvar, &ww_mutex_global) != 0) {
+					retval = -EINTR;
+					goto done;
+				}
+			} else {
+				cv_wait(&lock->condvar, &ww_mutex_global);
+			}
+		} while (sx_try_xlock(&lock->base.sx) == 0);
+done:
+		TAILQ_REMOVE(&ww_mutex_head, &entry, entry);
+
+		/* if the lock is free, wakeup next lock waiter, if any */
+		if ((struct thread *)SX_OWNER(lock->base.sx.sx_lock) == NULL)
+			cv_signal(&lock->condvar);
+	}
+	linux_ww_unlock();
+	return (retval);
+}
+
+void
+linux_ww_mutex_unlock_sub(struct ww_mutex *lock)
+{
+	/* protect ww_mutex ownership change */
+	linux_ww_lock();
+	sx_xunlock(&lock->base.sx);
+	/* wakeup a lock waiter, if any */
+	cv_signal(&lock->condvar);
+	linux_ww_unlock();
+}
diff --git a/sys/compat/linuxkpi/common/src/linux_page.c b/sys/compat/linuxkpi/common/src/linux_page.c
new file mode 100644
index 00000000000..b7d79da46ce
--- /dev/null
+++ b/sys/compat/linuxkpi/common/src/linux_page.c
@@ -0,0 +1,395 @@
+/*-
+ * Copyright (c) 2010 Isilon Systems, Inc.
+ * Copyright (c) 2016 Matthew Macy (mmacy@mattmacy.io)
+ * Copyright (c) 2017 Mellanox Technologies, Ltd.
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice unmodified, this list of conditions, and the following
+ *    disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS OR
+ * IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+ * OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.
+ * IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT,
+ * INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT
+ * NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+ * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+ * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF
+ * THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include <sys/cdefs.h>
+__FBSDID("$FreeBSD$");
+
+#include <sys/param.h>
+#include <sys/systm.h>
+#include <sys/malloc.h>
+#include <sys/kernel.h>
+#include <sys/sysctl.h>
+#include <sys/lock.h>
+#include <sys/mutex.h>
+#include <sys/rwlock.h>
+#include <sys/proc.h>
+#include <sys/sched.h>
+
+#include <machine/bus.h>
+
+#include <vm/vm.h>
+#include <vm/pmap.h>
+#include <vm/vm_param.h>
+#include <vm/vm_kern.h>
+#include <vm/vm_object.h>
+#include <vm/vm_map.h>
+#include <vm/vm_page.h>
+#include <vm/vm_pageout.h>
+#include <vm/vm_pager.h>
+#include <vm/vm_radix.h>
+#include <vm/vm_reserv.h>
+#include <vm/vm_extern.h>
+
+#include <vm/uma.h>
+#include <vm/uma_int.h>
+
+#include <linux/gfp.h>
+#include <linux/mm.h>
+#include <linux/preempt.h>
+#include <linux/fs.h>
+
+#if defined(__amd64__) || defined(__aarch64__) || defined(__riscv)
+#define	LINUXKPI_HAVE_DMAP
+#else
+#undef	LINUXKPI_HAVE_DMAP
+#endif
+
+void *
+linux_page_address(struct page *page)
+{
+
+	if (page->object != kmem_object && page->object != kernel_object) {
+#ifdef LINUXKPI_HAVE_DMAP
+		return ((void *)PHYS_TO_DMAP(VM_PAGE_TO_PHYS(page)));
+#else
+		return (NULL);
+#endif
+	}
+	return ((void *)(uintptr_t)(VM_MIN_KERNEL_ADDRESS +
+	    IDX_TO_OFF(page->pindex)));
+}
+
+vm_page_t
+linux_alloc_pages(gfp_t flags, unsigned int order)
+{
+#ifdef LINUXKPI_HAVE_DMAP
+	unsigned long npages = 1UL << order;
+	int req = (flags & M_ZERO) ? (VM_ALLOC_ZERO | VM_ALLOC_NOOBJ |
+	    VM_ALLOC_NORMAL) : (VM_ALLOC_NOOBJ | VM_ALLOC_NORMAL);
+	vm_page_t page;
+
+	if (order == 0 && (flags & GFP_DMA32) == 0) {
+		page = vm_page_alloc(NULL, 0, req);
+		if (page == NULL)
+			return (NULL);
+	} else {
+		vm_paddr_t pmax = (flags & GFP_DMA32) ?
+		    BUS_SPACE_MAXADDR_32BIT : BUS_SPACE_MAXADDR;
+retry:
+		page = vm_page_alloc_contig(NULL, 0, req,
+		    npages, 0, pmax, PAGE_SIZE, 0, VM_MEMATTR_DEFAULT);
+
+		if (page == NULL) {
+			if (flags & M_WAITOK) {
+				if (!vm_page_reclaim_contig(req,
+				    npages, 0, pmax, PAGE_SIZE, 0)) {
+					VM_WAIT;
+				}
+				flags &= ~M_WAITOK;
+				goto retry;
+			}
+			return (NULL);
+		}
+	}
+	if (flags & M_ZERO) {
+		unsigned long x;
+
+		for (x = 0; x != npages; x++) {
+			vm_page_t pgo = page + x;
+
+			if ((pgo->flags & PG_ZERO) == 0)
+				pmap_zero_page(pgo);
+		}
+	}
+#else
+	vm_offset_t vaddr;
+	vm_page_t page;
+
+	vaddr = linux_alloc_kmem(flags, order);
+	if (vaddr == 0)
+		return (NULL);
+
+	page = PHYS_TO_VM_PAGE(vtophys((void *)vaddr));
+
+	KASSERT(vaddr == (vm_offset_t)page_address(page),
+	    ("Page address mismatch"));
+#endif
+	return (page);
+}
+
+void
+linux_free_pages(vm_page_t page, unsigned int order)
+{
+#ifdef LINUXKPI_HAVE_DMAP
+	unsigned long npages = 1UL << order;
+	unsigned long x;
+
+	for (x = 0; x != npages; x++) {
+		vm_page_t pgo = page + x;
+
+		vm_page_lock(pgo);
+		vm_page_free(pgo);
+		vm_page_unlock(pgo);
+	}
+#else
+	vm_offset_t vaddr;
+
+	vaddr = (vm_offset_t)page_address(page);
+
+	linux_free_kmem(vaddr, order);
+#endif
+}
+
+vm_offset_t
+linux_alloc_kmem(gfp_t flags, unsigned int order)
+{
+	size_t size = ((size_t)PAGE_SIZE) << order;
+	vm_offset_t addr;
+
+	if ((flags & GFP_DMA32) == 0) {
+		addr = kmem_malloc(kmem_arena, size, flags & GFP_NATIVE_MASK);
+	} else {
+		addr = kmem_alloc_contig(kmem_arena, size,
+		    flags & GFP_NATIVE_MASK, 0, BUS_SPACE_MAXADDR_32BIT,
+		    PAGE_SIZE, 0, VM_MEMATTR_DEFAULT);
+	}
+	return (addr);
+}
+
+void
+linux_free_kmem(vm_offset_t addr, unsigned int order)
+{
+	size_t size = ((size_t)PAGE_SIZE) << order;
+
+	kmem_free(kmem_arena, addr, size);
+}
+
+static int
+linux_get_user_pages_internal(vm_map_t map, unsigned long start, int nr_pages,
+    int write, struct page **pages)
+{
+	vm_prot_t prot;
+	size_t len;
+	int count;
+	int i;
+
+	prot = write ? (VM_PROT_READ | VM_PROT_WRITE) : VM_PROT_READ;
+	len = ((size_t)nr_pages) << PAGE_SHIFT;
+	count = vm_fault_quick_hold_pages(map, start, len, prot, pages, nr_pages);
+	if (count == -1)
+		return (-EFAULT);
+
+	for (i = 0; i != nr_pages; i++) {
+		struct page *pg = pages[i];
+
+		vm_page_lock(pg);
+		vm_page_wire(pg);
+		vm_page_unhold(pg);
+		vm_page_unlock(pg);
+	}
+	return (nr_pages);
+}
+
+int
+__get_user_pages_fast(unsigned long start, int nr_pages, int write,
+    struct page **pages)
+{
+	vm_map_t map;
+	vm_page_t *mp;
+	vm_offset_t va;
+	vm_offset_t end;
+	vm_prot_t prot;
+	int count;
+
+	if (nr_pages == 0 || in_interrupt())
+		return (0);
+
+	MPASS(pages != NULL);
+	va = start;
+	map = &curthread->td_proc->p_vmspace->vm_map;
+	end = start + (((size_t)nr_pages) << PAGE_SHIFT);
+	if (start < vm_map_min(map) || end > vm_map_max(map))
+		return (-EINVAL);
+	prot = write ? (VM_PROT_READ | VM_PROT_WRITE) : VM_PROT_READ;
+	for (count = 0, mp = pages, va = start; va < end;
+	    mp++, va += PAGE_SIZE, count++) {
+		*mp = pmap_extract_and_hold(map->pmap, va, prot);
+		if (*mp == NULL)
+			break;
+
+		vm_page_lock(*mp);
+		vm_page_wire(*mp);
+		vm_page_unhold(*mp);
+		vm_page_unlock(*mp);
+
+		if ((prot & VM_PROT_WRITE) != 0 &&
+		    (*mp)->dirty != VM_PAGE_BITS_ALL) {
+			/*
+			 * Explicitly dirty the physical page.  Otherwise, the
+			 * caller's changes may go unnoticed because they are
+			 * performed through an unmanaged mapping or by a DMA
+			 * operation.
+			 *
+			 * The object lock is not held here.
+			 * See vm_page_clear_dirty_mask().
+			 */
+			vm_page_dirty(*mp);
+		}
+	}
+	return (count);
+}
+
+long
+get_user_pages_remote(struct task_struct *task, struct mm_struct *mm,
+    unsigned long start, unsigned long nr_pages, int gup_flags,
+    struct page **pages, struct vm_area_struct **vmas)
+{
+	vm_map_t map;
+
+	map = &task->task_thread->td_proc->p_vmspace->vm_map;
+	return (linux_get_user_pages_internal(map, start, nr_pages,
+	    !!(gup_flags & FOLL_WRITE), pages));
+}
+
+long
+get_user_pages(unsigned long start, unsigned long nr_pages, int gup_flags,
+    struct page **pages, struct vm_area_struct **vmas)
+{
+	vm_map_t map;
+
+	map = &curthread->td_proc->p_vmspace->vm_map;
+	return (linux_get_user_pages_internal(map, start, nr_pages,
+	    !!(gup_flags & FOLL_WRITE), pages));
+}
+
+int
+is_vmalloc_addr(const void *addr)
+{
+	return (vtoslab((vm_offset_t)addr & ~UMA_SLAB_MASK) != NULL);
+}
+
+struct page *
+linux_shmem_read_mapping_page_gfp(vm_object_t obj, int pindex, gfp_t gfp)
+{
+	vm_page_t page;
+	int rv;
+
+	if ((gfp & GFP_NOWAIT) != 0)
+		panic("GFP_NOWAIT is unimplemented");
+
+	VM_OBJECT_WLOCK(obj);
+	page = vm_page_grab(obj, pindex, VM_ALLOC_NORMAL | VM_ALLOC_NOBUSY |
+	    VM_ALLOC_WIRED);
+	if (page->valid != VM_PAGE_BITS_ALL) {
+		vm_page_xbusy(page);
+		if (vm_pager_has_page(obj, pindex, NULL, NULL)) {
+			rv = vm_pager_get_pages(obj, &page, 1, NULL, NULL);
+			if (rv != VM_PAGER_OK) {
+				vm_page_lock(page);
+				vm_page_unwire(page, PQ_NONE);
+				vm_page_free(page);
+				vm_page_unlock(page);
+				VM_OBJECT_WUNLOCK(obj);
+				return (ERR_PTR(-EINVAL));
+			}
+			MPASS(page->valid == VM_PAGE_BITS_ALL);
+		} else {
+			pmap_zero_page(page);
+			page->valid = VM_PAGE_BITS_ALL;
+			page->dirty = 0;
+		}
+		vm_page_xunbusy(page);
+	}
+	VM_OBJECT_WUNLOCK(obj);
+	return (page);
+}
+
+struct linux_file *
+linux_shmem_file_setup(const char *name, loff_t size, unsigned long flags)
+{
+	struct fileobj {
+		struct linux_file file __aligned(sizeof(void *));
+		struct vnode vnode __aligned(sizeof(void *));
+	};
+	struct fileobj *fileobj;
+	struct linux_file *filp;
+	struct vnode *vp;
+	int error;
+
+	fileobj = kzalloc(sizeof(*fileobj), GFP_KERNEL);
+	if (fileobj == NULL) {
+		error = -ENOMEM;
+		goto err_0;
+	}
+	filp = &fileobj->file;
+	vp = &fileobj->vnode;
+
+	filp->f_count = 1;
+	filp->f_vnode = vp;
+	filp->f_shmem = vm_pager_allocate(OBJT_DEFAULT, NULL, size,
+	    VM_PROT_READ | VM_PROT_WRITE, 0, curthread->td_ucred);
+	if (filp->f_shmem == NULL) {
+		error = -ENOMEM;
+		goto err_1;
+	}
+	return (filp);
+err_1:
+	kfree(filp);
+err_0:
+	return (ERR_PTR(error));
+}
+
+static vm_ooffset_t
+linux_invalidate_mapping_pages_sub(vm_object_t obj, vm_pindex_t start,
+    vm_pindex_t end, int flags)
+{
+	int start_count, end_count;
+
+	VM_OBJECT_WLOCK(obj);
+	start_count = obj->resident_page_count;
+	vm_object_page_remove(obj, start, end, flags);
+	end_count = obj->resident_page_count;
+	VM_OBJECT_WUNLOCK(obj);
+	return (start_count - end_count);
+}
+
+unsigned long
+linux_invalidate_mapping_pages(vm_object_t obj, pgoff_t start, pgoff_t end)
+{
+
+	return (linux_invalidate_mapping_pages_sub(obj, start, end, OBJPR_CLEANONLY));
+}
+
+void
+linux_shmem_truncate_range(vm_object_t obj, loff_t lstart, loff_t lend)
+{
+	vm_pindex_t start = OFF_TO_IDX(lstart + PAGE_SIZE - 1);
+	vm_pindex_t end = OFF_TO_IDX(lend + 1);
+
+	(void) linux_invalidate_mapping_pages_sub(obj, start, end, 0);
+}
diff --git a/sys/compat/linuxkpi/common/src/linux_pci.c b/sys/compat/linuxkpi/common/src/linux_pci.c
index 22db26495e5..a1fc2461800 100644
--- a/sys/compat/linuxkpi/common/src/linux_pci.c
+++ b/sys/compat/linuxkpi/common/src/linux_pci.c
@@ -108,7 +108,7 @@ linux_pci_probe(device_t dev)
 
 	if ((pdrv = linux_pci_find(dev, &id)) == NULL)
 		return (ENXIO);
-	if (device_get_driver(dev) != &pdrv->driver)
+	if (device_get_driver(dev) != &pdrv->bsddriver)
 		return (ENXIO);
 	device_set_desc(dev, pdrv->name);
 	return (0);
@@ -118,22 +118,39 @@ static int
 linux_pci_attach(device_t dev)
 {
 	struct resource_list_entry *rle;
+	struct pci_bus *pbus;
 	struct pci_dev *pdev;
+	struct pci_devinfo *dinfo;
 	struct pci_driver *pdrv;
 	const struct pci_device_id *id;
-	struct task_struct t;
-	struct thread *td;
+	device_t parent;
+	devclass_t devclass;
 	int error;
 
-	td = curthread;
-	linux_set_current(td, &t);
+	linux_set_current(curthread);
+
 	pdrv = linux_pci_find(dev, &id);
 	pdev = device_get_softc(dev);
+
+	parent = device_get_parent(dev);
+	devclass = device_get_devclass(parent);
+	if (pdrv->isdrm) {
+		dinfo = device_get_ivars(parent);
+		device_set_ivars(dev, dinfo);
+	} else {
+		dinfo = device_get_ivars(dev);
+	}
+
 	pdev->dev.parent = &linux_root_device;
 	pdev->dev.bsddev = dev;
 	INIT_LIST_HEAD(&pdev->dev.irqents);
+	pdev->devfn = PCI_DEVFN(pci_get_slot(dev), pci_get_function(dev));
 	pdev->device = id->device;
 	pdev->vendor = id->vendor;
+	pdev->subsystem_vendor = dinfo->cfg.subvendor;
+	pdev->subsystem_device = dinfo->cfg.subdevice;
+	pdev->class = pci_get_class(dev);
+	pdev->revision = pci_get_revid(dev);
 	pdev->dev.dma_mask = &pdev->dma_mask;
 	pdev->pdrv = pdrv;
 	kobject_init(&pdev->dev.kobj, &linux_dev_ktype);
@@ -146,6 +163,14 @@ linux_pci_attach(device_t dev)
 	else
 		pdev->dev.irq = LINUX_IRQ_INVALID;
 	pdev->irq = pdev->dev.irq;
+
+	if (pdev->bus == NULL) {
+		pbus = malloc(sizeof(*pbus), M_DEVBUF, M_WAITOK | M_ZERO);
+		pbus->self = pdev;
+		pbus->number = pci_get_bus(dev);
+		pdev->bus = pbus;
+	}
+
 	DROP_GIANT();
 	spin_lock(&pci_lock);
 	list_add(&pdev->links, &pci_devices);
@@ -159,7 +184,6 @@ linux_pci_attach(device_t dev)
 		put_device(&pdev->dev);
 		error = -error;
 	}
-	linux_clear_current(td);
 	return (error);
 }
 
@@ -167,11 +191,8 @@ static int
 linux_pci_detach(device_t dev)
 {
 	struct pci_dev *pdev;
-	struct task_struct t;
-	struct thread *td;
 
-	td = curthread;
-	linux_set_current(td, &t);
+	linux_set_current(curthread);
 	pdev = device_get_softc(dev);
 	DROP_GIANT();
 	pdev->pdrv->remove(pdev);
@@ -180,7 +201,6 @@ linux_pci_detach(device_t dev)
 	list_del(&pdev->links);
 	spin_unlock(&pci_lock);
 	put_device(&pdev->dev);
-	linux_clear_current(td);
 
 	return (0);
 }
@@ -188,95 +208,121 @@ linux_pci_detach(device_t dev)
 static int
 linux_pci_suspend(device_t dev)
 {
+	const struct dev_pm_ops *pmops;
 	struct pm_message pm = { };
 	struct pci_dev *pdev;
-	struct task_struct t;
-	struct thread *td;
-	int err;
+	int error;
 
-	td = curthread;
-	linux_set_current(td, &t);
+	error = 0;
+	linux_set_current(curthread);
 	pdev = device_get_softc(dev);
+	pmops = pdev->pdrv->driver.pm;
+
 	if (pdev->pdrv->suspend != NULL)
-		err = -pdev->pdrv->suspend(pdev, pm);
-	else
-		err = 0;
-	linux_clear_current(td);
-	return (err);
+		error = -pdev->pdrv->suspend(pdev, pm);
+	else if (pmops != NULL && pmops->suspend != NULL) {
+		error = -pmops->suspend(&pdev->dev);
+		if (error == 0 && pmops->suspend_late != NULL)
+			error = -pmops->suspend_late(&pdev->dev);
+	}
+	return (error);
 }
 
 static int
 linux_pci_resume(device_t dev)
 {
+	const struct dev_pm_ops *pmops;
 	struct pci_dev *pdev;
-	struct task_struct t;
-	struct thread *td;
-	int err;
+	int error;
 
-	td = curthread;
-	linux_set_current(td, &t);
+	error = 0;
+	linux_set_current(curthread);
 	pdev = device_get_softc(dev);
+	pmops = pdev->pdrv->driver.pm;
+
 	if (pdev->pdrv->resume != NULL)
-		err = -pdev->pdrv->resume(pdev);
-	else
-		err = 0;
-	linux_clear_current(td);
-	return (err);
+		error = -pdev->pdrv->resume(pdev);
+	else if (pmops != NULL && pmops->resume != NULL) {
+		if (pmops->resume_early != NULL)
+			error = -pmops->resume_early(&pdev->dev);
+		if (error == 0 && pmops->resume != NULL)
+			error = -pmops->resume(&pdev->dev);
+	}
+	return (error);
 }
 
 static int
 linux_pci_shutdown(device_t dev)
 {
 	struct pci_dev *pdev;
-	struct task_struct t;
-	struct thread *td;
 
-	td = curthread;
-	linux_set_current(td, &t);
+	linux_set_current(curthread);
 	pdev = device_get_softc(dev);
 	if (pdev->pdrv->shutdown != NULL) {
 		DROP_GIANT();
 		pdev->pdrv->shutdown(pdev);
 		PICKUP_GIANT();
 	}
-	linux_clear_current(td);
 	return (0);
 }
 
-int
-pci_register_driver(struct pci_driver *pdrv)
+static int
+_linux_pci_register_driver(struct pci_driver *pdrv, devclass_t dc)
 {
-	devclass_t bus;
-	int error = 0;
-
-	bus = devclass_find("pci");
+	int error;
 
+	linux_set_current(curthread);
 	spin_lock(&pci_lock);
 	list_add(&pdrv->links, &pci_drivers);
 	spin_unlock(&pci_lock);
-	pdrv->driver.name = pdrv->name;
-	pdrv->driver.methods = pci_methods;
-	pdrv->driver.size = sizeof(struct pci_dev);
+	pdrv->bsddriver.name = pdrv->name;
+	pdrv->bsddriver.methods = pci_methods;
+	pdrv->bsddriver.size = sizeof(struct pci_dev);
+
 	mtx_lock(&Giant);
-	if (bus != NULL) {
-		error = devclass_add_driver(bus, &pdrv->driver, BUS_PASS_DEFAULT,
-		    &pdrv->bsdclass);
-	}
+	error = devclass_add_driver(dc, &pdrv->bsddriver,
+	    BUS_PASS_DEFAULT, &pdrv->bsdclass);
 	mtx_unlock(&Giant);
 	return (-error);
 }
 
+int
+linux_pci_register_driver(struct pci_driver *pdrv)
+{
+	devclass_t dc;
+
+	dc = devclass_find("pci");
+	if (dc == NULL)
+		return (-ENXIO);
+	pdrv->isdrm = false;
+	return (_linux_pci_register_driver(pdrv, dc));
+}
+
+int
+linux_pci_register_drm_driver(struct pci_driver *pdrv)
+{
+	devclass_t dc;
+
+	dc = devclass_create("vgapci");
+	if (dc == NULL)
+		return (-ENXIO);
+	pdrv->isdrm = true;
+	pdrv->name = "drmn";
+	return (_linux_pci_register_driver(pdrv, dc));
+}
+
 void
-pci_unregister_driver(struct pci_driver *pdrv)
+linux_pci_unregister_driver(struct pci_driver *pdrv)
 {
 	devclass_t bus;
 
 	bus = devclass_find("pci");
 
+	spin_lock(&pci_lock);
 	list_del(&pdrv->links);
+	spin_unlock(&pci_lock);
 	mtx_lock(&Giant);
 	if (bus != NULL)
-		devclass_delete_driver(bus, &pdrv->driver);
+		devclass_delete_driver(bus, &pdrv->bsddriver);
 	mtx_unlock(&Giant);
 }
-
diff --git a/sys/compat/linuxkpi/common/src/linux_rcu.c b/sys/compat/linuxkpi/common/src/linux_rcu.c
new file mode 100644
index 00000000000..0ece0342aad
--- /dev/null
+++ b/sys/compat/linuxkpi/common/src/linux_rcu.c
@@ -0,0 +1,398 @@
+/*-
+ * Copyright (c) 2016 Matthew Macy (mmacy@mattmacy.io)
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice unmodified, this list of conditions, and the following
+ *    disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS OR
+ * IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+ * OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.
+ * IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT,
+ * INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT
+ * NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+ * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+ * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF
+ * THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include <sys/cdefs.h>
+__FBSDID("$FreeBSD$");
+
+#include <sys/types.h>
+#include <sys/systm.h>
+#include <sys/malloc.h>
+#include <sys/kernel.h>
+#include <sys/lock.h>
+#include <sys/mutex.h>
+#include <sys/proc.h>
+#include <sys/sched.h>
+#include <sys/smp.h>
+#include <sys/queue.h>
+#include <sys/taskqueue.h>
+#include <sys/kdb.h>
+
+#include <ck_epoch.h>
+
+#include <linux/rcupdate.h>
+#include <linux/srcu.h>
+#include <linux/slab.h>
+#include <linux/kernel.h>
+#include <linux/compat.h>
+
+/*
+ * By defining CONFIG_NO_RCU_SKIP LinuxKPI RCU locks and asserts will
+ * not be skipped during panic().
+ */
+#ifdef CONFIG_NO_RCU_SKIP
+#define	RCU_SKIP(void) 0
+#else
+#define	RCU_SKIP(void)	unlikely(SCHEDULER_STOPPED() || kdb_active)
+#endif
+
+struct callback_head {
+	STAILQ_ENTRY(callback_head) entry;
+	rcu_callback_t func;
+};
+
+struct linux_epoch_head {
+	STAILQ_HEAD(, callback_head) cb_head;
+	struct mtx lock;
+	struct task task;
+} __aligned(CACHE_LINE_SIZE);
+
+struct linux_epoch_record {
+	ck_epoch_record_t epoch_record;
+	TAILQ_HEAD(, task_struct) ts_head;
+	int cpuid;
+} __aligned(CACHE_LINE_SIZE);
+
+/*
+ * Verify that "struct rcu_head" is big enough to hold "struct
+ * callback_head". This has been done to avoid having to add special
+ * compile flags for including ck_epoch.h to all clients of the
+ * LinuxKPI.
+ */
+CTASSERT(sizeof(struct rcu_head) == sizeof(struct callback_head));
+
+/*
+ * Verify that "epoch_record" is at beginning of "struct
+ * linux_epoch_record":
+ */
+CTASSERT(offsetof(struct linux_epoch_record, epoch_record) == 0);
+
+static ck_epoch_t linux_epoch;
+static struct linux_epoch_head linux_epoch_head;
+static DPCPU_DEFINE(struct linux_epoch_record, linux_epoch_record);
+
+static void linux_rcu_cleaner_func(void *, int);
+
+static void
+linux_rcu_runtime_init(void *arg __unused)
+{
+	struct linux_epoch_head *head;
+	int i;
+
+	ck_epoch_init(&linux_epoch);
+
+	head = &linux_epoch_head;
+
+	mtx_init(&head->lock, "LRCU-HEAD", NULL, MTX_DEF);
+	TASK_INIT(&head->task, 0, linux_rcu_cleaner_func, NULL);
+	STAILQ_INIT(&head->cb_head);
+
+	CPU_FOREACH(i) {
+		struct linux_epoch_record *record;
+
+		record = &DPCPU_ID_GET(i, linux_epoch_record);
+
+		record->cpuid = i;
+		ck_epoch_register(&linux_epoch, &record->epoch_record, NULL);
+		TAILQ_INIT(&record->ts_head);
+	}
+}
+SYSINIT(linux_rcu_runtime, SI_SUB_CPU, SI_ORDER_ANY, linux_rcu_runtime_init, NULL);
+
+static void
+linux_rcu_runtime_uninit(void *arg __unused)
+{
+	struct linux_epoch_head *head;
+
+	head = &linux_epoch_head;
+
+	/* destroy head lock */
+	mtx_destroy(&head->lock);
+}
+SYSUNINIT(linux_rcu_runtime, SI_SUB_LOCK, SI_ORDER_SECOND, linux_rcu_runtime_uninit, NULL);
+
+static void
+linux_rcu_cleaner_func(void *context __unused, int pending __unused)
+{
+	struct linux_epoch_head *head;
+	struct callback_head *rcu;
+	STAILQ_HEAD(, callback_head) tmp_head;
+
+	linux_set_current(curthread);
+
+	head = &linux_epoch_head;
+
+	/* move current callbacks into own queue */
+	mtx_lock(&head->lock);
+	STAILQ_INIT(&tmp_head);
+	STAILQ_CONCAT(&tmp_head, &head->cb_head);
+	mtx_unlock(&head->lock);
+
+	/* synchronize */
+	linux_synchronize_rcu();
+
+	/* dispatch all callbacks, if any */
+	while ((rcu = STAILQ_FIRST(&tmp_head)) != NULL) {
+		uintptr_t offset;
+
+		STAILQ_REMOVE_HEAD(&tmp_head, entry);
+
+		offset = (uintptr_t)rcu->func;
+
+		if (offset < LINUX_KFREE_RCU_OFFSET_MAX)
+			kfree((char *)rcu - offset);
+		else
+			rcu->func((struct rcu_head *)rcu);
+	}
+}
+
+void
+linux_rcu_read_lock(void)
+{
+	struct linux_epoch_record *record;
+	struct task_struct *ts;
+
+	if (RCU_SKIP())
+		return;
+
+	/*
+	 * Pin thread to current CPU so that the unlock code gets the
+	 * same per-CPU epoch record:
+	 */
+	sched_pin();
+
+	record = &DPCPU_GET(linux_epoch_record);
+	ts = current;
+
+	/*
+	 * Use a critical section to prevent recursion inside
+	 * ck_epoch_begin(). Else this function supports recursion.
+	 */
+	critical_enter();
+	ck_epoch_begin(&record->epoch_record, NULL);
+	ts->rcu_recurse++;
+	if (ts->rcu_recurse == 1)
+		TAILQ_INSERT_TAIL(&record->ts_head, ts, rcu_entry);
+	critical_exit();
+}
+
+void
+linux_rcu_read_unlock(void)
+{
+	struct linux_epoch_record *record;
+	struct task_struct *ts;
+
+	if (RCU_SKIP())
+		return;
+
+	record = &DPCPU_GET(linux_epoch_record);
+	ts = current;
+
+	/*
+	 * Use a critical section to prevent recursion inside
+	 * ck_epoch_end(). Else this function supports recursion.
+	 */
+	critical_enter();
+	ck_epoch_end(&record->epoch_record, NULL);
+	ts->rcu_recurse--;
+	if (ts->rcu_recurse == 0)
+		TAILQ_REMOVE(&record->ts_head, ts, rcu_entry);
+	critical_exit();
+
+	sched_unpin();
+}
+
+static void
+linux_synchronize_rcu_cb(ck_epoch_t *epoch __unused, ck_epoch_record_t *epoch_record, void *arg __unused)
+{
+	struct linux_epoch_record *record =
+	    container_of(epoch_record, struct linux_epoch_record, epoch_record);
+	struct thread *td = curthread;
+	struct task_struct *ts;
+
+	/* check if blocked on the current CPU */
+	if (record->cpuid == PCPU_GET(cpuid)) {
+		bool is_sleeping = 0;
+		u_char prio = 0;
+
+		/*
+		 * Find the lowest priority or sleeping thread which
+		 * is blocking synchronization on this CPU core. All
+		 * the threads in the queue are CPU-pinned and cannot
+		 * go anywhere while the current thread is locked.
+		 */
+		TAILQ_FOREACH(ts, &record->ts_head, rcu_entry) {
+			if (ts->task_thread->td_priority > prio)
+				prio = ts->task_thread->td_priority;
+			is_sleeping |= (ts->task_thread->td_inhibitors != 0);
+		}
+
+		if (is_sleeping) {
+			thread_unlock(td);
+			pause("W", 1);
+			thread_lock(td);
+		} else {
+			/* set new thread priority */
+			sched_prio(td, prio);
+			/* task switch */
+			mi_switch(SW_VOL | SWT_RELINQUISH, NULL);
+
+			/*
+			 * Release the thread lock while yielding to
+			 * allow other threads to acquire the lock
+			 * pointed to by TDQ_LOCKPTR(td). Else a
+			 * deadlock like situation might happen.
+			 */
+			thread_unlock(td);
+			thread_lock(td);
+		}
+	} else {
+		/*
+		 * To avoid spinning move execution to the other CPU
+		 * which is blocking synchronization. Set highest
+		 * thread priority so that code gets run. The thread
+		 * priority will be restored later.
+		 */
+		sched_prio(td, 0);
+		sched_bind(td, record->cpuid);
+	}
+}
+
+void
+linux_synchronize_rcu(void)
+{
+	struct thread *td;
+	int was_bound;
+	int old_cpu;
+	int old_pinned;
+	u_char old_prio;
+
+	if (RCU_SKIP())
+		return;
+
+	WITNESS_WARN(WARN_GIANTOK | WARN_SLEEPOK, NULL,
+	    "linux_synchronize_rcu() can sleep");
+
+	td = curthread;
+
+	DROP_GIANT();
+
+	/*
+	 * Synchronizing RCU might change the CPU core this function
+	 * is running on. Save current values:
+	 */
+	thread_lock(td);
+
+	old_cpu = PCPU_GET(cpuid);
+	old_pinned = td->td_pinned;
+	old_prio = td->td_priority;
+	was_bound = sched_is_bound(td);
+	sched_unbind(td);
+	td->td_pinned = 0;
+	sched_bind(td, old_cpu);
+
+	ck_epoch_synchronize_wait(&linux_epoch,
+	    &linux_synchronize_rcu_cb, NULL);
+
+	/* restore CPU binding, if any */
+	if (was_bound != 0) {
+		sched_bind(td, old_cpu);
+	} else {
+		/* get thread back to initial CPU, if any */
+		if (old_pinned != 0)
+			sched_bind(td, old_cpu);
+		sched_unbind(td);
+	}
+	/* restore pinned after bind */
+	td->td_pinned = old_pinned;
+
+	/* restore thread priority */
+	sched_prio(td, old_prio);
+	thread_unlock(td);
+
+	PICKUP_GIANT();
+}
+
+void
+linux_rcu_barrier(void)
+{
+	struct linux_epoch_head *head;
+
+	linux_synchronize_rcu();
+
+	head = &linux_epoch_head;
+
+	/* wait for callbacks to complete */
+	taskqueue_drain(taskqueue_fast, &head->task);
+}
+
+void
+linux_call_rcu(struct rcu_head *context, rcu_callback_t func)
+{
+	struct callback_head *rcu = (struct callback_head *)context;
+	struct linux_epoch_head *head = &linux_epoch_head;
+
+	mtx_lock(&head->lock);
+	rcu->func = func;
+	STAILQ_INSERT_TAIL(&head->cb_head, rcu, entry);
+	taskqueue_enqueue(taskqueue_fast, &head->task);
+	mtx_unlock(&head->lock);
+}
+
+int
+init_srcu_struct(struct srcu_struct *srcu)
+{
+	return (0);
+}
+
+void
+cleanup_srcu_struct(struct srcu_struct *srcu)
+{
+}
+
+int
+srcu_read_lock(struct srcu_struct *srcu)
+{
+	linux_rcu_read_lock();
+	return (0);
+}
+
+void
+srcu_read_unlock(struct srcu_struct *srcu, int key __unused)
+{
+	linux_rcu_read_unlock();
+}
+
+void
+synchronize_srcu(struct srcu_struct *srcu)
+{
+	linux_synchronize_rcu();
+}
+
+void
+srcu_barrier(struct srcu_struct *srcu)
+{
+	linux_rcu_barrier();
+}
diff --git a/sys/compat/linuxkpi/common/src/linux_schedule.c b/sys/compat/linuxkpi/common/src/linux_schedule.c
new file mode 100644
index 00000000000..dc3dd918d1e
--- /dev/null
+++ b/sys/compat/linuxkpi/common/src/linux_schedule.c
@@ -0,0 +1,393 @@
+/*-
+ * Copyright (c) 2017 Mark Johnston <markj@FreeBSD.org>
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conds
+ * are met:
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice unmodified, this list of conds, and the following
+ *    disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conds and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS OR
+ * IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+ * OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.
+ * IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT,
+ * INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT
+ * NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+ * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+ * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF
+ * THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include <sys/cdefs.h>
+__FBSDID("$FreeBSD$");
+
+#include <sys/param.h>
+#include <sys/systm.h>
+#include <sys/proc.h>
+#include <sys/signalvar.h>
+#include <sys/sleepqueue.h>
+
+#include <linux/errno.h>
+#include <linux/kernel.h>
+#include <linux/list.h>
+#include <linux/sched.h>
+#include <linux/spinlock.h>
+#include <linux/wait.h>
+
+static int
+linux_add_to_sleepqueue(void *wchan, const char *wmesg, int timeout, int state)
+{
+	int flags, ret;
+
+	MPASS((state & ~TASK_NORMAL) == 0);
+
+	flags = SLEEPQ_SLEEP | ((state & TASK_INTERRUPTIBLE) != 0 ?
+	    SLEEPQ_INTERRUPTIBLE : 0);
+
+	sleepq_add(wchan, NULL, wmesg, flags, 0);
+	if (timeout != 0)
+		sleepq_set_timeout(wchan, timeout);
+	if ((state & TASK_INTERRUPTIBLE) != 0) {
+		if (timeout == 0)
+			ret = -sleepq_wait_sig(wchan, 0);
+		else
+			ret = -sleepq_timedwait_sig(wchan, 0);
+	} else {
+		if (timeout == 0) {
+			sleepq_wait(wchan, 0);
+			ret = 0;
+		} else
+			ret = -sleepq_timedwait(wchan, 0);
+	}
+	/* filter return value */
+	if (ret != 0 && ret != -EWOULDBLOCK)
+		ret = -ERESTARTSYS;
+	return (ret);
+}
+
+static int
+wake_up_task(struct task_struct *task, unsigned int state)
+{
+	int ret, wakeup_swapper;
+
+	ret = wakeup_swapper = 0;
+	sleepq_lock(task);
+	if ((atomic_read(&task->state) & state) != 0) {
+		set_task_state(task, TASK_WAKING);
+		wakeup_swapper = sleepq_signal(task, SLEEPQ_SLEEP, 0, 0);
+		ret = 1;
+	}
+	sleepq_release(task);
+	if (wakeup_swapper)
+		kick_proc0();
+	return (ret);
+}
+
+bool
+linux_signal_pending(struct task_struct *task)
+{
+	struct thread *td;
+	sigset_t pending;
+
+	td = task->task_thread;
+	PROC_LOCK(td->td_proc);
+	pending = td->td_siglist;
+	SIGSETOR(pending, td->td_proc->p_siglist);
+	SIGSETNAND(pending, td->td_sigmask);
+	PROC_UNLOCK(td->td_proc);
+	return (!SIGISEMPTY(pending));
+}
+
+bool
+linux_fatal_signal_pending(struct task_struct *task)
+{
+	struct thread *td;
+	bool ret;
+
+	td = task->task_thread;
+	PROC_LOCK(td->td_proc);
+	ret = SIGISMEMBER(td->td_siglist, SIGKILL) ||
+	    SIGISMEMBER(td->td_proc->p_siglist, SIGKILL);
+	PROC_UNLOCK(td->td_proc);
+	return (ret);
+}
+
+bool
+linux_signal_pending_state(long state, struct task_struct *task)
+{
+
+	MPASS((state & ~TASK_NORMAL) == 0);
+
+	if ((state & TASK_INTERRUPTIBLE) == 0)
+		return (false);
+	return (linux_signal_pending(task));
+}
+
+void
+linux_send_sig(int signo, struct task_struct *task)
+{
+	struct thread *td;
+
+	td = task->task_thread;
+	PROC_LOCK(td->td_proc);
+	tdsignal(td, signo);
+	PROC_UNLOCK(td->td_proc);
+}
+
+int
+autoremove_wake_function(wait_queue_t *wq, unsigned int state, int flags,
+    void *key __unused)
+{
+	struct task_struct *task;
+	int ret;
+
+	task = wq->private;
+	if ((ret = wake_up_task(task, state)) != 0)
+		list_del_init(&wq->task_list);
+	return (ret);
+}
+
+void
+linux_wake_up(wait_queue_head_t *wqh, unsigned int state, int nr, bool locked)
+{
+	wait_queue_t *pos, *next;
+
+	if (!locked)
+		spin_lock(&wqh->lock);
+	list_for_each_entry_safe(pos, next, &wqh->task_list, task_list) {
+		if (pos->func == NULL) {
+			if (wake_up_task(pos->private, state) != 0 && --nr == 0)
+				break;
+		} else {
+			if (pos->func(pos, state, 0, NULL) != 0 && --nr == 0)
+				break;
+		}
+	}
+	if (!locked)
+		spin_unlock(&wqh->lock);
+}
+
+void
+linux_prepare_to_wait(wait_queue_head_t *wqh, wait_queue_t *wq, int state)
+{
+
+	spin_lock(&wqh->lock);
+	if (list_empty(&wq->task_list))
+		__add_wait_queue(wqh, wq);
+	set_task_state(current, state);
+	spin_unlock(&wqh->lock);
+}
+
+void
+linux_finish_wait(wait_queue_head_t *wqh, wait_queue_t *wq)
+{
+
+	spin_lock(&wqh->lock);
+	set_task_state(current, TASK_RUNNING);
+	if (!list_empty(&wq->task_list)) {
+		__remove_wait_queue(wqh, wq);
+		INIT_LIST_HEAD(&wq->task_list);
+	}
+	spin_unlock(&wqh->lock);
+}
+
+bool
+linux_waitqueue_active(wait_queue_head_t *wqh)
+{
+	bool ret;
+
+	spin_lock(&wqh->lock);
+	ret = !list_empty(&wqh->task_list);
+	spin_unlock(&wqh->lock);
+	return (ret);
+}
+
+int
+linux_wait_event_common(wait_queue_head_t *wqh, wait_queue_t *wq, int timeout,
+    unsigned int state, spinlock_t *lock)
+{
+	struct task_struct *task;
+	int ret;
+
+	if (lock != NULL)
+		spin_unlock_irq(lock);
+
+	DROP_GIANT();
+
+	/* range check timeout */
+	if (timeout < 1)
+		timeout = 1;
+	else if (timeout == MAX_SCHEDULE_TIMEOUT)
+		timeout = 0;
+
+	task = current;
+
+	/*
+	 * Our wait queue entry is on the stack - make sure it doesn't
+	 * get swapped out while we sleep.
+	 */
+	PHOLD(task->task_thread->td_proc);
+	sleepq_lock(task);
+	if (atomic_read(&task->state) != TASK_WAKING) {
+		ret = linux_add_to_sleepqueue(task, "wevent", timeout, state);
+	} else {
+		sleepq_release(task);
+		ret = linux_signal_pending_state(state, task) ? -ERESTARTSYS : 0;
+	}
+	PRELE(task->task_thread->td_proc);
+
+	PICKUP_GIANT();
+
+	if (lock != NULL)
+		spin_lock_irq(lock);
+	return (ret);
+}
+
+int
+linux_schedule_timeout(int timeout)
+{
+	struct task_struct *task;
+	int state;
+	int remainder;
+
+	task = current;
+
+	/* range check timeout */
+	if (timeout < 1)
+		timeout = 1;
+	else if (timeout == MAX_SCHEDULE_TIMEOUT)
+		timeout = 0;
+
+	remainder = ticks + timeout;
+
+	DROP_GIANT();
+
+	sleepq_lock(task);
+	state = atomic_read(&task->state);
+	if (state != TASK_WAKING)
+		(void)linux_add_to_sleepqueue(task, "sched", timeout, state);
+	else
+		sleepq_release(task);
+	set_task_state(task, TASK_RUNNING);
+
+	PICKUP_GIANT();
+
+	if (timeout == 0)
+		return (MAX_SCHEDULE_TIMEOUT);
+
+	/* range check return value */
+	remainder -= ticks;
+	if (remainder < 0)
+		remainder = 0;
+	else if (remainder > timeout)
+		remainder = timeout;
+	return (remainder);
+}
+
+static void
+wake_up_sleepers(void *wchan)
+{
+	int wakeup_swapper;
+
+	sleepq_lock(wchan);
+	wakeup_swapper = sleepq_signal(wchan, SLEEPQ_SLEEP, 0, 0);
+	sleepq_release(wchan);
+	if (wakeup_swapper)
+		kick_proc0();
+}
+
+#define	bit_to_wchan(word, bit)	((void *)(((uintptr_t)(word) << 6) | (bit)))
+
+void
+linux_wake_up_bit(void *word, int bit)
+{
+
+	wake_up_sleepers(bit_to_wchan(word, bit));
+}
+
+int
+linux_wait_on_bit_timeout(unsigned long *word, int bit, unsigned int state,
+    int timeout)
+{
+	struct task_struct *task;
+	void *wchan;
+	int ret;
+
+	DROP_GIANT();
+
+	/* range check timeout */
+	if (timeout < 1)
+		timeout = 1;
+	else if (timeout == MAX_SCHEDULE_TIMEOUT)
+		timeout = 0;
+
+	task = current;
+	wchan = bit_to_wchan(word, bit);
+	for (;;) {
+		sleepq_lock(wchan);
+		if ((*word & (1 << bit)) == 0) {
+			sleepq_release(wchan);
+			ret = 0;
+			break;
+		}
+		set_task_state(task, state);
+		ret = linux_add_to_sleepqueue(wchan, "wbit", timeout, state);
+		if (ret != 0)
+			break;
+	}
+	set_task_state(task, TASK_RUNNING);
+
+	PICKUP_GIANT();
+
+	return (ret);
+}
+
+void
+linux_wake_up_atomic_t(atomic_t *a)
+{
+
+	wake_up_sleepers(a);
+}
+
+int
+linux_wait_on_atomic_t(atomic_t *a, unsigned int state)
+{
+	struct task_struct *task;
+	void *wchan;
+	int ret;
+
+	DROP_GIANT();
+
+	task = current;
+	wchan = a;
+	for (;;) {
+		sleepq_lock(wchan);
+		if (atomic_read(a) == 0) {
+			sleepq_release(wchan);
+			ret = 0;
+			break;
+		}
+		set_task_state(task, state);
+		ret = linux_add_to_sleepqueue(wchan, "watomic", 0, state);
+		if (ret != 0)
+			break;
+	}
+	set_task_state(task, TASK_RUNNING);
+
+	PICKUP_GIANT();
+
+	return (ret);
+}
+
+bool
+linux_wake_up_state(struct task_struct *task, unsigned int state)
+{
+
+	return (wake_up_task(task, state) != 0);
+}
diff --git a/sys/compat/linuxkpi/common/src/linux_slab.c b/sys/compat/linuxkpi/common/src/linux_slab.c
new file mode 100644
index 00000000000..34bff0c8a2c
--- /dev/null
+++ b/sys/compat/linuxkpi/common/src/linux_slab.c
@@ -0,0 +1,128 @@
+/*-
+ * Copyright (c) 2017 Mellanox Technologies, Ltd.
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice unmodified, this list of conditions, and the following
+ *    disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS OR
+ * IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+ * OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.
+ * IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT,
+ * INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT
+ * NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+ * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+ * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF
+ * THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include <sys/cdefs.h>
+__FBSDID("$FreeBSD$");
+
+#include <linux/slab.h>
+#include <linux/rcupdate.h>
+#include <linux/kernel.h>
+
+struct linux_kmem_rcu {
+	struct rcu_head rcu_head;
+	struct linux_kmem_cache *cache;
+};
+
+#define	LINUX_KMEM_TO_RCU(c, m)					\
+	((struct linux_kmem_rcu *)((char *)(m) +		\
+	(c)->cache_size - sizeof(struct linux_kmem_rcu)))
+
+#define	LINUX_RCU_TO_KMEM(r)					\
+	((void *)((char *)(r) + sizeof(struct linux_kmem_rcu) - \
+	(r)->cache->cache_size))
+
+static int
+linux_kmem_ctor(void *mem, int size, void *arg, int flags)
+{
+	struct linux_kmem_cache *c = arg;
+
+	if (unlikely(c->cache_flags & SLAB_DESTROY_BY_RCU)) {
+		struct linux_kmem_rcu *rcu = LINUX_KMEM_TO_RCU(c, mem);
+
+		/* duplicate cache pointer */
+		rcu->cache = c;
+	}
+
+	/* check for constructor */
+	if (likely(c->cache_ctor != NULL))
+		c->cache_ctor(mem);
+
+	return (0);
+}
+
+static void
+linux_kmem_cache_free_rcu_callback(struct rcu_head *head)
+{
+	struct linux_kmem_rcu *rcu =
+	    container_of(head, struct linux_kmem_rcu, rcu_head);
+
+	uma_zfree(rcu->cache->cache_zone, LINUX_RCU_TO_KMEM(rcu));
+}
+
+struct linux_kmem_cache *
+linux_kmem_cache_create(const char *name, size_t size, size_t align,
+    unsigned flags, linux_kmem_ctor_t *ctor)
+{
+	struct linux_kmem_cache *c;
+
+	c = malloc(sizeof(*c), M_KMALLOC, M_WAITOK);
+
+	if (flags & SLAB_HWCACHE_ALIGN)
+		align = UMA_ALIGN_CACHE;
+	else if (align != 0)
+		align--;
+
+	if (flags & SLAB_DESTROY_BY_RCU) {
+		/* make room for RCU structure */
+		size = ALIGN(size, sizeof(void *));
+		size += sizeof(struct linux_kmem_rcu);
+
+		/* create cache_zone */
+		c->cache_zone = uma_zcreate(name, size,
+		    linux_kmem_ctor, NULL, NULL, NULL,
+		    align, UMA_ZONE_ZINIT);
+	} else {
+		/* create cache_zone */
+		c->cache_zone = uma_zcreate(name, size,
+		    ctor ? linux_kmem_ctor : NULL, NULL,
+		    NULL, NULL, align, 0);
+	}
+
+	c->cache_flags = flags;
+	c->cache_ctor = ctor;
+	c->cache_size = size;
+	return (c);
+}
+
+void
+linux_kmem_cache_free_rcu(struct linux_kmem_cache *c, void *m)
+{
+	struct linux_kmem_rcu *rcu = LINUX_KMEM_TO_RCU(c, m);
+
+	call_rcu(&rcu->rcu_head, linux_kmem_cache_free_rcu_callback);
+}
+
+void
+linux_kmem_cache_destroy(struct linux_kmem_cache *c)
+{
+	if (unlikely(c->cache_flags & SLAB_DESTROY_BY_RCU)) {
+		/* make sure all free callbacks have been called */
+		rcu_barrier();
+	}
+
+	uma_zdestroy(c->cache_zone);
+	free(c, M_KMALLOC);
+}
diff --git a/sys/compat/linuxkpi/common/src/linux_tasklet.c b/sys/compat/linuxkpi/common/src/linux_tasklet.c
new file mode 100644
index 00000000000..5fe94553c9d
--- /dev/null
+++ b/sys/compat/linuxkpi/common/src/linux_tasklet.c
@@ -0,0 +1,198 @@
+/*-
+ * Copyright (c) 2017 Hans Petter Selasky
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice unmodified, this list of conditions, and the following
+ *    disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS OR
+ * IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+ * OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.
+ * IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT,
+ * INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT
+ * NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+ * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+ * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF
+ * THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include <sys/cdefs.h>
+__FBSDID("$FreeBSD$");
+
+#include <sys/types.h>
+#include <sys/malloc.h>
+#include <sys/gtaskqueue.h>
+#include <sys/proc.h>
+#include <sys/sched.h>
+
+#include <linux/compiler.h>
+#include <linux/interrupt.h>
+#include <linux/compat.h>
+
+#define	TASKLET_ST_IDLE 0
+#define	TASKLET_ST_BUSY 1
+#define	TASKLET_ST_EXEC 2
+#define	TASKLET_ST_LOOP 3
+
+#define	TASKLET_ST_CMPSET(ts, old, new)	\
+	atomic_cmpset_ptr((volatile uintptr_t *)&(ts)->entry.tqe_prev, old, new)
+
+#define	TASKLET_ST_SET(ts, new)	\
+	WRITE_ONCE(*(volatile uintptr_t *)&(ts)->entry.tqe_prev, new)
+
+#define	TASKLET_ST_GET(ts) \
+	READ_ONCE(*(volatile uintptr_t *)&(ts)->entry.tqe_prev)
+
+struct tasklet_worker {
+	struct mtx mtx;
+	TAILQ_HEAD(, tasklet_struct) head;
+	struct grouptask gtask;
+} __aligned(CACHE_LINE_SIZE);
+
+#define	TASKLET_WORKER_LOCK(tw) mtx_lock(&(tw)->mtx)
+#define	TASKLET_WORKER_UNLOCK(tw) mtx_unlock(&(tw)->mtx)
+
+static DPCPU_DEFINE(struct tasklet_worker, tasklet_worker);
+
+static void
+tasklet_handler(void *arg)
+{
+	struct tasklet_worker *tw = (struct tasklet_worker *)arg;
+	struct tasklet_struct *ts;
+
+	linux_set_current(curthread);
+
+	TASKLET_WORKER_LOCK(tw);
+	while (1) {
+		ts = TAILQ_FIRST(&tw->head);
+		if (ts == NULL)
+			break;
+		TAILQ_REMOVE(&tw->head, ts, entry);
+
+		TASKLET_WORKER_UNLOCK(tw);
+		do {
+			/* reset executing state */
+			TASKLET_ST_SET(ts, TASKLET_ST_EXEC);
+
+			ts->func(ts->data);
+
+		} while (TASKLET_ST_CMPSET(ts, TASKLET_ST_EXEC, TASKLET_ST_IDLE) == 0);
+		TASKLET_WORKER_LOCK(tw);
+	}
+	TASKLET_WORKER_UNLOCK(tw);
+}
+
+static void
+tasklet_subsystem_init(void *arg __unused)
+{
+	struct tasklet_worker *tw;
+	char buf[32];
+	int i;
+
+	CPU_FOREACH(i) {
+		if (CPU_ABSENT(i))
+			continue;
+
+		tw = DPCPU_ID_PTR(i, tasklet_worker);
+
+		mtx_init(&tw->mtx, "linux_tasklet", NULL, MTX_DEF);
+		TAILQ_INIT(&tw->head);
+		GROUPTASK_INIT(&tw->gtask, 0, tasklet_handler, tw);
+		snprintf(buf, sizeof(buf), "softirq%d", i);
+		taskqgroup_attach_cpu(qgroup_softirq, &tw->gtask,
+		    "tasklet", i, -1, buf);
+       }
+}
+SYSINIT(linux_tasklet, SI_SUB_TASKQ, SI_ORDER_THIRD, tasklet_subsystem_init, NULL);
+
+static void
+tasklet_subsystem_uninit(void *arg __unused)
+{
+	struct tasklet_worker *tw;
+	int i;
+
+	CPU_FOREACH(i) {
+		if (CPU_ABSENT(i))
+			continue;
+
+		tw = DPCPU_ID_PTR(i, tasklet_worker);
+
+		taskqgroup_detach(qgroup_softirq, &tw->gtask);
+		mtx_destroy(&tw->mtx);
+	}
+}
+SYSUNINIT(linux_tasklet, SI_SUB_TASKQ, SI_ORDER_THIRD, tasklet_subsystem_uninit, NULL);
+
+void
+tasklet_init(struct tasklet_struct *ts,
+    tasklet_func_t *func, unsigned long data)
+{
+	ts->entry.tqe_prev = NULL;
+	ts->entry.tqe_next = NULL;
+	ts->func = func;
+	ts->data = data;
+}
+
+void
+local_bh_enable(void)
+{
+	sched_unpin();
+}
+
+void
+local_bh_disable(void)
+{
+	sched_pin();
+}
+
+void
+tasklet_schedule(struct tasklet_struct *ts)
+{
+
+	if (TASKLET_ST_CMPSET(ts, TASKLET_ST_EXEC, TASKLET_ST_LOOP)) {
+		/* tasklet_handler() will loop */
+	} else if (TASKLET_ST_CMPSET(ts, TASKLET_ST_IDLE, TASKLET_ST_BUSY)) {
+		struct tasklet_worker *tw;
+
+		tw = &DPCPU_GET(tasklet_worker);
+
+		/* tasklet_handler() was not queued */
+		TASKLET_WORKER_LOCK(tw);
+		/* enqueue tasklet */
+		TAILQ_INSERT_TAIL(&tw->head, ts, entry);
+		/* schedule worker */
+		GROUPTASK_ENQUEUE(&tw->gtask);
+		TASKLET_WORKER_UNLOCK(tw);
+	} else {
+		/*
+		 * tasklet_handler() is already executing
+		 *
+		 * If the state is neither EXEC nor IDLE, it is either
+		 * LOOP or BUSY. If the state changed between the two
+		 * CMPSET's above the only possible transitions by
+		 * elimination are LOOP->EXEC and BUSY->EXEC. If a
+		 * EXEC->LOOP transition was missed that is not a
+		 * problem because the callback function is then
+		 * already about to be called again.
+		 */
+	}
+}
+
+void
+tasklet_kill(struct tasklet_struct *ts)
+{
+
+	WITNESS_WARN(WARN_GIANTOK | WARN_SLEEPOK, NULL, "tasklet_kill() can sleep");
+
+	/* wait until tasklet is no longer busy */
+	while (TASKLET_ST_GET(ts) != TASKLET_ST_IDLE)
+		pause("W", 1);
+}
diff --git a/sys/compat/linuxkpi/common/src/linux_work.c b/sys/compat/linuxkpi/common/src/linux_work.c
new file mode 100644
index 00000000000..76d3a2225f0
--- /dev/null
+++ b/sys/compat/linuxkpi/common/src/linux_work.c
@@ -0,0 +1,612 @@
+/*-
+ * Copyright (c) 2017 Hans Petter Selasky
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice unmodified, this list of conditions, and the following
+ *    disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS OR
+ * IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+ * OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.
+ * IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT,
+ * INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT
+ * NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+ * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+ * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF
+ * THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include <sys/cdefs.h>
+__FBSDID("$FreeBSD$");
+
+#include <linux/workqueue.h>
+#include <linux/wait.h>
+#include <linux/compat.h>
+#include <linux/spinlock.h>
+
+#include <sys/kernel.h>
+
+/*
+ * Define all work struct states
+ */
+enum {
+	WORK_ST_IDLE,			/* idle - not started */
+	WORK_ST_TIMER,			/* timer is being started */
+	WORK_ST_TASK,			/* taskqueue is being queued */
+	WORK_ST_EXEC,			/* callback is being called */
+	WORK_ST_CANCEL,			/* cancel is being requested */
+	WORK_ST_MAX,
+};
+
+/*
+ * Define global workqueues
+ */
+static struct workqueue_struct *linux_system_short_wq;
+static struct workqueue_struct *linux_system_long_wq;
+
+struct workqueue_struct *system_wq;
+struct workqueue_struct *system_long_wq;
+struct workqueue_struct *system_unbound_wq;
+struct workqueue_struct *system_power_efficient_wq;
+
+static int linux_default_wq_cpus = 4;
+
+static void linux_delayed_work_timer_fn(void *);
+
+/*
+ * This function atomically updates the work state and returns the
+ * previous state at the time of update.
+ */
+static uint8_t
+linux_update_state(atomic_t *v, const uint8_t *pstate)
+{
+	int c, old;
+
+	c = v->counter;
+
+	while ((old = atomic_cmpxchg(v, c, pstate[c])) != c)
+		c = old;
+
+	return (c);
+}
+
+/*
+ * A LinuxKPI task is allowed to free itself inside the callback function
+ * and cannot safely be referred after the callback function has
+ * completed. This function gives the linux_work_fn() function a hint,
+ * that the task is not going away and can have its state checked
+ * again. Without this extra hint LinuxKPI tasks cannot be serialized
+ * accross multiple worker threads.
+ */
+static bool
+linux_work_exec_unblock(struct work_struct *work)
+{
+	struct workqueue_struct *wq;
+	struct work_exec *exec;
+	bool retval = 0;
+
+	wq = work->work_queue;
+	if (unlikely(wq == NULL))
+		goto done;
+
+	WQ_EXEC_LOCK(wq);
+	TAILQ_FOREACH(exec, &wq->exec_head, entry) {
+		if (exec->target == work) {
+			exec->target = NULL;
+			retval = 1;
+			break;
+		}
+	}
+	WQ_EXEC_UNLOCK(wq);
+done:
+	return (retval);
+}
+
+static void
+linux_delayed_work_enqueue(struct delayed_work *dwork)
+{
+	struct taskqueue *tq;
+
+	tq = dwork->work.work_queue->taskqueue;
+	taskqueue_enqueue(tq, &dwork->work.work_task);
+}
+
+/*
+ * This function queues the given work structure on the given
+ * workqueue. It returns non-zero if the work was successfully
+ * [re-]queued. Else the work is already pending for completion.
+ */
+bool
+linux_queue_work_on(int cpu __unused, struct workqueue_struct *wq,
+    struct work_struct *work)
+{
+	static const uint8_t states[WORK_ST_MAX] __aligned(8) = {
+		[WORK_ST_IDLE] = WORK_ST_TASK,		/* start queuing task */
+		[WORK_ST_TIMER] = WORK_ST_TIMER,	/* NOP */
+		[WORK_ST_TASK] = WORK_ST_TASK,		/* NOP */
+		[WORK_ST_EXEC] = WORK_ST_TASK,		/* queue task another time */
+		[WORK_ST_CANCEL] = WORK_ST_TASK,	/* start queuing task again */
+	};
+
+	if (atomic_read(&wq->draining) != 0)
+		return (!work_pending(work));
+
+	switch (linux_update_state(&work->state, states)) {
+	case WORK_ST_EXEC:
+	case WORK_ST_CANCEL:
+		if (linux_work_exec_unblock(work) != 0)
+			return (1);
+		/* FALLTHROUGH */
+	case WORK_ST_IDLE:
+		work->work_queue = wq;
+		taskqueue_enqueue(wq->taskqueue, &work->work_task);
+		return (1);
+	default:
+		return (0);		/* already on a queue */
+	}
+}
+
+/*
+ * This function queues the given work structure on the given
+ * workqueue after a given delay in ticks. It returns non-zero if the
+ * work was successfully [re-]queued. Else the work is already pending
+ * for completion.
+ */
+bool
+linux_queue_delayed_work_on(int cpu, struct workqueue_struct *wq,
+    struct delayed_work *dwork, unsigned delay)
+{
+	static const uint8_t states[WORK_ST_MAX] __aligned(8) = {
+		[WORK_ST_IDLE] = WORK_ST_TIMER,		/* start timeout */
+		[WORK_ST_TIMER] = WORK_ST_TIMER,	/* NOP */
+		[WORK_ST_TASK] = WORK_ST_TASK,		/* NOP */
+		[WORK_ST_EXEC] = WORK_ST_TIMER,		/* start timeout */
+		[WORK_ST_CANCEL] = WORK_ST_TIMER,	/* start timeout */
+	};
+
+	if (atomic_read(&wq->draining) != 0)
+		return (!work_pending(&dwork->work));
+
+	switch (linux_update_state(&dwork->work.state, states)) {
+	case WORK_ST_EXEC:
+	case WORK_ST_CANCEL:
+		if (delay == 0 && linux_work_exec_unblock(&dwork->work) != 0) {
+			dwork->timer.expires = jiffies;
+			return (1);
+		}
+		/* FALLTHROUGH */
+	case WORK_ST_IDLE:
+		dwork->work.work_queue = wq;
+		dwork->timer.expires = jiffies + delay;
+
+		if (delay == 0) {
+			linux_delayed_work_enqueue(dwork);
+		} else if (unlikely(cpu != WORK_CPU_UNBOUND)) {
+			mtx_lock(&dwork->timer.mtx);
+			callout_reset_on(&dwork->timer.callout, delay,
+			    &linux_delayed_work_timer_fn, dwork, cpu);
+			mtx_unlock(&dwork->timer.mtx);
+		} else {
+			mtx_lock(&dwork->timer.mtx);
+			callout_reset(&dwork->timer.callout, delay,
+			    &linux_delayed_work_timer_fn, dwork);
+			mtx_unlock(&dwork->timer.mtx);
+		}
+		return (1);
+	default:
+		return (0);		/* already on a queue */
+	}
+}
+
+void
+linux_work_fn(void *context, int pending)
+{
+	static const uint8_t states[WORK_ST_MAX] __aligned(8) = {
+		[WORK_ST_IDLE] = WORK_ST_IDLE,		/* NOP */
+		[WORK_ST_TIMER] = WORK_ST_EXEC,		/* delayed work w/o timeout */
+		[WORK_ST_TASK] = WORK_ST_EXEC,		/* call callback */
+		[WORK_ST_EXEC] = WORK_ST_IDLE,		/* complete callback */
+		[WORK_ST_CANCEL] = WORK_ST_EXEC,	/* failed to cancel */
+	};
+	struct work_struct *work;
+	struct workqueue_struct *wq;
+	struct work_exec exec;
+
+	linux_set_current(curthread);
+
+	/* setup local variables */
+	work = context;
+	wq = work->work_queue;
+
+	/* store target pointer */
+	exec.target = work;
+
+	/* insert executor into list */
+	WQ_EXEC_LOCK(wq);
+	TAILQ_INSERT_TAIL(&wq->exec_head, &exec, entry);
+	while (1) {
+		switch (linux_update_state(&work->state, states)) {
+		case WORK_ST_TIMER:
+		case WORK_ST_TASK:
+		case WORK_ST_CANCEL:
+			WQ_EXEC_UNLOCK(wq);
+
+			/* call work function */
+			work->func(work);
+
+			WQ_EXEC_LOCK(wq);
+			/* check if unblocked */
+			if (exec.target != work) {
+				/* reapply block */
+				exec.target = work;
+				break;
+			}
+			/* FALLTHROUGH */
+		default:
+			goto done;
+		}
+	}
+done:
+	/* remove executor from list */
+	TAILQ_REMOVE(&wq->exec_head, &exec, entry);
+	WQ_EXEC_UNLOCK(wq);
+}
+
+void
+linux_delayed_work_fn(void *context, int pending)
+{
+	struct delayed_work *dwork = context;
+
+	/*
+	 * Make sure the timer belonging to the delayed work gets
+	 * drained before invoking the work function. Else the timer
+	 * mutex may still be in use which can lead to use-after-free
+	 * situations, because the work function might free the work
+	 * structure before returning.
+	 */
+	callout_drain(&dwork->timer.callout);
+
+	linux_work_fn(&dwork->work, pending);
+}
+
+static void
+linux_delayed_work_timer_fn(void *arg)
+{
+	static const uint8_t states[WORK_ST_MAX] __aligned(8) = {
+		[WORK_ST_IDLE] = WORK_ST_IDLE,		/* NOP */
+		[WORK_ST_TIMER] = WORK_ST_TASK,		/* start queueing task */
+		[WORK_ST_TASK] = WORK_ST_TASK,		/* NOP */
+		[WORK_ST_EXEC] = WORK_ST_EXEC,		/* NOP */
+		[WORK_ST_CANCEL] = WORK_ST_TASK,	/* failed to cancel */
+	};
+	struct delayed_work *dwork = arg;
+
+	switch (linux_update_state(&dwork->work.state, states)) {
+	case WORK_ST_TIMER:
+	case WORK_ST_CANCEL:
+		linux_delayed_work_enqueue(dwork);
+		break;
+	default:
+		break;
+	}
+}
+
+/*
+ * This function cancels the given work structure in a synchronous
+ * fashion. It returns non-zero if the work was successfully
+ * cancelled. Else the work was already cancelled.
+ */
+bool
+linux_cancel_work_sync(struct work_struct *work)
+{
+	static const uint8_t states[WORK_ST_MAX] __aligned(8) = {
+		[WORK_ST_IDLE] = WORK_ST_IDLE,		/* NOP */
+		[WORK_ST_TIMER] = WORK_ST_TIMER,	/* can't happen */
+		[WORK_ST_TASK] = WORK_ST_IDLE,		/* cancel and drain */
+		[WORK_ST_EXEC] = WORK_ST_IDLE,		/* too late, drain */
+		[WORK_ST_CANCEL] = WORK_ST_IDLE,	/* cancel and drain */
+	};
+	struct taskqueue *tq;
+
+	WITNESS_WARN(WARN_GIANTOK | WARN_SLEEPOK, NULL,
+	    "linux_cancel_work_sync() might sleep");
+
+	switch (linux_update_state(&work->state, states)) {
+	case WORK_ST_IDLE:
+	case WORK_ST_TIMER:
+		return (0);
+	case WORK_ST_EXEC:
+		tq = work->work_queue->taskqueue;
+		if (taskqueue_cancel(tq, &work->work_task, NULL) != 0)
+			taskqueue_drain(tq, &work->work_task);
+		return (0);
+	default:
+		tq = work->work_queue->taskqueue;
+		if (taskqueue_cancel(tq, &work->work_task, NULL) != 0)
+			taskqueue_drain(tq, &work->work_task);
+		return (1);
+	}
+}
+
+/*
+ * This function atomically stops the timer and callback. The timer
+ * callback will not be called after this function returns. This
+ * functions returns true when the timeout was cancelled. Else the
+ * timeout was not started or has already been called.
+ */
+static inline bool
+linux_cancel_timer(struct delayed_work *dwork, bool drain)
+{
+	bool cancelled;
+
+	mtx_lock(&dwork->timer.mtx);
+	cancelled = (callout_stop(&dwork->timer.callout) == 1);
+	mtx_unlock(&dwork->timer.mtx);
+
+	/* check if we should drain */
+	if (drain)
+		callout_drain(&dwork->timer.callout);
+	return (cancelled);
+}
+
+/*
+ * This function cancels the given delayed work structure in a
+ * non-blocking fashion. It returns non-zero if the work was
+ * successfully cancelled. Else the work may still be busy or already
+ * cancelled.
+ */
+bool
+linux_cancel_delayed_work(struct delayed_work *dwork)
+{
+	static const uint8_t states[WORK_ST_MAX] __aligned(8) = {
+		[WORK_ST_IDLE] = WORK_ST_IDLE,		/* NOP */
+		[WORK_ST_TIMER] = WORK_ST_CANCEL,	/* try to cancel */
+		[WORK_ST_TASK] = WORK_ST_CANCEL,	/* try to cancel */
+		[WORK_ST_EXEC] = WORK_ST_EXEC,		/* NOP */
+		[WORK_ST_CANCEL] = WORK_ST_CANCEL,	/* NOP */
+	};
+	struct taskqueue *tq;
+
+	switch (linux_update_state(&dwork->work.state, states)) {
+	case WORK_ST_TIMER:
+	case WORK_ST_CANCEL:
+		if (linux_cancel_timer(dwork, 0)) {
+			atomic_cmpxchg(&dwork->work.state,
+			    WORK_ST_CANCEL, WORK_ST_IDLE);
+			return (1);
+		}
+		/* FALLTHROUGH */
+	case WORK_ST_TASK:
+		tq = dwork->work.work_queue->taskqueue;
+		if (taskqueue_cancel(tq, &dwork->work.work_task, NULL) == 0) {
+			atomic_cmpxchg(&dwork->work.state,
+			    WORK_ST_CANCEL, WORK_ST_IDLE);
+			return (1);
+		}
+		/* FALLTHROUGH */
+	default:
+		return (0);
+	}
+}
+
+/*
+ * This function cancels the given work structure in a synchronous
+ * fashion. It returns non-zero if the work was successfully
+ * cancelled. Else the work was already cancelled.
+ */
+bool
+linux_cancel_delayed_work_sync(struct delayed_work *dwork)
+{
+	static const uint8_t states[WORK_ST_MAX] __aligned(8) = {
+		[WORK_ST_IDLE] = WORK_ST_IDLE,		/* NOP */
+		[WORK_ST_TIMER] = WORK_ST_IDLE,		/* cancel and drain */
+		[WORK_ST_TASK] = WORK_ST_IDLE,		/* cancel and drain */
+		[WORK_ST_EXEC] = WORK_ST_IDLE,		/* too late, drain */
+		[WORK_ST_CANCEL] = WORK_ST_IDLE,	/* cancel and drain */
+	};
+	struct taskqueue *tq;
+
+	WITNESS_WARN(WARN_GIANTOK | WARN_SLEEPOK, NULL,
+	    "linux_cancel_delayed_work_sync() might sleep");
+
+	switch (linux_update_state(&dwork->work.state, states)) {
+	case WORK_ST_IDLE:
+		return (0);
+	case WORK_ST_EXEC:
+		tq = dwork->work.work_queue->taskqueue;
+		if (taskqueue_cancel(tq, &dwork->work.work_task, NULL) != 0)
+			taskqueue_drain(tq, &dwork->work.work_task);
+		return (0);
+	case WORK_ST_TIMER:
+	case WORK_ST_CANCEL:
+		if (linux_cancel_timer(dwork, 1)) {
+			/*
+			 * Make sure taskqueue is also drained before
+			 * returning:
+			 */
+			tq = dwork->work.work_queue->taskqueue;
+			taskqueue_drain(tq, &dwork->work.work_task);
+			return (1);
+		}
+		/* FALLTHROUGH */
+	default:
+		tq = dwork->work.work_queue->taskqueue;
+		if (taskqueue_cancel(tq, &dwork->work.work_task, NULL) != 0)
+			taskqueue_drain(tq, &dwork->work.work_task);
+		return (1);
+	}
+}
+
+/*
+ * This function waits until the given work structure is completed.
+ * It returns non-zero if the work was successfully
+ * waited for. Else the work was not waited for.
+ */
+bool
+linux_flush_work(struct work_struct *work)
+{
+	struct taskqueue *tq;
+
+	WITNESS_WARN(WARN_GIANTOK | WARN_SLEEPOK, NULL,
+	    "linux_flush_work() might sleep");
+
+	switch (atomic_read(&work->state)) {
+	case WORK_ST_IDLE:
+		return (0);
+	default:
+		tq = work->work_queue->taskqueue;
+		taskqueue_drain(tq, &work->work_task);
+		return (1);
+	}
+}
+
+/*
+ * This function waits until the given delayed work structure is
+ * completed. It returns non-zero if the work was successfully waited
+ * for. Else the work was not waited for.
+ */
+bool
+linux_flush_delayed_work(struct delayed_work *dwork)
+{
+	struct taskqueue *tq;
+
+	WITNESS_WARN(WARN_GIANTOK | WARN_SLEEPOK, NULL,
+	    "linux_flush_delayed_work() might sleep");
+
+	switch (atomic_read(&dwork->work.state)) {
+	case WORK_ST_IDLE:
+		return (0);
+	case WORK_ST_TIMER:
+		if (linux_cancel_timer(dwork, 1))
+			linux_delayed_work_enqueue(dwork);
+		/* FALLTHROUGH */
+	default:
+		tq = dwork->work.work_queue->taskqueue;
+		taskqueue_drain(tq, &dwork->work.work_task);
+		return (1);
+	}
+}
+
+/*
+ * This function returns true if the given work is pending, and not
+ * yet executing:
+ */
+bool
+linux_work_pending(struct work_struct *work)
+{
+	switch (atomic_read(&work->state)) {
+	case WORK_ST_TIMER:
+	case WORK_ST_TASK:
+	case WORK_ST_CANCEL:
+		return (1);
+	default:
+		return (0);
+	}
+}
+
+/*
+ * This function returns true if the given work is busy.
+ */
+bool
+linux_work_busy(struct work_struct *work)
+{
+	struct taskqueue *tq;
+
+	switch (atomic_read(&work->state)) {
+	case WORK_ST_IDLE:
+		return (0);
+	case WORK_ST_EXEC:
+		tq = work->work_queue->taskqueue;
+		return (taskqueue_poll_is_busy(tq, &work->work_task));
+	default:
+		return (1);
+	}
+}
+
+struct workqueue_struct *
+linux_create_workqueue_common(const char *name, int cpus)
+{
+	struct workqueue_struct *wq;
+
+	/*
+	 * If zero CPUs are specified use the default number of CPUs:
+	 */
+	if (cpus == 0)
+		cpus = linux_default_wq_cpus;
+
+	wq = kmalloc(sizeof(*wq), M_WAITOK | M_ZERO);
+	wq->taskqueue = taskqueue_create(name, M_WAITOK,
+	    taskqueue_thread_enqueue, &wq->taskqueue);
+	atomic_set(&wq->draining, 0);
+	taskqueue_start_threads(&wq->taskqueue, cpus, PWAIT, "%s", name);
+	TAILQ_INIT(&wq->exec_head);
+	mtx_init(&wq->exec_mtx, "linux_wq_exec", NULL, MTX_DEF);
+
+	return (wq);
+}
+
+void
+linux_destroy_workqueue(struct workqueue_struct *wq)
+{
+	atomic_inc(&wq->draining);
+	drain_workqueue(wq);
+	taskqueue_free(wq->taskqueue);
+	mtx_destroy(&wq->exec_mtx);
+	kfree(wq);
+}
+
+void
+linux_init_delayed_work(struct delayed_work *dwork, work_func_t func)
+{
+	memset(dwork, 0, sizeof(*dwork));
+	dwork->work.func = func;
+	TASK_INIT(&dwork->work.work_task, 0, linux_delayed_work_fn, dwork);
+	mtx_init(&dwork->timer.mtx, spin_lock_name("lkpi-dwork"), NULL,
+	    MTX_DEF | MTX_NOWITNESS);
+	callout_init_mtx(&dwork->timer.callout, &dwork->timer.mtx, 0);
+}
+
+static void
+linux_work_init(void *arg)
+{
+	int max_wq_cpus = mp_ncpus + 1;
+
+	/* avoid deadlock when there are too few threads */
+	if (max_wq_cpus < 4)
+		max_wq_cpus = 4;
+
+	/* set default number of CPUs */
+	linux_default_wq_cpus = max_wq_cpus;
+
+	linux_system_short_wq = alloc_workqueue("linuxkpi_short_wq", 0, max_wq_cpus);
+	linux_system_long_wq = alloc_workqueue("linuxkpi_long_wq", 0, max_wq_cpus);
+
+	/* populate the workqueue pointers */
+	system_long_wq = linux_system_long_wq;
+	system_wq = linux_system_short_wq;
+	system_power_efficient_wq = linux_system_short_wq;
+	system_unbound_wq = linux_system_short_wq;
+}
+SYSINIT(linux_work_init, SI_SUB_TASKQ, SI_ORDER_THIRD, linux_work_init, NULL);
+
+static void
+linux_work_uninit(void *arg)
+{
+	destroy_workqueue(linux_system_short_wq);
+	destroy_workqueue(linux_system_long_wq);
+
+	/* clear workqueue pointers */
+	system_long_wq = NULL;
+	system_wq = NULL;
+	system_power_efficient_wq = NULL;
+	system_unbound_wq = NULL;
+}
+SYSUNINIT(linux_work_uninit, SI_SUB_TASKQ, SI_ORDER_THIRD, linux_work_uninit, NULL);
diff --git a/sys/conf/files b/sys/conf/files
index 28e7d58451b..0ef670ac2de 100644
--- a/sys/conf/files
+++ b/sys/conf/files
@@ -4177,14 +4177,34 @@ compat/linuxkpi/common/src/linux_kmod.c		optional compat_linuxkpi \
 	compile-with "${LINUXKPI_C}"
 compat/linuxkpi/common/src/linux_compat.c	optional compat_linuxkpi \
 	compile-with "${LINUXKPI_C}"
+compat/linuxkpi/common/src/linux_current.c	optional compat_linuxkpi \
+	compile-with "${LINUXKPI_C}"
+compat/linuxkpi/common/src/linux_hrtimer.c	optional compat_linuxkpi \
+	compile-with "${LINUXKPI_C}"
+compat/linuxkpi/common/src/linux_kthread.c	optional compat_linuxkpi \
+	compile-with "${LINUXKPI_C}"
+compat/linuxkpi/common/src/linux_lock.c		optional compat_linuxkpi \
+	compile-with "${LINUXKPI_C}"
+compat/linuxkpi/common/src/linux_page.c		optional compat_linuxkpi \
+	compile-with "${LINUXKPI_C}"
 compat/linuxkpi/common/src/linux_pci.c		optional compat_linuxkpi pci \
 	compile-with "${LINUXKPI_C}"
+compat/linuxkpi/common/src/linux_tasklet.c	optional compat_linuxkpi \
+	compile-with "${LINUXKPI_C}"
 compat/linuxkpi/common/src/linux_idr.c		optional compat_linuxkpi \
 	compile-with "${LINUXKPI_C}"
 compat/linuxkpi/common/src/linux_radix.c	optional compat_linuxkpi \
 	compile-with "${LINUXKPI_C}"
+compat/linuxkpi/common/src/linux_rcu.c		optional compat_linuxkpi \
+	compile-with "${LINUXKPI_C} -I$S/contrib/ck/include"
+compat/linuxkpi/common/src/linux_schedule.c	optional compat_linuxkpi \
+	compile-with "${LINUXKPI_C}"
+compat/linuxkpi/common/src/linux_slab.c		optional compat_linuxkpi \
+	compile-with "${LINUXKPI_C}"
 compat/linuxkpi/common/src/linux_usb.c		optional compat_linuxkpi usb \
 	compile-with "${LINUXKPI_C}"
+compat/linuxkpi/common/src/linux_work.c		optional compat_linuxkpi \
+	compile-with "${LINUXKPI_C}"
 
 # OpenFabrics Enterprise Distribution (Infiniband)
 ofed/drivers/infiniband/core/addr.c		optional ofed		\
diff --git a/sys/conf/files.amd64 b/sys/conf/files.amd64
index c7c87847b50..404e6e2db01 100644
--- a/sys/conf/files.amd64
+++ b/sys/conf/files.amd64
@@ -362,20 +362,34 @@ dev/qlxgbe/ql_isr.c		optional	qlxgbe pci
 dev/qlxgbe/ql_misc.c		optional	qlxgbe pci
 dev/qlxgbe/ql_os.c		optional	qlxgbe pci
 dev/qlxgbe/ql_reset.c		optional	qlxgbe pci
-dev/qlnx/qlnxe/ecore_cxt.c	optional	qlnxe pci
-dev/qlnx/qlnxe/ecore_dbg_fw_funcs.c optional	qlnxe pci
-dev/qlnx/qlnxe/ecore_dcbx.c	optional	qlnxe pci
-dev/qlnx/qlnxe/ecore_dev.c	optional	qlnxe pci
-dev/qlnx/qlnxe/ecore_hw.c	optional	qlnxe pci
-dev/qlnx/qlnxe/ecore_init_fw_funcs.c optional	qlnxe pci
-dev/qlnx/qlnxe/ecore_init_ops.c	optional	qlnxe pci
-dev/qlnx/qlnxe/ecore_int.c	optional	qlnxe pci
-dev/qlnx/qlnxe/ecore_l2.c	optional	qlnxe pci
-dev/qlnx/qlnxe/ecore_mcp.c	optional	qlnxe pci
-dev/qlnx/qlnxe/ecore_sp_commands.c optional	qlnxe pci
-dev/qlnx/qlnxe/ecore_spq.c	optional	qlnxe pci
-dev/qlnx/qlnxe/qlnx_ioctl.c	optional	qlnxe pci
-dev/qlnx/qlnxe/qlnx_os.c	optional	qlnxe pci
+dev/qlnx/qlnxe/ecore_cxt.c	optional	qlnxe pci \
+	compile-with "${LINUXKPI_C}"
+dev/qlnx/qlnxe/ecore_dbg_fw_funcs.c optional	qlnxe pci \
+	compile-with "${LINUXKPI_C}"
+dev/qlnx/qlnxe/ecore_dcbx.c	optional	qlnxe pci \
+	compile-with "${LINUXKPI_C}"
+dev/qlnx/qlnxe/ecore_dev.c	optional	qlnxe pci \
+	compile-with "${LINUXKPI_C}"
+dev/qlnx/qlnxe/ecore_hw.c	optional	qlnxe pci \
+	compile-with "${LINUXKPI_C}"
+dev/qlnx/qlnxe/ecore_init_fw_funcs.c optional	qlnxe pci \
+	compile-with "${LINUXKPI_C}"
+dev/qlnx/qlnxe/ecore_init_ops.c	optional	qlnxe pci \
+	compile-with "${LINUXKPI_C}"
+dev/qlnx/qlnxe/ecore_int.c	optional	qlnxe pci \
+	compile-with "${LINUXKPI_C}"
+dev/qlnx/qlnxe/ecore_l2.c	optional	qlnxe pci \
+	compile-with "${LINUXKPI_C}"
+dev/qlnx/qlnxe/ecore_mcp.c	optional	qlnxe pci \
+	compile-with "${LINUXKPI_C}"
+dev/qlnx/qlnxe/ecore_sp_commands.c optional	qlnxe pci \
+	compile-with "${LINUXKPI_C}"
+dev/qlnx/qlnxe/ecore_spq.c	optional	qlnxe pci \
+	compile-with "${LINUXKPI_C}"
+dev/qlnx/qlnxe/qlnx_ioctl.c	optional	qlnxe pci \
+	compile-with "${LINUXKPI_C}"
+dev/qlnx/qlnxe/qlnx_os.c	optional	qlnxe pci \
+	compile-with "${LINUXKPI_C}"
 dev/sfxge/common/ef10_ev.c	optional	sfxge pci
 dev/sfxge/common/ef10_filter.c	optional	sfxge pci
 dev/sfxge/common/ef10_intr.c	optional	sfxge pci
diff --git a/sys/contrib/rdma/krping/krping.c b/sys/contrib/rdma/krping/krping.c
index 6a8f3680877..55932611491 100644
--- a/sys/contrib/rdma/krping/krping.c
+++ b/sys/contrib/rdma/krping/krping.c
@@ -44,6 +44,7 @@ __FBSDID("$FreeBSD$");
 #include <linux/device.h>
 #include <linux/pci.h>
 #include <linux/sched.h>
+#include <linux/wait.h>
 
 #include <asm/atomic.h>
 
diff --git a/sys/dev/mlx5/mlx5_core/mlx5_uar.c b/sys/dev/mlx5/mlx5_core/mlx5_uar.c
index 7188f71cc43..c8084d5eafd 100644
--- a/sys/dev/mlx5/mlx5_core/mlx5_uar.c
+++ b/sys/dev/mlx5/mlx5_core/mlx5_uar.c
@@ -189,7 +189,8 @@ int mlx5_alloc_map_uar(struct mlx5_core_dev *mdev, struct mlx5_uar *uar)
 
 	if (mdev->priv.bf_mapping)
 		uar->bf_map = io_mapping_map_wc(mdev->priv.bf_mapping,
-						uar->index << PAGE_SHIFT);
+						uar->index << PAGE_SHIFT,
+						PAGE_SIZE);
 
 	return 0;
 
diff --git a/sys/dev/qlnx/qlnxe/bcm_osal.h b/sys/dev/qlnx/qlnxe/bcm_osal.h
index 8bdb4d30b25..664239c6932 100644
--- a/sys/dev/qlnx/qlnxe/bcm_osal.h
+++ b/sys/dev/qlnx/qlnxe/bcm_osal.h
@@ -38,7 +38,7 @@
 #include <compat/linuxkpi/common/include/linux/bitops.h>
 #else
 #if __FreeBSD_version >= 1100090
-#include <compat/linuxkpi/common/include/linux/bitops.h>
+#include <linux/bitmap.h>
 #else
 #include <ofed/include/linux/bitops.h>
 #endif
diff --git a/sys/modules/linuxkpi/Makefile b/sys/modules/linuxkpi/Makefile
index 7288448260e..bc9716b50ad 100644
--- a/sys/modules/linuxkpi/Makefile
+++ b/sys/modules/linuxkpi/Makefile
@@ -2,12 +2,22 @@
 .PATH:	${SRCTOP}/sys/compat/linuxkpi/common/src
 
 KMOD=	linuxkpi
-SRCS=	linux_kmod.c \
-	linux_compat.c \
+SRCS=	linux_compat.c \
+	linux_current.c \
+	linux_hrtimer.c \
+	linux_idr.c \
+	linux_kmod.c \
+	linux_kthread.c \
+	linux_lock.c \
+	linux_page.c \
 	linux_pci.c \
 	linux_radix.c \
-	linux_idr.c \
-	linux_usb.c
+	linux_rcu.c \
+	linux_schedule.c \
+	linux_slab.c \
+	linux_tasklet.c \
+	linux_usb.c \
+	linux_work.c
 
 SRCS+=	bus_if.h \
 	device_if.h \
@@ -17,5 +27,6 @@ SRCS+=	bus_if.h \
 	opt_usb.h
 
 CFLAGS+= -I${SRCTOP}/sys/compat/linuxkpi/common/include
+CFLAGS+= -I${SRCTOP}/sys/contrib/ck/include
 
 .include <bsd.kmod.mk>
diff --git a/sys/modules/qlnx/qlnxe/Makefile b/sys/modules/qlnx/qlnxe/Makefile
index d491028f520..4e89e4ece2a 100644
--- a/sys/modules/qlnx/qlnxe/Makefile
+++ b/sys/modules/qlnx/qlnxe/Makefile
@@ -57,6 +57,8 @@ CFLAGS += -DECORE_PACKAGE
 CFLAGS += -DCONFIG_ECORE_L2
 CFLAGS += -DECORE_CONFIG_DIRECT_HWFN
 
+CFLAGS+= -I${SRCTOP}/sys/compat/linuxkpi/common/include
+
 #CFLAGS += -g
 #CFLAGS += -fno-inline
 
diff --git a/sys/ofed/drivers/infiniband/core/cma.c b/sys/ofed/drivers/infiniband/core/cma.c
index 22c4689f411..7e31203e766 100644
--- a/sys/ofed/drivers/infiniband/core/cma.c
+++ b/sys/ofed/drivers/infiniband/core/cma.c
@@ -2473,7 +2473,7 @@ static int cma_alloc_any_port(struct idr *ps, struct rdma_id_private *id_priv)
 	int low, high, remaining;
 	unsigned int rover;
 
-	inet_get_local_port_range(&low, &high);
+	inet_get_local_port_range(&init_net, &low, &high);
 	remaining = (high - low) + 1;
 	rover = random() % remaining + low;
 retry:
diff --git a/sys/ofed/drivers/infiniband/core/fmr_pool.c b/sys/ofed/drivers/infiniband/core/fmr_pool.c
index c73196aa876..733aa38b363 100644
--- a/sys/ofed/drivers/infiniband/core/fmr_pool.c
+++ b/sys/ofed/drivers/infiniband/core/fmr_pool.c
@@ -37,6 +37,7 @@
 #include <linux/slab.h>
 #include <linux/jhash.h>
 #include <linux/kthread.h>
+#include <linux/wait.h>
 
 #include <rdma/ib_fmr_pool.h>
 
diff --git a/sys/ofed/drivers/infiniband/core/iwcm.c b/sys/ofed/drivers/infiniband/core/iwcm.c
index fa6b674bb0d..764754bc6f0 100644
--- a/sys/ofed/drivers/infiniband/core/iwcm.c
+++ b/sys/ofed/drivers/infiniband/core/iwcm.c
@@ -50,6 +50,7 @@
 #include <linux/slab.h>
 #include <linux/module.h>
 #include <linux/string.h>
+#include <linux/wait.h>
 #include <netinet/tcp.h>
 #include <sys/mutex.h>
 
diff --git a/sys/ofed/drivers/infiniband/core/umem.c b/sys/ofed/drivers/infiniband/core/umem.c
index db1969af293..590abd6c91a 100644
--- a/sys/ofed/drivers/infiniband/core/umem.c
+++ b/sys/ofed/drivers/infiniband/core/umem.c
@@ -40,6 +40,7 @@
 #include <linux/dma-attrs.h>
 #include <linux/slab.h>
 #include <linux/module.h>
+#include <linux/wait.h>
 #include <sys/priv.h>
 #include <sys/resourcevar.h>
 #include <vm/vm_pageout.h>
diff --git a/sys/ofed/drivers/infiniband/hw/mthca/mthca_dev.h b/sys/ofed/drivers/infiniband/hw/mthca/mthca_dev.h
index 14e3f6288aa..a6dfc6d307d 100644
--- a/sys/ofed/drivers/infiniband/hw/mthca/mthca_dev.h
+++ b/sys/ofed/drivers/infiniband/hw/mthca/mthca_dev.h
@@ -45,6 +45,7 @@
 #include <linux/mutex.h>
 #include <linux/list.h>
 #include <linux/semaphore.h>
+#include <linux/wait.h>
 
 #include "mthca_provider.h"
 #include "mthca_doorbell.h"
diff --git a/sys/ofed/drivers/net/mlx4/pd.c b/sys/ofed/drivers/net/mlx4/pd.c
index 89a8854699a..ecbe3eae7f4 100644
--- a/sys/ofed/drivers/net/mlx4/pd.c
+++ b/sys/ofed/drivers/net/mlx4/pd.c
@@ -204,7 +204,7 @@ int mlx4_bf_alloc(struct mlx4_dev *dev, struct mlx4_bf *bf, int node)
 			goto free_uar;
 		}
 
-		uar->bf_map = io_mapping_map_wc(priv->bf_mapping, uar->index << PAGE_SHIFT);
+		uar->bf_map = io_mapping_map_wc(priv->bf_mapping, uar->index << PAGE_SHIFT, PAGE_SIZE);
 		if (!uar->bf_map) {
 			err = -ENOMEM;
 			goto unamp_uar;
diff --git a/sys/sys/param.h b/sys/sys/param.h
index 93e2e7d161c..b0a2394cfe7 100644
--- a/sys/sys/param.h
+++ b/sys/sys/param.h
@@ -58,7 +58,7 @@
  *		in the range 5 to 9.
  */
 #undef __FreeBSD_version
-#define __FreeBSD_version 1101508	/* Master, propagated to newvers */
+#define __FreeBSD_version 1101509	/* Master, propagated to newvers */
 
 /*
  * __FreeBSD_kernel__ indicates that this system uses the kernel of FreeBSD,
diff --git a/sys/sys/proc.h b/sys/sys/proc.h
index cce2d286492..02efbb91312 100644
--- a/sys/sys/proc.h
+++ b/sys/sys/proc.h
@@ -349,6 +349,7 @@ struct thread {
 	struct syscall_args td_sa;	/* (kx) Syscall parameters. Copied on
 					   fork for child tracing. */
 	siginfo_t	td_si;		/* (c) For debugger or core file */
+	void		*td_lkpi_task;	/* LinuxKPI task struct pointer */
 };
 
 struct thread0_storage {
